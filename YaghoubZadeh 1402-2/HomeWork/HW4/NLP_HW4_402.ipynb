{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the Fourth Assignment of the NLP Course!\n",
        "\n",
        "If you have any questions or need more explanation about this homework, feel free to reach out to me via email, Telegram, or in person!"
      ],
      "metadata": {
        "id": "ychper6HMHq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, you'll become comfortable working with large language models. You'll practice creating prompts and using the ChatGPT API.\n",
        "\n"
      ],
      "metadata": {
        "id": "T8E9c4uqhOhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1_Classification Task"
      ],
      "metadata": {
        "id": "V5BI_QzqcJ4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you'll do a task called NLI (Natural Language Inference) using BERT, LLaMA, and ChatGPT on the MNLI dataset.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ht1WJiPT4OFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by running the cells below to load the MNLI dataset. The following code will select 200 random samples from the 'mismatched' test dataset. Remember that setting the seed ensures that the sample remains the same in each run."
      ],
      "metadata": {
        "id": "Tw8rtNWQzE0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "FVIB3brH3E1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "SEED = 32\n",
        "N_SAMPLES = 200\n",
        "mnli_dataset = load_dataset(\"glue\", \"mnli\")['test_mismatched']\n",
        "mnli_samples = mnli_dataset.shuffle(SEED).select(range(N_SAMPLES))"
      ],
      "metadata": {
        "id": "Ul0wd7w6zX-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**BERT**"
      ],
      "metadata": {
        "id": "a9cRVQYSzagL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, load a BERT model. You can either fine-tune a BERT model for this task or find a pre-finetuned checkpoint on Hugging Face and load it. Please use the `bert-base-uncased`."
      ],
      "metadata": {
        "id": "ACSghqkRzerF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bert for NLI task"
      ],
      "metadata": {
        "id": "TsnZ7-WszdD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the accuracy of your BERT model for the 200-sample MNLI dataset"
      ],
      "metadata": {
        "id": "IaPCN-BTzxPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy"
      ],
      "metadata": {
        "id": "Pn20og6_z_aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5ecGfodV8F9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Llama\n",
        "\n",
        ">LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023. For the first version of LLaMA, four model sizes were trained: 7, 13, 33 and 65 billion parameters. LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla. Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license. In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.[Wiki](https://en.wikipedia.org/wiki/LLaMA)"
      ],
      "metadata": {
        "id": "6XShUqPJ0CEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you'll utilize the LLaMA model to perform the NLI task. Given the server account provided for you, I've already downloaded the model checkpoint, so you just need to load it again. Run the codes below to load the LLaMA model.\n",
        "\n",
        "Keep in mind that if you choose not to use the server account for this part, you should submit an access request to the model using this [form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) for Meta models, this [video](https://youtu.be/Z6sCl6abJj4?si=DN428WDxnQ8pUBiF) might be helpful in this matter.\n",
        "\n"
      ],
      "metadata": {
        "id": "4JfyR4C68Zjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnIQDvU-OXwa",
        "outputId": "7db2da3f-9086-433b-9103-9717d0efb75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login('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') # toker for the course account on huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IsRRcu0OXDQ",
        "outputId": "4a6be13b-402c-4bae-f964-c592400c28c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "fAcPrU3YLgQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have your tokenizer and the model ready you need to desing a suitable promt to perform classification task on the provided 200 samples.\n",
        "\n",
        "To perform a classification task:\n",
        "\n",
        "1. Create a clear and informative prompt to guide the generative model in understanding the specific requirements of the classification task.\n",
        "\n",
        "2. Generate output using the prompt. For LLaMa, utilize the `generate` function.\n",
        "\n",
        "3. Post-process the generated output to identify the classification format answer. In MNLI, determine if the input falls into categories such as contradiction, entailment, or neutral.\n",
        "\n",
        "Finally, calculate the accuracy.\n"
      ],
      "metadata": {
        "id": "ybbpEf8gLgwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate answers and claculate accuracy"
      ],
      "metadata": {
        "id": "3PfWJcVy0Eo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat GPT"
      ],
      "metadata": {
        "id": "Devg9FI_Le3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're familiar with ChatGPT and use it daily. However, for more extensive tasks, such as performing the NLI task on our 200-sample dataset, you'll need to utilize its API to automatically obtain answers for the input.\n",
        "\n",
        "To begin, you'll require an OpenAI account with an authorized phone number to access the API. Once you obtain the token from your account, you can use the provided code to connect to the OpenAI API and access its services. For your convenience, we've created an account with a token that you can use."
      ],
      "metadata": {
        "id": "TJUM_YKJXbXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openaiw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3PycPKgqpuX",
        "outputId": "ed15706a-1ac0-41d6-e487-dd0b6bb95955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.8-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/221.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/221.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "fWKaLQ1dLeFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform a classification task using ChatGPT, follow the general steps outlined in the previous section. Refer to this [link](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) for instructions on preparing and requesting the model. As mention in the link please use the `gpt-3.5-turbo` model. The pricing information for API usage is also available on this [page](https://openai.com/pricing), in case you're curious.\n",
        "\n",
        "Keep in mind that there is a limit on API usage, so optimize your requests accordingly.\n"
      ],
      "metadata": {
        "id": "rfOwCM8HwJYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#requesting GPT model"
      ],
      "metadata": {
        "id": "Lzh4AafWrZUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do not forget to calculate the accuracy!"
      ],
      "metadata": {
        "id": "NR1yVMntzcns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy"
      ],
      "metadata": {
        "id": "bFzojKz_zdFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, compare the results from all three models and discuss the obtained accuracies."
      ],
      "metadata": {
        "id": "N5xHRBK10I6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2_Temperature"
      ],
      "metadata": {
        "id": "4e7gp7aqzE5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The **temperature** parameter is an important hyperparameter in generative language models that can be used to control the randomness and creativity of the generated text.\n",
        "\n",
        "Search about this hyperparameter and explain how it works? Where is it used in generative models? Also, include the formula for the part of the model that has the temperature parameter and describe how it affects the creativity of the model based on it. Please provide a theoretical explanation for your response."
      ],
      "metadata": {
        "id": "CbxoeyJe4cE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "For both generative models in this assignment, you have the option to adjust the temperature hyper-parameter when generating the output. For the given prompts, explore the responses of *Llama* and *ChatGPT* models. use five different temperature settings that cover both low and high ranges.\n"
      ],
      "metadata": {
        "id": "f0LRuGlm69kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "promt1 = 'Once upon a time'\n",
        "promt2 = 'In a world where cats can speak, the first thing a cat said was:'"
      ],
      "metadata": {
        "id": "qb4ZPGymAIbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss the results and compare the outputs for different temperatures. Do the outcomes align with your expectations based on the definition of temperature?\n",
        "\n",
        "Enjoy!!"
      ],
      "metadata": {
        "id": "rs7sSTT-77yO"
      }
    }
  ]
}
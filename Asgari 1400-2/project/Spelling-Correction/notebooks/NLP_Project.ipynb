{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vFve7cwmXBsv",
        "vnYsZplsc6X9",
        "Hz1BtP5QSbpg",
        "Sh4W7hR3SBmu",
        "3VzChF0E35j4",
        "AXfgJbjmwScQ",
        "IlJ6xBpM5HIf",
        "YoBAZxXlFKSv",
        "XgFf3sJ3-6dj",
        "a7aLUQb76L80",
        "QBjETHHbPn4M",
        "rW2La_scbviZ",
        "bGiGuMtdFRHN",
        "yUP2VAPxGlaP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project\n",
        "\n",
        "* Peyman Naseri, `96100522`\n",
        "* Mobina Pournemat, `97105833`\n",
        "* \n",
        "*\n",
        "*"
      ],
      "metadata": {
        "id": "qAR7rIo58DXm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCK8LnZQBNVM"
      },
      "source": [
        "# setting gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for use local gpu run this comand in anaconda prompet and copy url and paste on colab:\n",
        "\n",
        "    pip install jupyter_http_over_ws\n",
        "    jupyter serverextension enable --py jupyter_http_over_ws\n",
        "    jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
        "\n",
        "https://research.google.com/colaboratory/local-runtimes.html  \n"
      ],
      "metadata": {
        "id": "QzI27CQ3cP1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g36USjN18vKn",
        "outputId": "d1e0a962-02be-4d9a-93f0-4989a601793a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "NVIDIA GeForce RTX 2080 with Max-Q Design\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#   try:\n",
        "#     for gpu in gpus:\n",
        "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#       # tf.config.experimental.set_virtual_device_configuration(gpu,\n",
        "#       #                                                         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*6)])\n",
        "\n",
        "#   except RuntimeError as e:\n",
        "#     print(e)\n",
        "\n",
        "\n",
        "# if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "#   !nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "metadata": {
        "id": "LycGcMqnX9DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gpu monitoring"
      ],
      "metadata": {
        "id": "8Tl9GWX8p2NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets accelerate nvidia-ml-py3"
      ],
      "metadata": {
        "id": "zCqyErlop-mJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4794578b-64b8-4455-8fa4-4571c1f43e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /home/peyman/anaconda3/lib/python3.9/site-packages (4.21.1)\r\n",
            "Requirement already satisfied: datasets in /home/peyman/anaconda3/lib/python3.9/site-packages (2.4.0)\r\n",
            "Requirement already satisfied: accelerate in /home/peyman/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /home/peyman/anaconda3/lib/python3.9/site-packages (7.352.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/peyman/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: xxhash in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (2022.2.0)\n",
            "Requirement already satisfied: pandas in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
            "Requirement already satisfied: dill<0.3.6 in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: responses<0.19 in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: aiohttp in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: psutil in /home/peyman/anaconda3/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from accelerate) (1.12.1+cu116)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/peyman/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/peyman/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/peyman/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/peyman/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/peyman/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/peyman/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/peyman/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/peyman/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/peyman/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/peyman/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()"
      ],
      "metadata": {
        "id": "gm1cRDuDqmAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqZFJZTHq7O3",
        "outputId": "dfe21433-4804-4417-e940-2002c1e604d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i29aa6pqoMv",
        "outputId": "94b244a3-69e2-4526-ca06-62942f1f6fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 218 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# requirement package"
      ],
      "metadata": {
        "id": "7leuEyoOvzTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# !pip install Levenshtein\n",
        "# !pip install transformers\n",
        "# !pip install hazm"
      ],
      "metadata": {
        "id": "hFeT5mhh9fUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import Levenshtein\n",
        "import re\n",
        "from hazm import *"
      ],
      "metadata": {
        "id": "GAEbF4MnFW0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "# from pandas_profiling import ProfileReport\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns               \n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "import warnings                                            \n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "tZWKj1xpY2vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "CYHM4Zm1f001"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "vFve7cwmXBsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sZ3FVu0LQKiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/NLP/HW3/NLP-HW3-Resources/\n",
        "\n",
        "import zipfile\n",
        "\n",
        "datasets = [\"cultural.zip\", \"economics.zip\", \"politics.zip\", \"sports.zip\"]\n",
        "data = []\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "    with zipfile.ZipFile(dataset) as zipper:\n",
        "        with zipper.open(dataset.split(\".\")[0]+'.txt') as fp:\n",
        "            d = fp.read().decode('utf-8').split('\\n')\n",
        "            data += d #:len(d)//16\n",
        "\n",
        "\n",
        "\n",
        "# %cd /content/drive/MyDrive/NLP/HW3/"
      ],
      "metadata": {
        "id": "HD27Ky5MUf3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uxTKLa7fmpl"
      },
      "source": [
        "**Manipulating the data according to the required Input formatting**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input format required by BERT asks us to add special tokens to the start and end of the sentence, pad and truncate all the sentences to a specific constant length and differentiate the pads from tokens by using attention masks."
      ],
      "metadata": {
        "id": "POEsc5DnaPUD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOCkZY_vWJcK"
      },
      "source": [
        "To get an idea about the length of the description and its corresponding frequency\n",
        "\n",
        "As we have to explicitly pad and truncate all the sentences to a fixed constant length, visualisation of the text length of the cleaned and pre processed data is done in order to select a value that is as close to the actual value (so that we do not lose useful information)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i.split()) for i in data]"
      ],
      "metadata": {
        "id": "VmhEOnNOXzXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(11.7*2,8.27*2)})\n",
        "his_plot = sns.histplot(seq_len ,bins=500 )\n",
        "his_plot.set(xlim=(0,500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "_8u9NlOXY8Gf",
        "outputId": "b04e79b1-1883-4f5c-eb89-0c0197234059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.0, 500.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1684.8x1190.88 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_len  = pd.DataFrame(seq_len, columns=['length'])\n",
        "df_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3R1CfSQ4aKNo",
        "outputId": "ba97a1e6-cc71-479c-f432-f2fc156ef420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         length\n",
              "0           130\n",
              "1             5\n",
              "2             9\n",
              "3           132\n",
              "4           353\n",
              "...         ...\n",
              "4331680      45\n",
              "4331681      75\n",
              "4331682      35\n",
              "4331683       0\n",
              "4331684       0\n",
              "\n",
              "[4331685 rows x 1 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4331680</th>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4331681</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4331682</th>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4331683</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4331684</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4331685 rows × 1 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_len.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "c2YYEPKnaLCO",
        "outputId": "7f8cc310-a89d-46c8-82ea-1832945594a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             length\n",
              "count  4.331685e+06\n",
              "mean   5.115670e+01\n",
              "std    7.158993e+01\n",
              "min    0.000000e+00\n",
              "25%    9.000000e+00\n",
              "50%    3.600000e+01\n",
              "75%    6.400000e+01\n",
              "max    5.088000e+03"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.331685e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.115670e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.158993e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.600000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.400000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.088000e+03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XlG2yAGgjCi"
      },
      "source": [
        "From the above graph, we can see that most of the description lengths are less than 100 words.\n",
        " <!-- **Hence, the MAX_LEN has been chosen to be 128**. -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SqRz4MnwWgDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "3Xc2vJp_3zoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface pre-trained Bert model"
      ],
      "metadata": {
        "id": "lsYjWZCTO8fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download model"
      ],
      "metadata": {
        "id": "vnYsZplsc6X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "در این بخش تمرین و برای پیاده سازی یک ترنسفورمر، از مدل pre-trained شده استفاده می کنیم. این مدل که bert نام دارد از وبسایت huggingface که شامل مدل های pre-trained شده می باشد، لود می شود. لینک مدل در بخش مراجع آمده است.<br>\n",
        "برای شروع کار ابتدا کتابخانه های مورد نیاز را دانلود میکنیم که به ترتیب برای محاسبه فاصله ویرایشی کلمات، ترنسفورمر مورد استفاده و نیز هضم برای توکنایز کردن می باشند.<br>\n",
        "در مرحله بعد، مدل مورد استفاده و نیز توکنایزر مخصوص به آن که pre-trained نیز هستند لود می شوند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "PLa2F3gTtXkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")"
      ],
      "metadata": {
        "id": "vZNhTvX20q_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5b2167-0148-4270-c31a-b66d9a89db4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find possible mistakes function"
      ],
      "metadata": {
        "id": "8ITVSt4aLTkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "حال باید بتوانیم با استفاده از این مدل، غلط های نگارشی موجود در متن را پیدا کنیم. تابع زیر بدین منظور تعریف شده است و به این صورت عمل می کند که تک تک کلمات جمله را به ترتیب ماسک می کند و با استفاده از مدل و بسته به مقدار top_k که در ورودی تابع تعریف می شود، کلماتی که به احتمال بالا می توانند بجای کلمه ماسک شده قرار بگیرند تولید و به اندازه top_k انتخاب خواهند شد. سپس این کلمات تک تک بررسی می شوند و کلمه ای را که فاصله ویرایشی آن با کلمه ماسک شده کمتر است را به عنوان خروجی نهایی تولید می کند. پس از این که این عمل بر روی تک تک کلمات انجام گرفت، خروجی های درست از نظر مدل تولید می شوند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "Y_M3K29TxsN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "misssplet_df = pd.read_csv('faspell_main.csv')\n",
        "misssplet_df"
      ],
      "metadata": {
        "id": "0o84mG2KW3iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_possible_mistakes(inp, model, top_k=10000):\n",
        "  # inp = inp.replace(\"\\u200C\", \" \")\n",
        "  tokens = word_tokenize(inp) # tokenize input\n",
        "  # tokens = inp.split()\n",
        "  # tokens = tokenizer.tokenize(inp)\n",
        "  mistakes = []\n",
        "  for i, token in enumerate(tokens):\n",
        "    # print(\"-----------\", token, \"-----------\")\n",
        "    token = token.replace(\"\\u200C\", \"\")\n",
        "\n",
        "    #mask ith word in input\n",
        "    text = \" \".join(tokens[:i]) + tokenizer.mask_token + \" \".join(tokens[i+1:])\n",
        "\n",
        "    # embedding text \n",
        "    input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "    mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "\n",
        "    logits = model(**input)\n",
        "    logits = logits.logits\n",
        "    softmax = F.softmax(logits, dim = -1)\n",
        "    mask_word = softmax[0, mask_index, :]\n",
        "    tops = torch.topk(mask_word, top_k, dim = 1)[1][0]\n",
        "\n",
        "    \n",
        "    least_dist = float(\"inf\")\n",
        "    corrected_word = token\n",
        "    \n",
        "    for w in tops:\n",
        "      word = tokenizer.decode([w])\n",
        "      # print(word)\n",
        "      dist = Levenshtein.distance(token, word)\n",
        "      if dist < least_dist:\n",
        "        corrected_word = word\n",
        "        least_dist = dist\n",
        "\n",
        "    if token != corrected_word:\n",
        "      for reg in re.finditer(token, inp):\n",
        "        s, e = reg.start(), reg.end()\n",
        "      mistakes.append({\"raw\": token, \"corrected\": corrected_word, \"span\": [s, e]})\n",
        "  return mistakes"
      ],
      "metadata": {
        "id": "hL_xhPn2_ls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input0 = \"این دانشمند تیرانی باعث افتخار است.\"\n",
        "input1 = \"پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\"\n",
        "input2 = \"بسیاری از مباحث علوم غیرطبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه‌های خاصی رجو کرد.\"\n",
        "input3 = 'اما متأسفانه به قدری ساختار سارمان سینمایی و در سطح وسیع‌تر وزارت فرهنگ و ارشاد اصلامی عقب‌مانده و ناکارآمد است که عملا جلوی بهبود هر مشکلی را می‌گیرد!'\n",
        "input4 = 'منطق جغرافیا و جئوپلیتیک همیشه ثابت است و قابل چشم‌پوسی نیست.‎'"
      ],
      "metadata": {
        "id": "_sgfbNfd9lgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained model results"
      ],
      "metadata": {
        "id": "Hz1BtP5QSbpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "result = find_possible_mistakes(input1, model, top_k=10000)\n",
        "print(input1)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "CNEwa1unA3qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d70787-f6cd-4fb0-a4c8-06371d174316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"کسف\",\n",
            "        \"corrected\": \"کشف\",\n",
            "        \"span\": [\n",
            "            31,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"الکل\",\n",
            "        \"corrected\": \"الکا\",\n",
            "        \"span\": [\n",
            "            35,\n",
            "            39\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"تیرانی\",\n",
            "        \"corrected\": \"ایرانی\",\n",
            "        \"span\": [\n",
            "            56,\n",
            "            62\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input2, model, top_k=10000)\n",
        "print(input2)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "khTioECUR39F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30d5dcb-67a2-4125-fc34-0101edc6c067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه‌های خاصی رجو کرد.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"فیریک\",\n",
            "        \"corrected\": \"فیزیک\",\n",
            "        \"span\": [\n",
            "            44,\n",
            "            49\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"ابل\",\n",
            "        \"corrected\": \"قابل\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            64\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"رجو\",\n",
            "        \"corrected\": \"رجوع\",\n",
            "        \"span\": [\n",
            "            114,\n",
            "            117\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input3, model, top_k=10000)\n",
        "print(input3)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "CGPoE2-l5ojw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb85b51-9150-4264-8367-8436e1eb80c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اما متأسفانه به قدری ساختار سارمان سینمایی و در سطح وسیع‌تر وزارت فرهنگ و ارشاد اصلامی عقب‌مانده و ناکارآمد است که عملا جلوی بهبود هر مشکلی را می‌گیرد!\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"متأسفانه\",\n",
            "        \"corrected\": \"متاسفانه\",\n",
            "        \"span\": [\n",
            "            4,\n",
            "            12\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"سارمان\",\n",
            "        \"corrected\": \"سازمان\",\n",
            "        \"span\": [\n",
            "            28,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"اصلامی\",\n",
            "        \"corrected\": \"اسلامی\",\n",
            "        \"span\": [\n",
            "            80,\n",
            "            86\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"ناکارآمد\",\n",
            "        \"corrected\": \"ناکارامد\",\n",
            "        \"span\": [\n",
            "            99,\n",
            "            107\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input4, model, top_k=10000)\n",
        "print(input4)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "e2xBwpv95pA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cff6eaf-844e-4d75-dcf5-8542b01e7a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "منطق جغرافیا و جئوپلیتیک همیشه ثابت است و قابل چشم‌پوسی نیست.‎\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"جغرافیا\",\n",
            "        \"corrected\": \"گرافین\",\n",
            "        \"span\": [\n",
            "            5,\n",
            "            12\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"جئوپلیتیک\",\n",
            "        \"corrected\": \"ژيوپلیتیک\",\n",
            "        \"span\": [\n",
            "            15,\n",
            "            24\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"چشمپوسی\",\n",
            "        \"corrected\": \"چشمپوشی\",\n",
            "        \"span\": [\n",
            "            15,\n",
            "            24\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"‎\",\n",
            "        \"corrected\": \".\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            62\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "قابل ذکر است که با تغییر مقدار top_k می توان مدل را بهتر یا بدتر کرد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "fxrenVqrzkdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune with more data"
      ],
      "metadata": {
        "id": "qJed9qr_QGdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "حال میخواهیم مدل را با استفاده از داده های خودمان دوباره آموزش دهیم. برای این کار از بین 4 دیتاست موجود، از هر کدام 500 جمله اول را انتخاب میکنیم تا مدل را با آن ها آموزش دهیم. به خاطر مشکل حجم و پردازش کولب، هنگام استفاده از داده های بیشتر، کولب کرش میکرد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "KsQa01-azvXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "برای آماده سازی دیتاست، آن را توکنایز کرده و در وکتور های 128 تایی امبد می کنیم. لیبل های این ورودی هارا بصورت رندم ماسک می کنیم و نهایتا دیتاست مورد نظر را با استفاده از این ورودی ها و خروجی ها می سازیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "PDUaq-sq02jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = len(data)//8\n",
        "print('num data for train :', l)\n",
        "inputs = tokenizer(data[:l], return_tensors='pt', max_length=64, truncation=True, padding='max_length')\n",
        "\n",
        "# create a key for labels\n",
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "\n",
        "# create random array of floats in equal dimension to input_ids\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "# create mask array: where the random array is less than 0.15, we set true, also where it is not CLS or SEP\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
        "\n",
        "# create selection from mask_arr\n",
        "# FYI: [MASK] == 103\n",
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n",
        "    inputs.input_ids[i, selection[i]] = 103"
      ],
      "metadata": {
        "id": "1Ux81YmpEzMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4ea3e0-50cf-4f26-abbc-126529ebee77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num data for train : 541460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "dataset = Dataset(inputs)"
      ],
      "metadata": {
        "id": "Z2i-7QC8JE99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "نهایتا مدل را با استفاده توابع آماده ترنسفورمر با تعداد ایپاک 5 و batch size مساوی 8 ترین می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "xMzEOV4v28qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 40\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='out',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "\n",
        "    optim=\"adafactor\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    logging_strategy=\"no\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-Vo79hJgKRhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "6591af19-b2d7-4028-c8cf-23dca67240cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 541460\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 40\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 160\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 3384\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1497' max='3384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1497/3384 1:06:08 < 1:23:28, 0.38 it/s, Epoch 0.44/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to out/checkpoint-500\n",
            "Configuration saved in out/checkpoint-500/config.json\n",
            "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to out/checkpoint-1000\n",
            "Configuration saved in out/checkpoint-1000/config.json\n",
            "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results after fine-tuning"
      ],
      "metadata": {
        "id": "Sh4W7hR3SBmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "مدل را از gpu به cpu انتقال می دهیم و نتایج را مشاهده می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "546X0pYP3Szv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model.to(\"cpu\")"
      ],
      "metadata": {
        "id": "8ZkW6W7hKg6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input1, model, top_k=2000)\n",
        "print(input1)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "RtvgcEaJSFvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input2, model, top_k=2000)\n",
        "print(input2)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "Gm_VM6G6LDac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input3, model, top_k=1000)\n",
        "print(input3)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "zOC-gycec2J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input4, model, top_k=1000)\n",
        "print(input4)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "1wtonjLJc2KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "متاسفانه نتایج بعد از یادگیری دوباره بهتر نشده اند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "WaK5BFaQ3cvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams"
      ],
      "metadata": {
        "id": "3VzChF0E35j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "یک راه دیگر برای اصلاح غلط های املایی، استفاده از مدل زبانی n-gram است. با توجه به محدودیت RAM در کولب، تنها بخشی از داده های هر فایل به مدل داده شده است.  \n",
        "در ابتدا باید روی داده پیش پردازش هایی نظیر Normalization، Tokenization، اضافه کردن کاراکتر های آغاز و پایان جمله و ... صورت گیرد. هم چنین برای جلوگیری از صفر شدن بسیاری از احتمال های n-gram ها فرایند smoothing نیز روی مدل زبانی اعمال شده است.\n",
        "</div>"
      ],
      "metadata": {
        "id": "yCMRUwKr9Gnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "# from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm\n",
        "from itertools import product\n",
        "import math\n",
        "import nltk\n",
        "import json"
      ],
      "metadata": {
        "id": "z0qoCIcs38R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oxKKDN3uUUYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/NLP/HW3/NLP-HW3-Resources/\n",
        "\n",
        "import zipfile\n",
        "\n",
        "datasets = [\"cultural.zip\", \"economics.zip\", \"politics.zip\", \"sports.zip\"]\n",
        "data = []\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "    with zipfile.ZipFile(dataset) as zipper:\n",
        "        with zipper.open(dataset.split(\".\")[0]+'.txt') as fp:\n",
        "            data += fp.read().decode('utf-8').split('\\n')[:100000]\n",
        "\n",
        "\n",
        "# %cd /content/drive/MyDrive/NLP/HW3/"
      ],
      "metadata": {
        "id": "tjj1sDL8Uc8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(\"\\n\".join(data))\n",
        "len(sentences)"
      ],
      "metadata": {
        "id": "Usg9V5quU4e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "sentences = [normalizer.normalize(x) for x in tqdm.tqdm(sentences)]\n",
        "sentences = [word_tokenize(sent) for sent in tqdm.tqdm(sentences)]\n",
        "sentences = [' '.join(x) for x in tqdm.tqdm(sentences)]"
      ],
      "metadata": {
        "id": "AmaFxNtCVWbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "\n",
        "    SOS = \"<s>\"\n",
        "    EOS = \"</s>\"\n",
        "    UNK = \"<UNK>\"\n",
        "    \n",
        "    def __init__(self, train_data, n, laplace=1):\n",
        "        self.n = n\n",
        "        self.vocab = dict()\n",
        "        self.laplace = laplace\n",
        "        self.tokens = self.preprocess(train_data, n)\n",
        "        self.vocab  = nltk.FreqDist(self.tokens)\n",
        "        self.model  = self._create_model()\n",
        "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
        "\n",
        "\n",
        "\n",
        "    def _smooth(self):\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
        "        n_vocab = nltk.FreqDist(n_grams)\n",
        "\n",
        "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
        "        m_vocab = nltk.FreqDist(m_grams)\n",
        "\n",
        "        def smoothed_count(n_gram, n_count):\n",
        "            m_gram = n_gram[:-1]\n",
        "            m_count = m_vocab[m_gram]\n",
        "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
        "\n",
        "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
        "\n",
        "\n",
        "\n",
        "    def _create_model(self):\n",
        "        if self.n == 1:\n",
        "            num_tokens = len(self.tokens)\n",
        "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
        "        else:\n",
        "            return self._smooth()\n",
        "\n",
        "\n",
        "\n",
        "    def _convert_oov(self, ngram):\n",
        "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
        "\n",
        "        ngram = (ngram,) if type(ngram) is str else ngram\n",
        "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
        "            if possible_known in self.model:\n",
        "                return possible_known\n",
        "\n",
        "\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        test_tokens = self.preprocess(test_data, self.n)\n",
        "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
        "        N = len(test_tokens)\n",
        "\n",
        "        known_ngrams  = [self._convert_oov(ngram) for ngram in test_ngrams]\n",
        "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
        "        \n",
        "        # for x,y in zip(known_ngrams, probabilities):\n",
        "        #     print(x,y)\n",
        "        \n",
        "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
        "\n",
        "\n",
        "\n",
        "    def _best_candidate(self, prev, k=10, without=[]):\n",
        "        \n",
        "        blacklist  = [LanguageModel.UNK] + without\n",
        "        # if len(prev) < self.n:\n",
        "        #     prev = [LanguageModel.SOS]*(self.n-1)\n",
        "\n",
        "        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==tuple(prev)))\n",
        "\n",
        "        probs = [y for x,y in candidates]\n",
        "        probs = probs/np.sum(probs)\n",
        "        words = [x for x,y in candidates]\n",
        "\n",
        "        candidates.sort(key = lambda x: -x[1])\n",
        "        return candidates[:k]\n",
        "\n",
        "\n",
        "    def preprocess(self, sentences, n):\n",
        "        sentences = self.add_sentence_tokens(sentences, n)\n",
        "        tokens = ' '.join(sentences).split()\n",
        "        tokens = self.replace_singletons(tokens)\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def add_sentence_tokens(self, sentences, n):\n",
        "        sos = ' '.join([LanguageModel.SOS] * (n-1)) if n > 1 else LanguageModel.SOS\n",
        "        return ['{} {} {}'.format(sos, s, LanguageModel.EOS) for s in sentences]\n",
        "\n",
        "\n",
        "    def replace_singletons(self, tokens):\n",
        "        if len(self.vocab) == 0:\n",
        "            self.vocab = nltk.FreqDist(tokens)\n",
        "        return [token if self.vocab[token] > 1 else LanguageModel.UNK for token in tokens]"
      ],
      "metadata": {
        "id": "DcWXx4HiVeTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "پس از بررسی مقادیر مختلف برای n و پارامتر smoothing بهترین مقادیر برای آن ها 3 و 1 به دست آمد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "bcQStviXAEkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_model = LanguageModel(sentences, 3, 1)"
      ],
      "metadata": {
        "id": "PoQY-vGSVg6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test using edit distance"
      ],
      "metadata": {
        "id": "AXfgJbjmwScQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "برای تست کردن مدل زبانی 3-gram مان، به ابتدا و انتهای جمله کاراکتر های شروع و پایان را اضافه می کنیم و سپس به ازای هر کلمه از جمله ، دو کلمه قبلی را به مدل زبانی میدهیم و k تا بهترین گزینه برای آن جایگاه را از آن دریافت می کنیم. سپس edit-distance کاندید ها با کلمه مورد نظر را حساب می کنیم و کاندیدی که کمترین فاصله را داشته باشد را به عنوان اصلاح شده آن کلمه در نظر می گیریم و جمله را اصلاح می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "LQ4rLLq6Af18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import probability\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import Levenshtein\n",
        "import re\n",
        "from hazm import *\n",
        "\n",
        "\n",
        "def find_possible_mistakes_ngram(inp, top_k=20):\n",
        "    mistakes = []\n",
        "    tokens = word_tokenize(inp)\n",
        "    tokens = 2 * [\"<s>\"] + tokens\n",
        "\n",
        "    for i in range(len(tokens)-2):\n",
        "        tops = language_model._best_candidate([tokens[i], tokens[i+1]], top_k)\n",
        "\n",
        "        least_dist = float(\"inf\")\n",
        "        corrected_word = tokens[i+2]\n",
        "\n",
        "        for word, prob in tops:\n",
        "            dist = Levenshtein.distance(tokens[i+2], word)\n",
        "            if dist < least_dist:\n",
        "                corrected_word = word\n",
        "                least_dist = dist\n",
        "\n",
        "        if tokens[i+2] != corrected_word:\n",
        "            for reg in re.finditer(tokens[i+2], inp):\n",
        "                s, e = reg.start(), reg.end()\n",
        "            mistakes.append({\"raw\": tokens[i+2], \"corrected\": corrected_word, \"span\": [s, e]})\n",
        "\n",
        "    return mistakes"
      ],
      "metadata": {
        "id": "N1hR_pSTWCi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input1, top_k=10)\n",
        "print(input1)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "NWQYDvyDjex0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input2, top_k=100)\n",
        "print(input2)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "ZMexNA1yV-Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "همان طور که مشاهده می کنید مدل زبانی روی جملات داده تست خوب کار نمی کند. دلیل این امر این است که داده کافی برای ترین نداشته و هم چنین به طور کلی مدل زبانی یک گزینه ایده آل برای چنین مسئله ای نیست و در مقایسه با مدل های نظیر LSTM خیلی ضعیف تر کار می کند. ولی در ادامه جملاتی مشابه با داده ترین را به مدل داده ایم و می بینیم که عملکرد آن بسیار بهتر از حالت قبل است و نشان می دهد که کمبود داده یادگیری عامل مهمی در نتایج بد مثال های قبل بوده است.\n",
        "</div>"
      ],
      "metadata": {
        "id": "mdGbhEwACJnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input3, top_k=100)\n",
        "print(input3)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "WXSxM_Bx1-S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input4, top_k=100)\n",
        "print(input4)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "eayZDjuC1-6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBOW"
      ],
      "metadata": {
        "id": "IlJ6xBpM5HIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "علاوه بر مدل های بالا، سعی کردیم از مدل های دیگری نیز استفاده کنیم تا نتایج را بهبود ببخشیم ولی به دلیل کمبود منابع نتیجه مطلوب حاصل نشد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "qd2WbPjxDNgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "در ادامه مدل CBOW را مشاهده می کنید که به کار گرفته شد ولی محدودیت حافظه در colab، اجازه ترین کردن مدل با داده کافی را نمیداد و به همین دلیل امبدینگ های حاصل information لازم را دارا نبودند و نتیجه مطلوب حاصل نمیشد.\n",
        "در ادامه از مدل CBOW از قبل ترین شده روی داده های فارسی نیز به کار گرفته شد ولی باز هم نتایج مطلوب نبود و در نهایت از آن استفاده نکردیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "93hM1369DnXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW from Scratch"
      ],
      "metadata": {
        "id": "YoBAZxXlFKSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inatll requirements"
      ],
      "metadata": {
        "id": "XgFf3sJ3-6dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "t38MJavy5Jpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "a7aLUQb76L80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "znDenAh6VDSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4hVQxhQOTpzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/NLP_HW3/Unzipped"
      ],
      "metadata": {
        "id": "qnrYYcx8UVpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! ls"
      ],
      "metadata": {
        "id": "4UQqQJaSUZhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cultural = ''\n",
        "economics = ''\n",
        "politics = ''\n",
        "sports = ''\n",
        "# with open('cultural.txt') as f:\n",
        "#   cultural = f.read()\n",
        "# with open('economics.txt') as f:\n",
        "#   economics = f.read()\n",
        "with open('processd_data.txt') as f:\n",
        "  politics = f.read()\n",
        "# with open('sports.txt') as f:\n",
        "#   sports = f.read()\n"
      ],
      "metadata": {
        "id": "sGSEERWgazac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = []\n",
        "with open('stopwords.txt') as f:\n",
        "  stopwords = f.read().split()"
      ],
      "metadata": {
        "id": "WRIQ6OZ4aw81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "QBjETHHbPn4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cultural_sents = sent_tokenize(cultural)\n",
        "# economics_sents = sent_tokenize(economics)\n",
        "politics_sents = sent_tokenize(politics)[:1000]\n",
        "# sports_sents = sent_tokenize(sports)"
      ],
      "metadata": {
        "id": "PO8mr0sybeXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politics_sents = list(set(politics_sents))"
      ],
      "metadata": {
        "id": "9jYG6EAMq3fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(politics_sents)"
      ],
      "metadata": {
        "id": "a_iZiH7WyJYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cultural_sents1 = cultural_sents[:200]\n",
        "# cultural_sents2 = cultural_sents[200:400]"
      ],
      "metadata": {
        "id": "FLFzNcSvpnTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# politics_sents = list(set(sent_tokenize(politics)))"
      ],
      "metadata": {
        "id": "v9MeJYMxJVZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pol_sents = []\n",
        "# for sent in politics_sents:\n",
        "#   if sent not in pol_sents:\n",
        "#     pol_sents.append(sent)\n"
      ],
      "metadata": {
        "id": "R9eyOv4eN6bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "all_words = []\n",
        "# all_words2 = []\n",
        "\n",
        "for i in range(len(politics_sents)):\n",
        "  all_words += word_tokenize(normalizer.normalize(politics_sents[i]))\n",
        "\n",
        "# for i in range(len(cultural_sents2)):\n",
        "#   all_words2 += word_tokenize(normalizer.normalize(cultural_sents2[i]))  \n",
        "\n",
        "# for i in range(len(economics_sents)):\n",
        "#   all_words += word_tokenize(normalizer.normalize(economics_sents[i]))\n",
        "  \n",
        "# for i in range(len(politics_sents)):\n",
        "#   all_words += word_tokenize(normalizer.normalize(politics_sents[i]))\n",
        "  \n",
        "# for i in range(len(sports_sents)):\n",
        "#   all_words += word_tokenize(normalizer.normalize(sports_sents[i]))\n",
        "  \n"
      ],
      "metadata": {
        "id": "-ntnxo0_Pgm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_words)"
      ],
      "metadata": {
        "id": "MZmGrRVWS_4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [t for t in tqdm.tqdm(all_words) if t not in stopwords]\n",
        "# words2 = [t for t in tqdm.tqdm(all_words2) if t not in stopwords]"
      ],
      "metadata": {
        "id": "y6RDf1A3aoWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "rW2La_scbviZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 4  # 4 words to the left, 4 to the right\n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(words) - CONTEXT_SIZE):\n",
        "  context = []\n",
        "  for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1, 1):\n",
        "    if j == 0:\n",
        "      continue\n",
        "    context.append(words[i+j])\n",
        "  target = words[i]\n",
        "  data.append((context, target))"
      ],
      "metadata": {
        "id": "WlWJs0A_fFGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "F2gR-U_9qLQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(CONTEXT_SIZE, len(words2) - CONTEXT_SIZE):\n",
        "  context = []\n",
        "  for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1, 1):\n",
        "    if j == 0:\n",
        "      continue\n",
        "    context.append(words2[i+j])\n",
        "  target = words2[i]\n",
        "  data.append((context, target))"
      ],
      "metadata": {
        "id": "QUqPj9ger_rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "SCO2uCP6sGu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(words)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class CBOW(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        \n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)\n",
        "\n",
        "\n",
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(20):\n",
        "  total_loss = 0\n",
        "  for context, target in data:\n",
        "    context_vector = make_context_vector(context, word_to_ix)  \n",
        "    log_probs = model(context_vector)\n",
        "    total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "  #optimize at the end of each epoch\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "6LfePQckb1QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TESTING\n",
        "context = ['محقق', 'کشف', 'تاریخ', 'افتخار', 'کشور', 'تلاش', 'باعث', 'نیست']\n",
        "context_vector = make_context_vector(context, word_to_ix)\n",
        "a = model(context_vector)\n",
        "\n",
        "#Print result\n",
        "print(f'Context: {context}\\n')\n",
        "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
      ],
      "metadata": {
        "id": "U79GnhbgckSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained CBOW"
      ],
      "metadata": {
        "id": "bGiGuMtdFRHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git "
      ],
      "metadata": {
        "id": "_M8A_1WOWIYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd fastText && pip install ."
      ],
      "metadata": {
        "id": "NdR2F5iDWMWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SKIPGRAM_MODEL_FILE_ID = '1wPnMG9_GNUVdSgbznQziQc5nMWI3QKNz'\n",
        "CBOW_MODEL_FILE_ID = '1cQP10CGV6kAwmRuESJ5RTsgHq5TveXwV'"
      ],
      "metadata": {
        "id": "tbB0lkNDWh0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id $CBOW_MODEL_FILE_ID "
      ],
      "metadata": {
        "id": "NzlLrjCzWj9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id $SKIPGRAM_MODEL_FILE_ID "
      ],
      "metadata": {
        "id": "PvUUqWYV8PGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "# loading the Model\n",
        "# model_cbow = fasttext.load_model('farsi-dedup-cbow.bin')\n",
        "model_skipgram = fasttext.load_model('farsi-dedup-skipgram.bin')"
      ],
      "metadata": {
        "id": "k85GOy87lErA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_skipgram.get_analogies('دانشمند', 'کشور', 'افتخار')"
      ],
      "metadata": {
        "id": "t2F9E6GbWwgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fasttext CBOW"
      ],
      "metadata": {
        "id": "yUP2VAPxGlaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fasttext"
      ],
      "metadata": {
        "id": "VJfrTIhP0m9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install levenshtein"
      ],
      "metadata": {
        "id": "cVV98ppjs1A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import Levenshtein"
      ],
      "metadata": {
        "id": "WlAEzPQtGwtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised('data/fil9', \"cbow\")"
      ],
      "metadata": {
        "id": "J3xiTSUxrUS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = sent_tokenize(politics)\n",
        "\n",
        "sents1 = sents[:len(sents)//2]\n",
        "sents2 = sents[len(sents)//2:]   \n",
        "\n",
        "normalizer = Normalizer()\n",
        "words = []\n",
        "with open('processd_data.txt', 'w') as f:\n",
        "  for i in range(len(sents1)):\n",
        "    words = word_tokenize(normalizer.normalize(sents1[i]))\n",
        "    words = [w for w in words if w not in [',', ':', '\"', '؛', '-', '»', 'ـ', '»']]\n",
        "    s = ' '.join(words)\n",
        "    f.write(s + '\\n')\n",
        "\n",
        "\n",
        "# with open('data.txt', 'w') as f:\n",
        "#   for sent in result:\n",
        "#     f.write(sent + '\\n')  "
      ],
      "metadata": {
        "id": "ez_Dy0Ei26Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('processd_data.txt', 'w') as f:\n",
        "  for i in range(len(sents2)):\n",
        "    words = word_tokenize(normalizer.normalize(sents2[i]))\n",
        "    words = [w for w in words if w not in [',', ':', '\"', '؛', '-', '»', 'ـ', '»']]\n",
        "    s = ' '.join(words)\n",
        "    f.write(s + '\\n')\n"
      ],
      "metadata": {
        "id": "GFfxm2jgQLjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised('processd_data.txt', model='cbow')"
      ],
      "metadata": {
        "id": "HB0hC4iiw0eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors('علوم',50)"
      ],
      "metadata": {
        "id": "RYUNvtJlzyKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_word(context, model, target): # with intersection\n",
        "  intersection = model.get_nearest_neighbors(context[0], 100)\n",
        "  for w in context[1:]:\n",
        "    neighbors = model.get_nearest_neighbors(w, 100)\n",
        "    intersection = [value for value in intersection if value in neighbors]\n",
        "  edit_dist = []\n",
        "  for w in intersection:\n",
        "    edit_dist.append(Levenshtein.distance(w, target))\n",
        "  if len(edit_dist) == 0:\n",
        "    return target\n",
        "  minimum = min(edit_dist)\n",
        "  index = edit_dist.index(minimum)\n",
        "  return intersection[index]"
      ],
      "metadata": {
        "id": "mj3VO6ZMksf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_word(context, model, target): # with union\n",
        "  union = []\n",
        "  for w in context:\n",
        "    neighbors = model.get_nearest_neighbors(w, 50)\n",
        "    union += neighbors\n",
        "  edit_dist = []\n",
        "  for w in union:\n",
        "    if w is not str:\n",
        "      continue\n",
        "    # try:\n",
        "    edit_dist.append(Levenshtein.distance(w, target))\n",
        "    # except:\n",
        "    #   print(w)\n",
        "    #   print(target)\n",
        "  if len(edit_dist) == 0:\n",
        "    return target\n",
        "  minimum = min(edit_dist)\n",
        "  index = edit_dist.index(minimum)\n",
        "  return union[index]"
      ],
      "metadata": {
        "id": "QTC-74KGx8TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'بسیاری از مباحث علوم طبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد.'\n",
        "words = word_tokenize(sent)\n",
        "\n",
        "context_size = 3 \n",
        "new_sent = words\n",
        "for i in range(context_size, len(new_sent) - context_size):\n",
        "  context = []\n",
        "  for j in range(-context_size, context_size+1, 1):\n",
        "    if j == 0:\n",
        "      continue\n",
        "    context.append(new_sent[i+j])\n",
        "  target = new_sent[i]\n",
        "  new_sent[i] = get_target_word(context, model, target)\n",
        "\n",
        "print(new_sent)"
      ],
      "metadata": {
        "id": "8O6jTCXQhxan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c\n",
        "* [Huggingface Bert Model](https://huggingface.co/HooshvareLab/bert-fa-base-uncased?text=%D8%A7%DB%8C%D9%86+%D8%AF%D8%A7%D9%86%D8%B4%D9%85%D9%86%D8%AF+%5BMASK%5D+%D8%A8%D8%A7%D8%B9%D8%AB+%D8%A7%D9%81%D8%AA%D8%AE%D8%A7%D8%B1+%D8%A7%D8%B3%D8%AA.)\n",
        "* https://github.com/language-ml/2-nlp-language-modeling/blob/main/1-Ngram-LanguageModeling-Persian.ipynb\n",
        "* ..."
      ],
      "metadata": {
        "id": "BeWfk-QVU6BO"
      }
    }
  ]
}
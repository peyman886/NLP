{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Xc2vJp_3zoS",
        "lsYjWZCTO8fp",
        "qJed9qr_QGdp",
        "3VzChF0E35j4",
        "IlJ6xBpM5HIf"
      ],
      "authorship_tag": "ABX9TyPIvMB1PcvQ/+6NO4gZnKds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peyman886/NLP/blob/main/HW3/NLP_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3 - Spell Correction\n",
        "\n",
        "* Peyman Naseri, `96100522`\n",
        "* Mobina Pournemat, `97105833`\n",
        "* Mahsa Amani, `97105769`"
      ],
      "metadata": {
        "id": "V3vFT6V84Fe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to drive folder: https://drive.google.com/drive/folders/1O4GtIxJf7UhCnEct8dzKR9LfMDn2_zNH?usp=sharing"
      ],
      "metadata": {
        "id": "rSgCsmcp4egD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add above folder to your drive (shortcut) and run this notebook inside `HW3` folder."
      ],
      "metadata": {
        "id": "qi747jdn4ktD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "3Xc2vJp_3zoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface pre-trained Bert model"
      ],
      "metadata": {
        "id": "lsYjWZCTO8fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "در این بخش تمرین و برای پیاده سازی یک ترنسفورمر، از مدل pre-trained شده استفاده می کنیم. این مدل که bert نام دارد از وبسایت huggingface که شامل مدل های pre-trained شده می باشد، لود می شود. لینک مدل در بخش مراجع آمده است.<br>\n",
        "برای شروع کار ابتدا کتابخانه های مورد نیاز را دانلود میکنیم که به ترتیب برای محاسبه فاصله ویرایشی کلمات، ترنسفورمر مورد استفاده و نیز هضم برای توکنایز کردن می باشند.<br>\n",
        "در مرحله بعد، مدل مورد استفاده و نیز توکنایزر مخصوص به آن که pre-trained نیز هستند لود می شوند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "PLa2F3gTtXkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install Levenshtein\n",
        "!pip install transformers\n",
        "!pip install hazm"
      ],
      "metadata": {
        "id": "hFeT5mhh9fUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "5788221ee1ce44f39dcd519f59999c12",
            "a76c99edcf824c38ac8af67837c1033e",
            "03c8b7daa72c48998b2075410a4b8321",
            "a75555db41fc4aa49a9dfe0ede124cc7",
            "cd3aafc64af240a3a3eac4a955155f21",
            "d35d6273e58e4e2dba8cedac677e9b65",
            "e4abf32b14a64e3aae348411e2eb1f5e",
            "03c5ee70c4464f089a8b78818aeca876",
            "ad5f74cd02a546f7a31784adc9503878",
            "43b64b7642034a68b1b21415e847d890",
            "4709c68ae03445ebb3e2ef90ed6384e4",
            "20c73f39cf444a9a91d307cedab3b291",
            "8aa25c84ce8c451e865a268ed527a50b",
            "1112a555a6934707bbbebc7ba344f26b",
            "276e998ec3434f9a8b77a97e3912bd34",
            "ee5a07d388c345abb7eff32c60104748",
            "c141309d9a8e4fa789df60ea046e1871",
            "a1f4b43b1b3b43fda3e91a280cfdece9",
            "b48c173d783246b1901f2b9dc3272a04",
            "9f116acdf3294acf9adf90445d709798",
            "b02b058f9eb143798b2bd5e0c81a7a11",
            "d38bf21af72d4953ac32036008be8cfd",
            "cc24119e2ab7419b8d9686c4de076c53",
            "9eb255bd38eb4792a7306ff1da31376f",
            "e888a022822c40b79fabe6dbd8a638d9",
            "7540f217780d4835b9dee4f58bcf448a",
            "ce5ad92d35bc4ecebaf4e277f230f071",
            "b02cf1a4b7e84bbcab987d966e083b97",
            "9fc9cd14d1d44086b76366e4154f8eec",
            "2fec1fcb6f2542b3b4be575ae6d494ab",
            "0fe2f54cea8140cfa491cecfcbb9c965",
            "d66e4a19106340b98673e7152d8ac363",
            "96adaae26a8041dd941197153433d1ec"
          ]
        },
        "id": "vZNhTvX20q_a",
        "outputId": "bb6f3ba4-5e31-452d-83c1-a9127ff4165b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/440 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5788221ee1ce44f39dcd519f59999c12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20c73f39cf444a9a91d307cedab3b291"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/624M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc24119e2ab7419b8d9686c4de076c53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "حال باید بتوانیم با استفاده از این مدل، غلط های نگارشی موجود در متن را پیدا کنیم. تابع زیر بدین منظور تعریف شده است و به این صورت عمل می کند که تک تک کلمات جمله را به ترتیب ماسک می کند و با استفاده از مدل و بسته به مقدار top_k که در ورودی تابع تعریف می شود، کلماتی که به احتمال بالا می توانند بجای کلمه ماسک شده قرار بگیرند تولید و به اندازه top_k انتخاب خواهند شد. سپس این کلمات تک تک بررسی می شوند و کلمه ای را که فاصله ویرایشی آن با کلمه ماسک شده کمتر است را به عنوان خروجی نهایی تولید می کند. پس از این که این عمل بر روی تک تک کلمات انجام گرفت، خروجی های درست از نظر مدل تولید می شوند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "Y_M3K29TxsN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import Levenshtein\n",
        "import re\n",
        "from hazm import *\n",
        "\n",
        "\n",
        "def find_possible_mistakes(inp, model, top_k=1000):\n",
        "    # inp = inp.replace(\"\\u200C\", \" \")\n",
        "    tokens = word_tokenize(inp)\n",
        "    # tokens = inp.split()\n",
        "    # tokens = tokenizer.tokenize(inp)\n",
        "    mistakes = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        # print(\"-----------\", token, \"-----------\")\n",
        "        token = token.replace(\"\\u200C\", \"\")\n",
        "        text = \" \".join(tokens[:i]) + tokenizer.mask_token + \" \".join(tokens[i+1:])\n",
        "        input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "        mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "        logits = model(**input)\n",
        "        logits = logits.logits\n",
        "        softmax = F.softmax(logits, dim = -1)\n",
        "        mask_word = softmax[0, mask_index, :]\n",
        "        tops = torch.topk(mask_word, top_k, dim = 1)[1][0]\n",
        "        \n",
        "        least_dist = float(\"inf\")\n",
        "        corrected_word = token\n",
        "        for w in tops:\n",
        "            word = tokenizer.decode([w])\n",
        "            dist = Levenshtein.distance(token, word)\n",
        "            if dist < least_dist:\n",
        "                corrected_word = word\n",
        "                least_dist = dist\n",
        "\n",
        "        if token != corrected_word:\n",
        "            for reg in re.finditer(token, inp):\n",
        "                s, e = reg.start(), reg.end()\n",
        "            mistakes.append({\"raw\": token, \"corrected\": corrected_word, \"span\": [s, e]})\n",
        "\n",
        "    return mistakes"
      ],
      "metadata": {
        "id": "hL_xhPn2_ls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input0 = \"این دانشمند تیرانی باعث افتخار است.\"\n",
        "input1 = \"پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\"\n",
        "input2 = \"بسیاری از مباحث علوم غیرطبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه‌های خاصی رجو کرد.\"\n",
        "input3 = 'اما متأسفانه به قدری ساختار سارمان سینمایی و در سطح وسیع‌تر وزارت فرهنگ و ارشاد اصلامی عقب‌مانده و ناکارآمد است که عملا جلوی بهبود هر مشکلی را می‌گیرد!'\n",
        "input4 = 'منطق جغرافیا و جئوپلیتیک همیشه ثابت است و قابل چشم‌پوسی نیست.‎'"
      ],
      "metadata": {
        "id": "_sgfbNfd9lgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained model results"
      ],
      "metadata": {
        "id": "Hz1BtP5QSbpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "result = find_possible_mistakes(input1, model, top_k=1000)\n",
        "print(input1)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNEwa1unA3qb",
        "outputId": "e270942d-e6cb-4c97-b1e5-442f6d322071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"رازی\",\n",
            "        \"corrected\": \"باز\",\n",
            "        \"span\": [\n",
            "            18,\n",
            "            22\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"کسف\",\n",
            "        \"corrected\": \"کشف\",\n",
            "        \"span\": [\n",
            "            31,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"الکل\",\n",
            "        \"corrected\": \"اول\",\n",
            "        \"span\": [\n",
            "            35,\n",
            "            39\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"تیرانی\",\n",
            "        \"corrected\": \"ایرانی\",\n",
            "        \"span\": [\n",
            "            56,\n",
            "            62\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"کور\",\n",
            "        \"corrected\": \"کشور\",\n",
            "        \"span\": [\n",
            "            84,\n",
            "            87\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "result = find_possible_mistakes(input2, model, top_k=1000)\n",
        "print(input2)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "khTioECUR39F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac83344e-5d7a-4dae-8251-7a629b83d697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه‌های خاصی رجو کرد.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"فیریک\",\n",
            "        \"corrected\": \"فیزیک\",\n",
            "        \"span\": [\n",
            "            44,\n",
            "            49\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"ابل\",\n",
            "        \"corrected\": \"قابل\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            64\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"توجیح\",\n",
            "        \"corrected\": \"توجیه\",\n",
            "        \"span\": [\n",
            "            65,\n",
            "            70\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"رجو\",\n",
            "        \"corrected\": \"رجوع\",\n",
            "        \"span\": [\n",
            "            114,\n",
            "            117\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input3, model, top_k=1000)\n",
        "print(input3)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGPoE2-l5ojw",
        "outputId": "f92d6acd-f71f-4cde-e624-7c01d9e35be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اما متأسفانه به قدری ساختار سارمان سینمایی و در سطح وسیع‌تر وزارت فرهنگ و ارشاد اصلامی عقب‌مانده و ناکارآمد است که عملا جلوی بهبود هر مشکلی را می‌گیرد!\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"متأسفانه\",\n",
            "        \"corrected\": \"متاسفانه\",\n",
            "        \"span\": [\n",
            "            4,\n",
            "            12\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"سارمان\",\n",
            "        \"corrected\": \"سازمان\",\n",
            "        \"span\": [\n",
            "            28,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"اصلامی\",\n",
            "        \"corrected\": \"اسلامی\",\n",
            "        \"span\": [\n",
            "            80,\n",
            "            86\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"عقبمانده\",\n",
            "        \"corrected\": \"نمانده\",\n",
            "        \"span\": [\n",
            "            80,\n",
            "            86\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"ناکارآمد\",\n",
            "        \"corrected\": \"ناکارامد\",\n",
            "        \"span\": [\n",
            "            99,\n",
            "            107\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input4, model, top_k=1000)\n",
        "print(input4)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2xBwpv95pA-",
        "outputId": "bdfcf12c-08ec-4bcf-fd4c-07e54929dd70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "منطق جغرافیا و جئوپلیتیک همیشه ثابت است و قابل چشم‌پوسی نیست.‎\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"جغرافیا\",\n",
            "        \"corrected\": \"جاوا\",\n",
            "        \"span\": [\n",
            "            5,\n",
            "            12\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"جئوپلیتیک\",\n",
            "        \"corrected\": \"ژيوپلیتیک\",\n",
            "        \"span\": [\n",
            "            15,\n",
            "            24\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"چشمپوسی\",\n",
            "        \"corrected\": \"چشمپوشی\",\n",
            "        \"span\": [\n",
            "            15,\n",
            "            24\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"‎\",\n",
            "        \"corrected\": \".\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            62\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "قابل ذکر است که با تغییر مقدار top_k می توان مدل را بهتر یا بدتر کرد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "fxrenVqrzkdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune with more data"
      ],
      "metadata": {
        "id": "qJed9qr_QGdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "حال میخواهیم مدل را با استفاده از داده های خودمان دوباره آموزش دهیم. برای این کار از بین 4 دیتاست موجود، از هر کدام 500 جمله اول را انتخاب میکنیم تا مدل را با آن ها آموزش دهیم. به خاطر مشکل حجم و پردازش کولب، هنگام استفاده از داده های بیشتر، کولب کرش میکرد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "KsQa01-azvXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ3FVu0LQKiD",
        "outputId": "468d4ec0-3026-4606-fd16-9b688f1b9a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP/HW3/NLP-HW3-Resources/\n",
        "\n",
        "import zipfile\n",
        "\n",
        "datasets = [\"cultural.zip\", \"economics.zip\", \"politics.zip\", \"sports.zip\"]\n",
        "data = []\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "    with zipfile.ZipFile(dataset) as zipper:\n",
        "        with zipper.open(dataset.split(\".\")[0]+'.txt') as fp:\n",
        "            data += fp.read().decode('utf-8').split('\\n')[:500]\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive/NLP/HW3/"
      ],
      "metadata": {
        "id": "HD27Ky5MUf3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4052b4a8-ced5-47ee-d24a-b3cadaea95ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1aAgsAE_TSLzzFsdBLSIvjyv20Iu-wLlq/NLP-HW3-Resources\n",
            "/content/drive/MyDrive/NLP/HW3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "برای آماده سازی دیتاست، آن را توکنایز کرده و در وکتور های 512 تایی امبد می کنیم. لیبل های این ورودی هارا بصورت رندم ماسک می کنیم و نهایتا دیتاست مورد نظر را با استفاده از این ورودی ها و خروجی ها می سازیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "PDUaq-sq02jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(data, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "# create a key for labels\n",
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "\n",
        "# create random array of floats in equal dimension to input_ids\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "# create mask array: where the random array is less than 0.15, we set true, also where it is not CLS or SEP\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
        "\n",
        "# create selection from mask_arr\n",
        "# FYI: [MASK] == 103\n",
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n",
        "    inputs.input_ids[i, selection[i]] = 103"
      ],
      "metadata": {
        "id": "1Ux81YmpEzMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "dataset = Dataset(inputs)"
      ],
      "metadata": {
        "id": "Z2i-7QC8JE99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "نهایتا مدل را با استفاده توابع آماده ترنسفورمر با تعداد ایپاک 5 و batch size مساوی 8 ترین می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "xMzEOV4v28qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='out',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    num_train_epochs=epochs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-Vo79hJgKRhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "5c66e247-7776-4fa3-a1fe-e7e2cb0786aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 2000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 31:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.022300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to out/checkpoint-500\n",
            "Configuration saved in out/checkpoint-500/config.json\n",
            "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to out/checkpoint-1000\n",
            "Configuration saved in out/checkpoint-1000/config.json\n",
            "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1250, training_loss=0.06618174839019775, metrics={'train_runtime': 1912.7121, 'train_samples_per_second': 5.228, 'train_steps_per_second': 0.654, 'total_flos': 2634182492160000.0, 'train_loss': 0.06618174839019775, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results after fine-tuning"
      ],
      "metadata": {
        "id": "Sh4W7hR3SBmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "مدل را از gpu به cpu انتقال می دهیم و نتایج را مشاهده می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "546X0pYP3Szv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model.to(\"cpu\")"
      ],
      "metadata": {
        "id": "8ZkW6W7hKg6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input1, model, top_k=2000)\n",
        "print(input1)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "RtvgcEaJSFvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6104ba6-ec42-4cdb-d80c-39e9e590facf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"تلاش\",\n",
            "        \"corrected\": \"تلال\",\n",
            "        \"span\": [\n",
            "            13,\n",
            "            17\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"رازی\",\n",
            "        \"corrected\": \"روزی\",\n",
            "        \"span\": [\n",
            "            18,\n",
            "            22\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"کسف\",\n",
            "        \"corrected\": \"کشف\",\n",
            "        \"span\": [\n",
            "            31,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"الکل\",\n",
            "        \"corrected\": \"الکا\",\n",
            "        \"span\": [\n",
            "            35,\n",
            "            39\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"تیرانی\",\n",
            "        \"corrected\": \"ایرانی\",\n",
            "        \"span\": [\n",
            "            56,\n",
            "            62\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"کور\",\n",
            "        \"corrected\": \"کشور\",\n",
            "        \"span\": [\n",
            "            84,\n",
            "            87\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes(input2, model, top_k=2000)\n",
        "print(input2)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "id": "Gm_VM6G6LDac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1416bdb3-7978-458f-bbe3-992e0ef49c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه‌های خاصی رجو کرد.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"فیریک\",\n",
            "        \"corrected\": \"فیزیک\",\n",
            "        \"span\": [\n",
            "            44,\n",
            "            49\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"مادی\",\n",
            "        \"corrected\": \"مادری\",\n",
            "        \"span\": [\n",
            "            56,\n",
            "            60\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"ابل\",\n",
            "        \"corrected\": \"قابل\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            64\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"توجیح\",\n",
            "        \"corrected\": \"توکیو\",\n",
            "        \"span\": [\n",
            "            65,\n",
            "            70\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"فلسفههای\",\n",
            "        \"corrected\": \"جلسههای\",\n",
            "        \"span\": [\n",
            "            65,\n",
            "            70\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"رجو\",\n",
            "        \"corrected\": \"رجوع\",\n",
            "        \"span\": [\n",
            "            114,\n",
            "            117\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "متاسفانه نتایج بعد از یادگیری دوباره بهتر نشده اند.\n",
        "</div>"
      ],
      "metadata": {
        "id": "WaK5BFaQ3cvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams"
      ],
      "metadata": {
        "id": "3VzChF0E35j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "یک راه دیگر برای اصلاح غلط های املایی، استفاده از مدل زبانی n-gram است. با توجه به محدودیت RAM در کولب، تنها بخشی از داده های هر فایل به مدل داده شده است.  \n",
        "در ابتدا باید روی داده پیش پردازش هایی نظیر Normalization، Tokenization، اضافه کردن کاراکتر های آغاز و پایان جمله و ... صورت گیرد. هم چنین برای جلوگیری از صفر شدن بسیاری از احتمال های n-gram ها فرایند smoothing نیز روی مدل زبانی اعمال شده است.\n",
        "</div>"
      ],
      "metadata": {
        "id": "yCMRUwKr9Gnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm\n",
        "from itertools import product\n",
        "import math\n",
        "import nltk\n",
        "import json"
      ],
      "metadata": {
        "id": "z0qoCIcs38R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oxKKDN3uUUYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP/HW3/NLP-HW3-Resources/\n",
        "\n",
        "import zipfile\n",
        "\n",
        "datasets = [\"cultural.zip\", \"economics.zip\", \"politics.zip\", \"sports.zip\"]\n",
        "data = []\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "    with zipfile.ZipFile(dataset) as zipper:\n",
        "        with zipper.open(dataset.split(\".\")[0]+'.txt') as fp:\n",
        "            data += fp.read().decode('utf-8').split('\\n')[:100000]\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive/NLP/HW3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjj1sDL8Uc8i",
        "outputId": "cb80c897-626a-4864-cc89-bd8f8db51fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1aAgsAE_TSLzzFsdBLSIvjyv20Iu-wLlq/NLP-HW3-Resources\n",
            "/content/drive/MyDrive/NLP/HW3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(\"\\n\".join(data))\n",
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usg9V5quU4e-",
        "outputId": "b2e421dd-19d9-43be-e80a-33d1333ab42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1292562"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "sentences = [normalizer.normalize(x) for x in tqdm.tqdm(sentences)]\n",
        "sentences = [word_tokenize(sent) for sent in tqdm.tqdm(sentences)]\n",
        "sentences = [' '.join(x) for x in tqdm.tqdm(sentences)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmaFxNtCVWbI",
        "outputId": "74eb9940-7c60-41c9-b87b-6c2851e57b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1292562/1292562 [01:57<00:00, 10992.08it/s]\n",
            "100%|██████████| 1292562/1292562 [00:59<00:00, 21743.79it/s]\n",
            "100%|██████████| 1292562/1292562 [00:02<00:00, 504704.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "\n",
        "    SOS = \"<s>\"\n",
        "    EOS = \"</s>\"\n",
        "    UNK = \"<UNK>\"\n",
        "    \n",
        "    def __init__(self, train_data, n, laplace=1):\n",
        "        self.n = n\n",
        "        self.vocab = dict()\n",
        "        self.laplace = laplace\n",
        "        self.tokens = self.preprocess(train_data, n)\n",
        "        self.vocab  = nltk.FreqDist(self.tokens)\n",
        "        self.model  = self._create_model()\n",
        "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
        "\n",
        "\n",
        "\n",
        "    def _smooth(self):\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
        "        n_vocab = nltk.FreqDist(n_grams)\n",
        "\n",
        "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
        "        m_vocab = nltk.FreqDist(m_grams)\n",
        "\n",
        "        def smoothed_count(n_gram, n_count):\n",
        "            m_gram = n_gram[:-1]\n",
        "            m_count = m_vocab[m_gram]\n",
        "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
        "\n",
        "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
        "\n",
        "\n",
        "\n",
        "    def _create_model(self):\n",
        "        if self.n == 1:\n",
        "            num_tokens = len(self.tokens)\n",
        "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
        "        else:\n",
        "            return self._smooth()\n",
        "\n",
        "\n",
        "\n",
        "    def _convert_oov(self, ngram):\n",
        "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
        "\n",
        "        ngram = (ngram,) if type(ngram) is str else ngram\n",
        "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
        "            if possible_known in self.model:\n",
        "                return possible_known\n",
        "\n",
        "\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        test_tokens = self.preprocess(test_data, self.n)\n",
        "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
        "        N = len(test_tokens)\n",
        "\n",
        "        known_ngrams  = [self._convert_oov(ngram) for ngram in test_ngrams]\n",
        "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
        "        \n",
        "        # for x,y in zip(known_ngrams, probabilities):\n",
        "        #     print(x,y)\n",
        "        \n",
        "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
        "\n",
        "\n",
        "\n",
        "    def _best_candidate(self, prev, k=10, without=[]):\n",
        "        \n",
        "        blacklist  = [LanguageModel.UNK] + without\n",
        "        # if len(prev) < self.n:\n",
        "        #     prev = [LanguageModel.SOS]*(self.n-1)\n",
        "\n",
        "        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==tuple(prev)))\n",
        "\n",
        "        probs = [y for x,y in candidates]\n",
        "        probs = probs/np.sum(probs)\n",
        "        words = [x for x,y in candidates]\n",
        "\n",
        "        candidates.sort(key = lambda x: -x[1])\n",
        "        return candidates[:k]\n",
        "\n",
        "\n",
        "    def preprocess(self, sentences, n):\n",
        "        sentences = self.add_sentence_tokens(sentences, n)\n",
        "        tokens = ' '.join(sentences).split()\n",
        "        tokens = self.replace_singletons(tokens)\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def add_sentence_tokens(self, sentences, n):\n",
        "        sos = ' '.join([LanguageModel.SOS] * (n-1)) if n > 1 else LanguageModel.SOS\n",
        "        return ['{} {} {}'.format(sos, s, LanguageModel.EOS) for s in sentences]\n",
        "\n",
        "\n",
        "    def replace_singletons(self, tokens):\n",
        "        if len(self.vocab) == 0:\n",
        "            self.vocab = nltk.FreqDist(tokens)\n",
        "        return [token if self.vocab[token] > 1 else LanguageModel.UNK for token in tokens]"
      ],
      "metadata": {
        "id": "DcWXx4HiVeTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "پس از بررسی مقادیر مختلف برای n و پارامتر smoothing بهترین مقادیر برای آن ها 3 و 1 به دست آمد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "bcQStviXAEkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_model = LanguageModel(sentences, 3, 1)"
      ],
      "metadata": {
        "id": "PoQY-vGSVg6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test using edit distance"
      ],
      "metadata": {
        "id": "AXfgJbjmwScQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "برای تست کردن مدل زبانی 3-gram مان، به ابتدا و انتهای جمله کاراکتر های شروع و پایان را اضافه می کنیم و سپس به ازای هر کلمه از جمله ، دو کلمه قبلی را به مدل زبانی میدهیم و k تا بهترین گزینه برای آن جایگاه را از آن دریافت می کنیم. سپس edit-distance کاندید ها با کلمه مورد نظر را حساب می کنیم و کاندیدی که کمترین فاصله را داشته باشد را به عنوان اصلاح شده آن کلمه در نظر می گیریم و جمله را اصلاح می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "LQ4rLLq6Af18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import probability\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import Levenshtein\n",
        "import re\n",
        "from hazm import *\n",
        "\n",
        "\n",
        "def find_possible_mistakes_ngram(inp, top_k=20):\n",
        "    mistakes = []\n",
        "    tokens = word_tokenize(inp)\n",
        "    tokens = 2 * [\"<s>\"] + tokens\n",
        "\n",
        "    for i in range(len(tokens)-2):\n",
        "        tops = language_model._best_candidate([tokens[i], tokens[i+1]], top_k)\n",
        "\n",
        "        least_dist = float(\"inf\")\n",
        "        corrected_word = tokens[i+2]\n",
        "\n",
        "        for word, prob in tops:\n",
        "            dist = Levenshtein.distance(tokens[i+2], word)\n",
        "            if dist < least_dist:\n",
        "                corrected_word = word\n",
        "                least_dist = dist\n",
        "\n",
        "        if tokens[i+2] != corrected_word:\n",
        "            for reg in re.finditer(tokens[i+2], inp):\n",
        "                s, e = reg.start(), reg.end()\n",
        "            mistakes.append({\"raw\": tokens[i+2], \"corrected\": corrected_word, \"span\": [s, e]})\n",
        "\n",
        "    return mistakes"
      ],
      "metadata": {
        "id": "N1hR_pSTWCi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input1, top_k=10)\n",
        "print(input1)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWQYDvyDjex0",
        "outputId": "fd884dd9-f5fd-40ae-ca90-e2d4c45ddde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"پس\",\n",
            "        \"corrected\": \"در\",\n",
            "        \"span\": [\n",
            "            0,\n",
            "            2\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"سال‌ها\",\n",
            "        \"corrected\": \"این\",\n",
            "        \"span\": [\n",
            "            6,\n",
            "            12\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"تلاش\",\n",
            "        \"corrected\": \"پیش\",\n",
            "        \"span\": [\n",
            "            13,\n",
            "            17\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"رازی\",\n",
            "        \"corrected\": \"برای\",\n",
            "        \"span\": [\n",
            "            18,\n",
            "            22\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"کسف\",\n",
            "        \"corrected\": \"کسب\",\n",
            "        \"span\": [\n",
            "            31,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"دانشمند\",\n",
            "        \"corrected\": \"کارشناس\",\n",
            "        \"span\": [\n",
            "            48,\n",
            "            55\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"تیرانی\",\n",
            "        \"corrected\": \"در\",\n",
            "        \"span\": [\n",
            "            56,\n",
            "            62\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"در\",\n",
            "        \"corrected\": \"من\",\n",
            "        \"span\": [\n",
            "            75,\n",
            "            77\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"تاریخ\",\n",
            "        \"corrected\": \"کنارش\",\n",
            "        \"span\": [\n",
            "            78,\n",
            "            83\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"کور\",\n",
            "        \"corrected\": \"هنر\",\n",
            "        \"span\": [\n",
            "            84,\n",
            "            87\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input2, top_k=100)\n",
        "print(input2)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMexNA1yV-Xp",
        "outputId": "f8143697-a5ae-4292-8bda-d81eca246811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه‌های خاصی رجو کرد.\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"مباحث\",\n",
            "        \"corrected\": \"موارد\",\n",
            "        \"span\": [\n",
            "            10,\n",
            "            15\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"علوم\",\n",
            "        \"corrected\": \"علمی\",\n",
            "        \"span\": [\n",
            "            16,\n",
            "            20\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"فیریک\",\n",
            "        \"corrected\": \"یک\",\n",
            "        \"span\": [\n",
            "            44,\n",
            "            49\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"ابل\",\n",
            "        \"corrected\": \"به\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            64\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"یادگیری\",\n",
            "        \"corrected\": \"جلوگیری\",\n",
            "        \"span\": [\n",
            "            83,\n",
            "            90\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"باید\",\n",
            "        \"corrected\": \"این\",\n",
            "        \"span\": [\n",
            "            91,\n",
            "            95\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"به\",\n",
            "        \"corrected\": \"صبر\",\n",
            "        \"span\": [\n",
            "            96,\n",
            "            98\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"فلسفه‌های\",\n",
            "        \"corrected\": \"گونه‌ای\",\n",
            "        \"span\": [\n",
            "            99,\n",
            "            108\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"خاصی\",\n",
            "        \"corrected\": \"مضاف\",\n",
            "        \"span\": [\n",
            "            109,\n",
            "            113\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"رجو\",\n",
            "        \"corrected\": \"برای\",\n",
            "        \"span\": [\n",
            "            114,\n",
            "            117\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "همان طور که مشاهده می کنید مدل زبانی روی جملات داده تست خوب کار نمی کند. دلیل این امر این است که داده کافی برای ترین نداشته و هم چنین به طور کلی مدل زبانی یک گزینه ایده آل برای چنین مسئله ای نیست و در مقایسه با مدل های نظیر LSTM خیلی ضعیف تر کار می کند. ولی در ادامه جملاتی مشابه با داده ترین را به مدل داده ایم و می بینیم که عملکرد آن بسیار بهتر از حالت قبل است و نشان می دهد که کمبود داده یادگیری عامل مهمی در نتایج بد مثال های قبل بوده است.\n",
        "</div>"
      ],
      "metadata": {
        "id": "mdGbhEwACJnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input3, top_k=100)\n",
        "print(input3)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXSxM_Bx1-S9",
        "outputId": "3a82723a-2f45-4134-b151-f28f465ef065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اما متأسفانه به قدری ساختار سارمان سینمایی و در سطح وسیع‌تر وزارت فرهنگ و ارشاد اصلامی عقب‌مانده و ناکارآمد است که عملا جلوی بهبود هر مشکلی را می‌گیرد!\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"سارمان\",\n",
            "        \"corrected\": \"سازمان\",\n",
            "        \"span\": [\n",
            "            28,\n",
            "            34\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"اصلامی\",\n",
            "        \"corrected\": \"اسلامی\",\n",
            "        \"span\": [\n",
            "            80,\n",
            "            86\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"عملا\",\n",
            "        \"corrected\": \"ما\",\n",
            "        \"span\": [\n",
            "            115,\n",
            "            119\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_possible_mistakes_ngram(input4, top_k=100)\n",
        "print(input4)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False, skipkeys=True,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eayZDjuC1-6v",
        "outputId": "b2290f82-abbd-4718-808e-43a400b3bbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "منطق جغرافیا و جئوپلیتیک همیشه ثابت است و قابل چشم‌پوسی نیست.‎\n",
            "[\n",
            "    {\n",
            "        \"raw\": \"منطق\",\n",
            "        \"corrected\": \"من\",\n",
            "        \"span\": [\n",
            "            0,\n",
            "            4\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"جئوپلیتیک\",\n",
            "        \"corrected\": \"ژئوپلیتیک\",\n",
            "        \"span\": [\n",
            "            15,\n",
            "            24\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"قابل\",\n",
            "        \"corrected\": \"این\",\n",
            "        \"span\": [\n",
            "            42,\n",
            "            46\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"چشم‌پوسی\",\n",
            "        \"corrected\": \"چشم‌پوشی\",\n",
            "        \"span\": [\n",
            "            47,\n",
            "            55\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"raw\": \"‎\",\n",
            "        \"corrected\": \"»\",\n",
            "        \"span\": [\n",
            "            61,\n",
            "            62\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBOW"
      ],
      "metadata": {
        "id": "IlJ6xBpM5HIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "علاوه بر مدل های بالا، سعی کردیم از مدل های دیگری نیز استفاده کنیم تا نتایج را بهبود ببخشیم ولی به دلیل کمبود منابع نتیجه مطلوب حاصل نشد.\n",
        "</div>"
      ],
      "metadata": {
        "id": "qd2WbPjxDNgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "در ادامه مدل CBOW را مشاهده می کنید که به کار گرفته شد ولی محدودیت حافظه در colab، اجازه ترین کردن مدل با داده کافی را نمیداد و به همین دلیل امبدینگ های حاصل information لازم را دارا نبودند و نتیجه مطلوب حاصل نمیشد.\n",
        "در ادامه از مدل CBOW از قبل ترین شده روی داده های فارسی نیز به کار گرفته شد ولی باز هم نتایج مطلوب نبود و در نهایت از آن استفاده نکردیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "93hM1369DnXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW from Scratch"
      ],
      "metadata": {
        "id": "YoBAZxXlFKSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inatll requirements"
      ],
      "metadata": {
        "id": "XgFf3sJ3-6dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2wDL6Bx-_3O",
        "outputId": "d2296af2-4036-419c-b030-f809bd2dba7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "t38MJavy5Jpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "a7aLUQb76L80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "znDenAh6VDSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hVQxhQOTpzN",
        "outputId": "ba78b191-2ce3-46d6-a55a-c3e7320ac3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP_HW3/Unzipped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnrYYcx8UVpm",
        "outputId": "d1266b6e-ac6f-4de6-a11b-e3971f609591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_HW3/Unzipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! ls"
      ],
      "metadata": {
        "id": "4UQqQJaSUZhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cultural = ''\n",
        "economics = ''\n",
        "politics = ''\n",
        "sports = ''\n",
        "# with open('cultural.txt') as f:\n",
        "#   cultural = f.read()\n",
        "# with open('economics.txt') as f:\n",
        "#   economics = f.read()\n",
        "with open('processd_data.txt') as f:\n",
        "  politics = f.read()\n",
        "# with open('sports.txt') as f:\n",
        "#   sports = f.read()\n"
      ],
      "metadata": {
        "id": "sGSEERWgazac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = []\n",
        "with open('stopwords.txt') as f:\n",
        "  stopwords = f.read().split()"
      ],
      "metadata": {
        "id": "WRIQ6OZ4aw81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "QBjETHHbPn4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cultural_sents = sent_tokenize(cultural)\n",
        "# economics_sents = sent_tokenize(economics)\n",
        "politics_sents = sent_tokenize(politics)[:1000]\n",
        "# sports_sents = sent_tokenize(sports)"
      ],
      "metadata": {
        "id": "PO8mr0sybeXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politics_sents = list(set(politics_sents))"
      ],
      "metadata": {
        "id": "9jYG6EAMq3fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(politics_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_iZiH7WyJYG",
        "outputId": "8a087a05-1f91-4aef-acf2-987229375eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cultural_sents1 = cultural_sents[:200]\n",
        "# cultural_sents2 = cultural_sents[200:400]"
      ],
      "metadata": {
        "id": "FLFzNcSvpnTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# politics_sents = list(set(sent_tokenize(politics)))"
      ],
      "metadata": {
        "id": "v9MeJYMxJVZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pol_sents = []\n",
        "# for sent in politics_sents:\n",
        "#   if sent not in pol_sents:\n",
        "#     pol_sents.append(sent)\n"
      ],
      "metadata": {
        "id": "R9eyOv4eN6bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "all_words = []\n",
        "# all_words2 = []\n",
        "\n",
        "for i in range(len(politics_sents)):\n",
        "  all_words += word_tokenize(normalizer.normalize(politics_sents[i]))\n",
        "\n",
        "# for i in range(len(cultural_sents2)):\n",
        "#   all_words2 += word_tokenize(normalizer.normalize(cultural_sents2[i]))  \n",
        "\n",
        "# for i in range(len(economics_sents)):\n",
        "#   all_words += word_tokenize(normalizer.normalize(economics_sents[i]))\n",
        "  \n",
        "# for i in range(len(politics_sents)):\n",
        "#   all_words += word_tokenize(normalizer.normalize(politics_sents[i]))\n",
        "  \n",
        "# for i in range(len(sports_sents)):\n",
        "#   all_words += word_tokenize(normalizer.normalize(sports_sents[i]))\n",
        "  \n"
      ],
      "metadata": {
        "id": "-ntnxo0_Pgm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZmGrRVWS_4a",
        "outputId": "ce28b18c-506b-4d91-b807-9dce0f5c33d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29627"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [t for t in tqdm.tqdm(all_words) if t not in stopwords]\n",
        "# words2 = [t for t in tqdm.tqdm(all_words2) if t not in stopwords]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6RDf1A3aoWq",
        "outputId": "3125a0b4-0038-4922-f87c-a7eee8a1361d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29627/29627 [00:00<00:00, 1345583.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "rW2La_scbviZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 4  # 4 words to the left, 4 to the right\n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(words) - CONTEXT_SIZE):\n",
        "  context = []\n",
        "  for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1, 1):\n",
        "    if j == 0:\n",
        "      continue\n",
        "    context.append(words[i+j])\n",
        "  target = words[i]\n",
        "  data.append((context, target))"
      ],
      "metadata": {
        "id": "WlWJs0A_fFGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2gR-U_9qLQC",
        "outputId": "63000766-b9d6-450b-c563-450b59a8605a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21126"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(CONTEXT_SIZE, len(words2) - CONTEXT_SIZE):\n",
        "  context = []\n",
        "  for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1, 1):\n",
        "    if j == 0:\n",
        "      continue\n",
        "    context.append(words2[i+j])\n",
        "  target = words2[i]\n",
        "  data.append((context, target))"
      ],
      "metadata": {
        "id": "QUqPj9ger_rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCO2uCP6sGu-",
        "outputId": "cd0eb7e5-efb2-4eb8-a6e3-da4f36851906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10126"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(words)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class CBOW(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        \n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)\n",
        "\n",
        "\n",
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(20):\n",
        "  total_loss = 0\n",
        "  for context, target in data:\n",
        "    context_vector = make_context_vector(context, word_to_ix)  \n",
        "    log_probs = model(context_vector)\n",
        "    total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "  #optimize at the end of each epoch\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "6LfePQckb1QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TESTING\n",
        "context = ['محقق', 'کشف', 'تاریخ', 'افتخار', 'کشور', 'تلاش', 'باعث', 'نیست']\n",
        "context_vector = make_context_vector(context, word_to_ix)\n",
        "a = model(context_vector)\n",
        "\n",
        "#Print result\n",
        "print(f'Context: {context}\\n')\n",
        "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
      ],
      "metadata": {
        "id": "U79GnhbgckSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45b7aad-72ca-428d-a0df-c15b6c84ce58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: ['محقق', 'کشف', 'تاریخ', 'افتخار', 'کشور', 'تلاش', 'باعث', 'نیست']\n",
            "\n",
            "Prediction: توفانی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained CBOW"
      ],
      "metadata": {
        "id": "bGiGuMtdFRHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M8A_1WOWIYk",
        "outputId": "484cdc50-21e5-4446-ffbb-d59ed1afcb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 3930 (delta 29), reused 70 (delta 29), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.33 MiB | 32.07 MiB/s, done.\n",
            "Resolving deltas: 100% (2446/2446), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd fastText && pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdR2F5iDWMWE",
        "outputId": "68e2a8ad-b0a1-454f-f52d-71a13d634320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fastText\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3137162 sha256=1156b48203ba4a46f94179495d373a44a956809cbc4676556ae9fa0ed79f103b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9smi8ed3/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SKIPGRAM_MODEL_FILE_ID = '1wPnMG9_GNUVdSgbznQziQc5nMWI3QKNz'\n",
        "CBOW_MODEL_FILE_ID = '1cQP10CGV6kAwmRuESJ5RTsgHq5TveXwV'"
      ],
      "metadata": {
        "id": "tbB0lkNDWh0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id $CBOW_MODEL_FILE_ID "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzlLrjCzWj9l",
        "outputId": "c6da09f8-198f-4316-8ef3-6ca340817192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cQP10CGV6kAwmRuESJ5RTsgHq5TveXwV\n",
            "To: /content/farsi-dedup-cbow.bin\n",
            "100% 4.37G/4.37G [00:36<00:00, 119MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id $SKIPGRAM_MODEL_FILE_ID "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvUUqWYV8PGc",
        "outputId": "3b527023-253c-49e8-d3e9-d1abc97f5778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wPnMG9_GNUVdSgbznQziQc5nMWI3QKNz\n",
            "To: /content/farsi-dedup-skipgram.bin\n",
            "100% 4.37G/4.37G [00:35<00:00, 124MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "# loading the Model\n",
        "# model_cbow = fasttext.load_model('farsi-dedup-cbow.bin')\n",
        "model_skipgram = fasttext.load_model('farsi-dedup-skipgram.bin')"
      ],
      "metadata": {
        "id": "k85GOy87lErA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_skipgram.get_analogies('دانشمند', 'کشور', 'افتخار')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2F9E6GbWwgw",
        "outputId": "c05de487-ef83-4a6a-f53b-1842a84d4a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.670722246170044, 'دانشمندم.'),\n",
              " (0.6568266749382019, 'دانشمندم'),\n",
              " (0.6276395916938782, 'مخترع.'),\n",
              " (0.6273905634880066, 'دانشمندی'),\n",
              " (0.6188674569129944, 'دانشمندم،'),\n",
              " (0.6125983595848083, 'دانشمندو'),\n",
              " (0.6124367713928223, 'افتخارلی'),\n",
              " (0.5998932123184204, 'افتخارکن'),\n",
              " (0.5969136357307434, 'ستودنش'),\n",
              " (0.5944597721099854, 'پروفسورر')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fasttext CBOW"
      ],
      "metadata": {
        "id": "yUP2VAPxGlaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fasttext"
      ],
      "metadata": {
        "id": "VJfrTIhP0m9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install levenshtein"
      ],
      "metadata": {
        "id": "cVV98ppjs1A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe8b226-28e1-4409-e4f3-05a879da0e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: levenshtein in /usr/local/lib/python3.7/dist-packages (0.18.1)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from levenshtein) (2.0.11)\n",
            "Requirement already satisfied: jarowinkler<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from rapidfuzz<3.0.0,>=2.0.1->levenshtein) (1.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import Levenshtein"
      ],
      "metadata": {
        "id": "WlAEzPQtGwtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised('data/fil9', \"cbow\")"
      ],
      "metadata": {
        "id": "J3xiTSUxrUS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = sent_tokenize(politics)\n",
        "\n",
        "sents1 = sents[:len(sents)//2]\n",
        "sents2 = sents[len(sents)//2:]   \n",
        "\n",
        "normalizer = Normalizer()\n",
        "words = []\n",
        "with open('processd_data.txt', 'w') as f:\n",
        "  for i in range(len(sents1)):\n",
        "    words = word_tokenize(normalizer.normalize(sents1[i]))\n",
        "    words = [w for w in words if w not in [',', ':', '\"', '؛', '-', '»', 'ـ', '»']]\n",
        "    s = ' '.join(words)\n",
        "    f.write(s + '\\n')\n",
        "\n",
        "\n",
        "# with open('data.txt', 'w') as f:\n",
        "#   for sent in result:\n",
        "#     f.write(sent + '\\n')  "
      ],
      "metadata": {
        "id": "ez_Dy0Ei26Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('processd_data.txt', 'w') as f:\n",
        "  for i in range(len(sents2)):\n",
        "    words = word_tokenize(normalizer.normalize(sents2[i]))\n",
        "    words = [w for w in words if w not in [',', ':', '\"', '؛', '-', '»', 'ـ', '»']]\n",
        "    s = ' '.join(words)\n",
        "    f.write(s + '\\n')\n"
      ],
      "metadata": {
        "id": "GFfxm2jgQLjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised('processd_data.txt', model='cbow')"
      ],
      "metadata": {
        "id": "HB0hC4iiw0eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors('علوم',50)"
      ],
      "metadata": {
        "id": "RYUNvtJlzyKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_word(context, model, target): # with intersection\n",
        "  intersection = model.get_nearest_neighbors(context[0], 100)\n",
        "  for w in context[1:]:\n",
        "    neighbors = model.get_nearest_neighbors(w, 100)\n",
        "    intersection = [value for value in intersection if value in neighbors]\n",
        "  edit_dist = []\n",
        "  for w in intersection:\n",
        "    edit_dist.append(Levenshtein.distance(w, target))\n",
        "  if len(edit_dist) == 0:\n",
        "    return target\n",
        "  minimum = min(edit_dist)\n",
        "  index = edit_dist.index(minimum)\n",
        "  return intersection[index]"
      ],
      "metadata": {
        "id": "mj3VO6ZMksf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_word(context, model, target): # with union\n",
        "  union = []\n",
        "  for w in context:\n",
        "    neighbors = model.get_nearest_neighbors(w, 50)\n",
        "    union += neighbors\n",
        "  edit_dist = []\n",
        "  for w in union:\n",
        "    if w is not str:\n",
        "      continue\n",
        "    # try:\n",
        "    edit_dist.append(Levenshtein.distance(w, target))\n",
        "    # except:\n",
        "    #   print(w)\n",
        "    #   print(target)\n",
        "  if len(edit_dist) == 0:\n",
        "    return target\n",
        "  minimum = min(edit_dist)\n",
        "  index = edit_dist.index(minimum)\n",
        "  return union[index]"
      ],
      "metadata": {
        "id": "QTC-74KGx8TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'بسیاری از مباحث علوم طبیعی با استفاده از فیریک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد.'\n",
        "words = word_tokenize(sent)\n",
        "\n",
        "context_size = 3 \n",
        "new_sent = words\n",
        "for i in range(context_size, len(new_sent) - context_size):\n",
        "  context = []\n",
        "  for j in range(-context_size, context_size+1, 1):\n",
        "    if j == 0:\n",
        "      continue\n",
        "    context.append(new_sent[i+j])\n",
        "  target = new_sent[i]\n",
        "  new_sent[i] = get_target_word(context, model, target)\n",
        "\n",
        "print(new_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O6jTCXQhxan",
        "outputId": "8a970a11-ae12-4654-a2ca-b8b5e8497e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['بسیاری', 'از', 'مباحث', 'علوم', 'طبیعی', 'با', 'استفاده', 'از', 'فیریک', 'دنیای', 'مادی', 'ابل', 'توجیح', 'نیست', 'و', 'برای', 'یادگیری', 'باید', 'به', 'فلسفه', 'های', 'خاصی', 'رجو', 'کرد', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c\n",
        "* [Huggingface Bert Model](https://huggingface.co/HooshvareLab/bert-fa-base-uncased?text=%D8%A7%DB%8C%D9%86+%D8%AF%D8%A7%D9%86%D8%B4%D9%85%D9%86%D8%AF+%5BMASK%5D+%D8%A8%D8%A7%D8%B9%D8%AB+%D8%A7%D9%81%D8%AA%D8%AE%D8%A7%D8%B1+%D8%A7%D8%B3%D8%AA.)\n",
        "* https://github.com/language-ml/2-nlp-language-modeling/blob/main/1-Ngram-LanguageModeling-Persian.ipynb\n",
        "* ..."
      ],
      "metadata": {
        "id": "BeWfk-QVU6BO"
      }
    }
  ]
}
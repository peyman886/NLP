{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Equivalent Words.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "\n",
        "import re\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xg-GwWTk3rIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SynWords:\n",
        "    def __init__(self):\n",
        "        # load trained model and tokenizer:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
        "        self.model = AutoModelForMaskedLM.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
        "        self.syn_dict = {}\n",
        "        self.load_syn_words()\n",
        "\n",
        "    def load_syn_words(self):\n",
        "        #load synonym dictionary and save synonyms in self.syn_dict:\n",
        "        file1 = open('Farhang_Motaradef-Motazad.txt', 'r', encoding=\"utf8\")\n",
        "        lines = file1.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            word, s = line.split('&')[0].split(':')\n",
        "            s = re.sub('\\d', '', s)\n",
        "            s = re.sub('\\n', '', s)\n",
        "            s = re.sub('\\u200c', '', s)\n",
        "            syns = s.split('،')\n",
        "            self.syn_dict[word.strip()] = syns\n",
        "\n",
        "\n",
        "    def find_equivalent_words(self, text,word ,topN=2):\n",
        "        output = []\n",
        "\n",
        "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
        "        tokenized_text = self.tokenizer.tokenize(text)\n",
        "\n",
        "        if self.tokenizer.tokenize(word)[0] not in tokenized_text:\n",
        "            output = 'معادلی پیدا نشد'\n",
        "            return output\n",
        "\n",
        "        # get contexualized embedding of query word if it 's in syn_dict and not empty:\n",
        "        word_index = tokenized_text.index(self.tokenizer.tokenize(word)[0])\n",
        "        model_output = self.model(**encoded_input)\n",
        "        word_embedding1 = model_output[0][0][word_index]\n",
        "\n",
        "        if word not in self.syn_dict:\n",
        "            output = 'معادلی پیدا نشد'\n",
        "            return output\n",
        "\n",
        "        syns = self.syn_dict[word]\n",
        "\n",
        "        similarities = []\n",
        "        for syn in syns:\n",
        "            syn = syn.strip()\n",
        "\n",
        "            #replace condidate synonym in text and get contexualized embedding:\n",
        "            text2 = text.replace(word, syn)\n",
        "            encoded_input = self.tokenizer(text2, return_tensors='pt')\n",
        "            tokenized_text = self.tokenizer.tokenize(text2)\n",
        "\n",
        "\n",
        "            if self.tokenizer.tokenize(syn)[0] not in tokenized_text:\n",
        "                continue\n",
        "\n",
        "            if syn in tokenized_text:\n",
        "                word_index = tokenized_text.index(self.tokenizer.tokenize(syn)[0])\n",
        "                model_output = self.model(**encoded_input)\n",
        "                word_embedding = model_output[0][0][word_index]\n",
        "\n",
        "                #calculate cosine similarity between base embedding and all condidates embeddings:\n",
        "                cos_dist = float(1 - cosine(word_embedding.detach().numpy(), word_embedding1.detach().numpy()))\n",
        "                print(syn,cos_dist)\n",
        "                similarities.append([syn, cos_dist])\n",
        "\n",
        "        for _ in range(topN):\n",
        "            # return words of topN most similar embeddings to base:\n",
        "            i = np.argmax(np.array(similarities)[:, 1].astype(float))\n",
        "            syn_i = similarities.pop(i)[0]\n",
        "            output.append(syn_i)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "T7wKzBS8iB8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synWords = SynWords()"
      ],
      "metadata": {
        "id": "lAHYkFDGiB6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'تمام هستی‌‌‌ ‌‌‌خود را در راه دفاع از سرزمین فدا کرد.'\n",
        "word = 'هستی'"
      ],
      "metadata": {
        "id": "K3EyZ2aFpE36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syns = synWords.find_equivalent_words(text,word)"
      ],
      "metadata": {
        "id": "KOSpJqVAiB4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mgUTp0MwiBvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
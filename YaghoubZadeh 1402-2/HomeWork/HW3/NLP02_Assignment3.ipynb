{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y7lz1DieBprm",
        "EpnRNceQBM-o",
        "r7lMCBOiYNqm",
        "kJO91j_qb87R"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment #03 - Exploring Generative Language Models\n",
        "### Natural Language Processing, Khatam University, Winter 1402\n",
        "\n",
        "- Instructor: Dr. Yadollah Yaghoobzadeh\n",
        "- Notebook Prepared By: Mahdi Zakizadeh\n",
        "\n",
        "In this assignment, we delve into the workings of generative language models, focusing on the decoding process, the intricacies of translation tasks, and the visualization of attention mechanisms. Through hands-on exercises, you'll gain insights into the capabilities and inner workings of state-of-the-art NLP models.\n",
        "\n",
        "**Contact**: Should you have any questions, feel free to email me at mahdizakizadeh.me@gmail.com or contact me on Telegram [@mahdizakizadeh](http://t.me/mahdizakizadeh).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "bFg5APF77dnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "- **Assignment Due:** <b><font color='red'>1402.09.05</font></b> 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in <font color=\"green\">**bold green**</font>, and the coding parts you need to implement are denoted by:\n",
        "\n",
        "```\n",
        "## Your code begins ##\n",
        "...\n",
        "## Your code ends ##\n",
        "```\n",
        "\n",
        "for a block of codes and\n",
        "\n",
        "```\n",
        "\"\"\" Implement this \"\"\"\n",
        "```\n",
        "\n",
        "for inline codes.\n",
        "\n",
        "- Collaboration and discussion within groups are encouraged for this assignment. However, **each student must complete all questions individually**. Should our matching system detect any form of copying, you will face the associated consequences.\n",
        "- In the spirit of academic integrity, if you use any form of large language models (LLMs) such as ChatGPT, Bing, Claude, etc., **you must provide detailed conversation logs and prompts used**. Responsible use of these systems will not impact your scores negatively. However, failure to adequately cite the use of these systems, if detected, can negatively affect your scores.\n",
        "- If you encounter any challenges or have questions, don't hesitate to reach out.\n",
        "- You can double click on collapsed code cells to expand them.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "WXJR2jO19m-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Welcome to Assignment #03 in our Natural Language Processing course. In this journey, we're going to dive deep into the fascinating world of generative language models, exploring their capabilities and understanding their inner workings.\n",
        "\n",
        "Generative models have been at the forefront of recent advances in NLP, offering remarkable abilities to generate text, translate languages, and even answer complex questions. They've opened doors to new possibilities, from chatbots that can converse like humans to systems that can write poetic verses or translate between languages with astonishing accuracy.\n",
        "\n",
        "In this assignment, we will focus on three key areas:\n",
        "1. **Decoding Mechanisms**: We'll start by unraveling the mysteries of how generative models produce text. You'll get hands-on experience with decoding strategies like beam search, learning to tweak them to see how they influence the generated text.\n",
        "2. **Translation Tasks**: Next, we'll delve into the translation capabilities of models, particularly the powerful encoder-decoder structures. You'll experiment with these models, comparing their effectiveness in translating languages.\n",
        "3. **Attention Visualization**: Finally, we'll peek into the \"brain\" of these models by visualizing their attention mechanisms. This will give us a glimpse into how these models focus on different parts of the input to generate coherent and contextually relevant responses.\n",
        "\n",
        "Throughout this assignment, you'll gain not only theoretical knowledge but also practical skills in implementing and manipulating these advanced NLP models. We encourage you to experiment, explore, and even make mistakes - that's the best way to learn!\n",
        "\n",
        "Remember, this is not just about getting to the right answers; it's about appreciating the elegance of these models and understanding their capabilities and limitations. So, let's get started on this exciting journey into the world of generative language models! 🚀\n",
        "\n",
        "Have fun exploring!"
      ],
      "metadata": {
        "id": "y7lz1DieBprm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment Setup\n",
        "# Note: Do NOT make changes to this block.\n",
        "# ----------------\n",
        "%pip install transformers datasets sentencepiece\n",
        "\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "  warnings.warn(\"CUDA is not available. GPU processing can't be performed.\")\n",
        "\n",
        "# ----------------"
      ],
      "metadata": {
        "id": "QiV5zaWlDgIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf654607-a712-4f24-9a1c-849681bc97bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "xvS6TDOKQ3Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Beam Search\n",
        "\n",
        "The beam search algorithm is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. It's like a breadth-first search that prunes away branches that are less likely to lead to the optimal solution. In the context of language models, it's a technique used to generate text where at each step, instead of considering all possible next tokens, only a specified number of the most probable next tokens (defined by the beam width `k`) are kept. This approach balances between the breadth of the search and computational efficiency, making it possible to generate more coherent and contextually appropriate sequences than a greedy search, which only considers the single best next token at each step.\n"
      ],
      "metadata": {
        "id": "EpnRNceQBM-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step by Step Guide\n",
        "\n",
        "Here’s what you need to implement:\n",
        "\n",
        "1. **Initialization**: Set up your initial beam containing the tokenized input and an initial score.\n",
        "\n",
        "2. **Beam Search Loop**:\n",
        "   - For each step until `max_length`, expand each sequence in the current beam to all possible next tokens.\n",
        "   - Calculate the score for each new sequence. This score is typically the product of the probabilities (often converted from logit predictions for numerical stability) of all tokens in the sequence.\n",
        "\n",
        "3. **Probability Calculation**:\n",
        "   - Implement softmax on the logits to get the probabilities for each token in the vocabulary.\n",
        "\n",
        "4. **Sequence Expansion**:\n",
        "   - For each sequence in your beam, create new sequences by appending each possible next token.\n",
        "   - Calculate the new score for each sequence.\n",
        "\n",
        "5. **Candidate Selection**:\n",
        "   - Combine all new sequences from all beam sequences and sort them by their scores.\n",
        "   - Select the top `k` sequences with the highest scores to form the new beam.\n",
        "\n",
        "6. **Sequence Decoding**:\n",
        "   - Once you reach the `max_length` or all sequences in the beam end with an end-of-sequence token, decode the sequences in the beam to strings.\n",
        "\n",
        "7. **Output**:\n",
        "   - Return the decoded sequences along with their final scores.\n",
        "\n",
        "Remember, the quality of the generated text depends on the proper implementation of each step. Take your time to think through the logic, and don't hesitate to ask questions if something doesn't quite make sense. Good luck!\n",
        "\n",
        "<font color=\"red\">Warning: You are not allowed to use predefined implementations of the beam search function from libraries or frameworks!</font>"
      ],
      "metadata": {
        "id": "QE2uGtYrJMx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Q1: Implementing the Vanilla Beam Search Decoder\n",
        "\n",
        "# def beam_search_decoder(model, tokenizer, text, k, max_length):\n",
        "#   \"\"\"\n",
        "#   Perform beam search decoding for the given model and a starting input text.\n",
        "\n",
        "#   Args:\n",
        "#   model (PreTrainedModel): The language model to use for generating predictions.\n",
        "#   tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
        "#   text (str): The input text to start the generation.\n",
        "#   k (int): The beam width; number of sequences to keep at each step.\n",
        "#   max_length (int): The maximum length of the generated sequence.\n",
        "\n",
        "#   Returns:\n",
        "#   list of tuples: A list where each tuple contains a generated sequence and its score.\n",
        "#   \"\"\"\n",
        "\n",
        "#   input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "#   sequences = [(input_ids, 1.0)]\n",
        "\n",
        "#   ## Your code begins ##\n",
        "#   for _ in range(max_length - input_ids.size(1)):\n",
        "#     all_candidates = list()\n",
        "\n",
        "#     for seq, score in sequences:\n",
        "\n",
        "#       probabilities = None\n",
        "\n",
        "#       for token_id in range(len(probabilities)):\n",
        "#         pass\n",
        "\n",
        "#       sequences = None\n",
        "#   ## Your code ends ##\n",
        "\n",
        "#   decoded_sequences = []\n",
        "#   for seq, score in sequences:\n",
        "#     decoded_sequence = tokenizer.decode(seq[0].squeeze().tolist())\n",
        "#     decoded_sequences.append((decoded_sequence, score))\n",
        "\n",
        "#   return decoded_sequences\n"
      ],
      "metadata": {
        "id": "5MJYt-_VHyu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Q1: Implementing the Vanilla Beam Search Decoder\n",
        "def beam_search_decoder(model, tokenizer, text, k, max_length):\n",
        "  \"\"\"\n",
        "  Perform beam search decoding for the given model and a starting input text.\n",
        "\n",
        "  Args:\n",
        "  model (PreTrainedModel): The language model to use for generating predictions.\n",
        "  tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
        "  text (str): The input text to start the generation.\n",
        "  k (int): The beam width; number of sequences to keep at each step.\n",
        "  max_length (int): The maximum length of the generated sequence.\n",
        "\n",
        "  Returns:\n",
        "  list of tuples: A list where each tuple contains a generated sequence and its score.\n",
        "  \"\"\"\n",
        "  # Handle empty input text\n",
        "  if text.strip() == \"\":\n",
        "    # Use a generic token (like start of sentence) if text is empty\n",
        "    input_ids = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0)\n",
        "  else:\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "  sequences = [(input_ids, 1.0)]\n",
        "\n",
        "  for _ in range(max_length - input_ids.size(1)):\n",
        "    all_candidates = []\n",
        "\n",
        "    for seq, score in sequences:\n",
        "      # Ensure the sequence is not empty\n",
        "      if seq.size(1) > 0:\n",
        "        # Generate the next token probabilities\n",
        "        outputs = model(seq)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        probabilities = torch.softmax(logits, dim=-1).squeeze()\n",
        "\n",
        "        # Consider each next token\n",
        "        for token_id in range(len(probabilities)):\n",
        "          new_seq = torch.cat([seq, torch.tensor([[token_id]]).to(seq.device)], dim=-1)\n",
        "          new_score = score * probabilities[token_id].item() # Multiply by the new token's probability\n",
        "          all_candidates.append((new_seq, new_score))\n",
        "\n",
        "    # Sort all candidates by score\n",
        "    ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "    # Select top-k sequences\n",
        "    sequences = ordered[:k]\n",
        "\n",
        "  decoded_sequences = []\n",
        "  for seq, score in sequences:\n",
        "    decoded_sequence = tokenizer.decode(seq[0].squeeze().tolist())\n",
        "    decoded_sequences.append((decoded_sequence, score))\n",
        "\n",
        "  return decoded_sequences"
      ],
      "metadata": {
        "id": "u0NwqZCDbaY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Q1: Implementing the Vanilla Beam Search Decoder\n",
        "\n",
        "# def beam_search_decoder(model, tokenizer, text, k, max_length):\n",
        "#   \"\"\"\n",
        "#   Perform beam search decoding for the given model and a starting input text.\n",
        "\n",
        "#   Args:\n",
        "#   model (PreTrainedModel): The language model to use for generating predictions.\n",
        "#   tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
        "#   text (str): The input text to start the generation.\n",
        "#   k (int): The beam width; number of sequences to keep at each step.\n",
        "#   max_length (int): The maximum length of the generated sequence.\n",
        "\n",
        "#   Returns:\n",
        "#   list of tuples: A list where each tuple contains a generated sequence and its score.\n",
        "#   \"\"\"\n",
        "\n",
        "#   input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "#   sequences = [(input_ids, 1.0)]\n",
        "\n",
        "#   ## Your code begins ##\n",
        "#   for _ in range(max_length - input_ids.size(1)):\n",
        "#     all_candidates = list()\n",
        "\n",
        "#     for seq, score in sequences:\n",
        "#       # Generate the next token probabilities\n",
        "#       output = model(seq)\n",
        "#       logits = output.logits\n",
        "#       probabilities = torch.softmax(logits[:, -1, :], dim=-1).squeeze()\n",
        "\n",
        "#       # Consider each next token\n",
        "#       for token_id in range(probabilities.size(0)):\n",
        "#         token_id_tensor = torch.tensor([token_id])  # Convert to a tensor and send to the same device\n",
        "#         token_id_tensor = token_id_tensor.unsqueeze(0)  # Reshape to [1, 1]\n",
        "#         next_seq = torch.cat([seq, token_id_tensor], dim=-1)\n",
        "#         next_score = score * probabilities[token_id].item()  # Multiply by the new token's probability\n",
        "\n",
        "#         all_candidates.append((next_seq, next_score))\n",
        "\n",
        "#     # Sort all candidates by score\n",
        "#     ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#     # Select top 'k' sequences\n",
        "#     sequences = ordered[:k]\n",
        "#   ## Your code ends ##\n",
        "\n",
        "#   decoded_sequences = []\n",
        "#   for seq, score in sequences:\n",
        "#     decoded_sequence = tokenizer.decode(seq[0].squeeze().tolist())\n",
        "#     decoded_sequences.append((decoded_sequence, score))\n",
        "\n",
        "#   return decoded_sequences\n"
      ],
      "metadata": {
        "id": "PTr4gMKCK_sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Analyzing Beam Search Output\n",
        "\n",
        "After you've implemented the vanilla beam search decoder, it's time to put it to the test and analyze its performance. This part of the assignment is crucial for understanding the behavior of the algorithm under different conditions and recognizing some of the common challenges associated with text generation in NLP.\n",
        "\n",
        "Start by experimenting with your beam search implementation. Try out different input sentences and beam widths (the `k` parameter), and observe how they affect the resulting generated text. You might want to consider questions like:\n",
        "\n",
        "- How does changing the beam width influence the diversity of the output?\n",
        "- What happens if the beam width is set to 1? How about a much larger number?\n",
        "- Do you notice any repetitive patterns or suboptimal text generation?"
      ],
      "metadata": {
        "id": "3R5DP2DvMFI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Initialization\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "xSjZDH1kQX67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing Your Vanilla Beam Search Decoder Implementation\n",
        "text = \"My name is \" # @param {type:\"string\"}\n",
        "k = 3 # @param {type:\"integer\"}\n",
        "max_length = 25 # @param {type:\"number\"}\n",
        "\n",
        "sequences = beam_search_decoder(model, tokenizer, text, k, max_length)\n",
        "\n",
        "for sequence in sequences:\n",
        "  print(f\"Sequence: {sequence[0]}, Score: {sequence[1]}\")\n"
      ],
      "metadata": {
        "id": "8k6DRKSdI0iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflect on your findings and document any patterns or behaviors you observe. Answer the following questions:\n",
        "\n",
        "<b><font color=\"green\">\n",
        "\n",
        "1. Are there instances where the model gets stuck in a loop, repeating the same phrase or sequence of tokens?\n",
        "2. How diverse are the generated sequences? Do they tend to converge to similar endings, or do they explore different possibilities?\n",
        "3. How contextually relevant are the generated sequences? Do they maintain the meaning and context set by the input text?\n",
        "\n",
        "</font></b>"
      ],
      "metadata": {
        "id": "JFXlc4GiMLr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><font color=\"yellow\">\n",
        "Using the input \"My name is\" and analyzing the output sequences:\n",
        "\n",
        "\n",
        "1. **Instances of Loops or Repetition**:\n",
        "   - In this case, there is a clear instance of repetition within individual sequences. For example, \"I'm a man. I'm a man. I'm a man.\" is repeated multiple times in the first and third sequences.\n",
        "   - This kind of repetition indicates that the model does get stuck in a loop, repeating the same phrase or sequence of tokens. Such behavior is more apparent with simple or incomplete prompts where the model might latch onto a probable phrase and repeat it.\n",
        "\n",
        "2. **Diversity of Generated Sequences**:\n",
        "   - While the beginnings of the sequences are diverse (with slight variations like \"and I'm a man\" vs. \"and I am a man of God\"), they converge into similar patterns of repetition, showing limited diversity in the latter parts.\n",
        "   - The repeated phrases dominate the sequences, indicating that despite different starts, the model tends towards similar repetitive structures as it continues generating text.\n",
        "\n",
        "3. **Contextual Relevance of Generated Sequences**:\n",
        "   - The sequences start contextually relevantly by attempting to complete the sentence started by \"My name is.\"\n",
        "   - However, the contextual relevance diminishes as the sequences progress into repetitive phrases. The repetition of phrases like \"I'm a man\" seems disconnected from the initial context set by \"My name is\" and doesn't contribute meaningfully to the narrative or context.\n",
        "\n",
        "### Additional Insights:\n",
        "\n",
        "- **Repetition and Model Limitations**: The repetition observed, especially in longer sequences, is a known limitation of many language models. It reflects a tendency to favor certain high-probability phrases, leading to their overuse.\n",
        "- **Prompt Specificity and Output Quality**: The quality and relevance of the output can significantly depend on the specificity and completeness of the input prompt. More specific and contextualized prompts may lead to more coherent and varied outputs.\n",
        "- **Model Training and Biases**: The patterns in the model's outputs also reflect the data it was trained on. Common phrases or structures in the training data might be replicated in the model's output, sometimes leading to repetitive or biased content.\n",
        "\n",
        "In summary, these observations indicate a tendency for the model to loop or repeat phrases, especially with less specific prompts. The diversity in the output is limited when repetition dominates, and the contextual relevance tends to decrease as the sequences progress.\n",
        "</font></b>\n"
      ],
      "metadata": {
        "id": "mKFyYJl4YLIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enhancing Beam Search with n-gram Blocking and Hamming Diversity Factor\n",
        "\n",
        "Through your experimentation, you might have noticed certain limitations in the plain beam search algorithm. To overcome these, we can integrate advanced techniques such as n-gram blocking and the Hamming diversity factor. Let’s explore these concepts in more detail and see how they can be applied to improve your beam search decoder.\n",
        "\n",
        "#### n-gram Blocking\n",
        "\n",
        "**What is n-gram blocking?**\n",
        "n-gram blocking is a technique used to prevent the model from generating the same sequence of tokens (an n-gram) more than once. An n-gram is a contiguous sequence of `n` tokens from the text. For example, in the sentence \"The cat sat on the mat\", \"the cat sat\" is a 3-gram.\n",
        "\n",
        "**Why use n-gram blocking?**\n",
        "It helps mitigate the issue of repetitive loops in the generated text, a common problem where the model outputs the same phrase over and over again. By blocking previously generated n-grams, we force the model to explore new paths and produce more varied and unique sequences.\n",
        "\n",
        "#### Hamming Diversity Factor\n",
        "\n",
        "**What is the Hamming diversity factor?**\n",
        "The Hamming diversity factor is a technique that introduces diversity into the beam search by penalizing new sequences that are too similar to any existing sequence in the beam. The Hamming distance measures how many positions two sequences of the same length differ. The diversity factor applies a penalty based on this distance, encouraging the selection of more diverse sequences.\n",
        "\n",
        "**Why apply the Hamming diversity factor?**\n",
        "In beam search, there's a risk that all top `k` sequences may become very similar to each other, as they might all share a common prefix. This similarity can reduce the overall diversity of the generated text. By applying the Hamming diversity factor, we encourage the model to generate a set of diverse sequences, potentially covering a wider range of meanings and contexts.\n",
        "\n",
        "#### Step by Step Guide\n",
        "\n",
        "- **n-gram Blocking**:\n",
        "   1. Implement a check within your beam search loop to ensure that no n-gram in the newly generated sequence is a repeat of any n-gram in the current beam.\n",
        "   2. You will need to maintain a list of all n-grams generated so far and prevent these from being considered in future steps.\n",
        "\n",
        "- **Hamming Diversity Factor**:\n",
        "   1. After generating all possible next step candidates but before selecting the top `k`, calculate the Hamming distance between each candidate sequence and all the sequences currently in the beam.\n",
        "   2. Apply a penalty to the scores of the candidate sequences based on their Hamming distance to the sequences in the beam. The penalty can be a simple multiplicative factor that decreases the score of a candidate sequence based on how similar it is to any sequence in the beam.\n",
        "\n",
        "You can use the beam search decoder function you've already completed as a starting point for this section. Integrate the n-gram blocking and Hamming diversity factor into the existing code. This will allow you to directly compare the outputs before and after applying these enhancements, giving you a clear understanding of their impact on the generated text.\n",
        "\n",
        "<font color=\"red\">Warning: You are not allowed to use predefined implementations of the beam serach function from libraries or frameworks!</font>"
      ],
      "metadata": {
        "id": "p2fNsBFqMa1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Q2: Implementing the Enhanced Beam Search Decoder\n",
        "\n",
        "# Function to calculate the Hamming penalty for diversity\n",
        "def calculate_hamming_penalty(sequence, all_sequences, diversity_factor):\n",
        "  \"\"\"\n",
        "  Calculate the cumulative Hamming penalty for a sequence against all other sequences.\n",
        "\n",
        "  Args:\n",
        "  sequence (list): The new sequence for which the penalty is being calculated.\n",
        "  all_sequences (list of lists): All the sequences currently in the beam.\n",
        "  diversity_factor (float): The factor by which the penalty is multiplied.\n",
        "\n",
        "  Returns:\n",
        "  float: The cumulative Hamming penalty for the sequence.\n",
        "  \"\"\"\n",
        "  # print('********')\n",
        "  # print('sequence', sequence)\n",
        "  # print('all_sequences', )\n",
        "  # print(all_sequences)\n",
        "  # print('********')\n",
        "  penalties = 0\n",
        "\n",
        "  ## Your code begins ##\n",
        "  for seq_in_beam in all_sequences:\n",
        "    # Calculate Hamming distance between sequence and seq_in_beam\n",
        "    hamming_distance = sum(token1 != token2 for token1, token2 in zip(sequence, seq_in_beam))\n",
        "    penalties += hamming_distance\n",
        "  ## Your code ends ##\n",
        "  return penalties * diversity_factor\n",
        "\n",
        "def enhanced_beam_search_decoder(model, tokenizer, text, k, max_length, n_gram_block_size, diversity_factor):\n",
        "  \"\"\"\n",
        "  Perform beam search decoding with N-gram blocking to prevent loops and apply a Hamming diversity factor.\n",
        "\n",
        "  Args:\n",
        "  model (PreTrainedModel): The language model used for generating predictions.\n",
        "  tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
        "  text (str): The input text to start the generation.\n",
        "  k (int): The beam width; the number of sequences to keep at each step.\n",
        "  max_length (int): The maximum length of the generated sequence.\n",
        "  n_gram_block_size (int): The size of the n-gram to block to prevent repetition.\n",
        "  diversity_factor (float): The factor to encourage diversity among the beam.\n",
        "\n",
        "  Returns:\n",
        "  list of tuples: A list where each tuple contains a generated sequence and its score.\n",
        "  \"\"\"\n",
        "\n",
        "  if text.strip() == \"\":\n",
        "    input_ids = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0)\n",
        "  else:\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "\n",
        "  sequences = [(input_ids, 1.0)]\n",
        "  generated_ngrams = set()\n",
        "\n",
        "  ## Your code begins ##\n",
        "  for _ in range(max_length - input_ids.size(1)):\n",
        "    all_candidates = []\n",
        "\n",
        "    for seq, score in sequences:\n",
        "      if seq.size(1) > 0:\n",
        "        outputs = model(seq)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "        probabilities = torch.softmax(logits, dim=-1).squeeze()\n",
        "\n",
        "      for token_id in range(len(probabilities)):\n",
        "        new_seq = torch.cat([seq, torch.tensor([[token_id]]).to(seq.device)], dim=-1)\n",
        "\n",
        "        # Check for n-gram repetition\n",
        "        ngrams = set(zip(*[new_seq[0][i:] for i in range(n_gram_block_size)]))\n",
        "        if not ngrams.isdisjoint(generated_ngrams):\n",
        "          continue\n",
        "\n",
        "        generated_ngrams.update(ngrams)\n",
        "\n",
        "        new_score = score * probabilities[token_id].item()\n",
        "\n",
        "        # Calculate and apply Hamming penalty\n",
        "        hamming_penalty = calculate_hamming_penalty(new_seq[0].tolist(), [s[0].tolist()[0] for s in sequences], diversity_factor)\n",
        "        adjusted_score = new_score - hamming_penalty\n",
        "\n",
        "        all_candidates.append((new_seq, adjusted_score))\n",
        "\n",
        "    print('***',new_seq[0].tolist(), '******')\n",
        "    # print('***', [s[0].tolist()[0] for s in sequences],'****')\n",
        "    # print('new_seq: ', new_seq)\n",
        "    # print('*******')\n",
        "    # print('sequences', sequences)\n",
        "    # print('*******')\n",
        "    # print('new score', new_score)\n",
        "    # print('*******')\n",
        "    # print('hamming_penalty', hamming_penalty)\n",
        "    # print('*******')\n",
        "    # print('new_seq: ', new_seq)\n",
        "    # print('********')\n",
        "    # print('adjusted_score: ',adjusted_score)\n",
        "    # print('********')\n",
        "    # print('all_candidates: ', all_candidates)\n",
        "    # print('********')\n",
        "    # Select top-k sequences\n",
        "    ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "    sequences = ordered[:k]\n",
        "\n",
        "  ## Your code ends ##\n",
        "\n",
        "  decoded_sequences = []\n",
        "  for seq, score in sequences:\n",
        "    decoded_sequence = tokenizer.decode(seq[0].squeeze().tolist())\n",
        "    decoded_sequences.append((decoded_sequence, score))\n",
        "\n",
        "  return decoded_sequences"
      ],
      "metadata": {
        "id": "de1YJ4w_Fu7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing and Reflecting on Your Enhanced Beam Search Decoder\n",
        "\n",
        "After implementing the enhancements to your beam search decoder, it's crucial to test its performance and understand the impact of these modifications. By experimenting with different settings and inputs, you can gain valuable insights into the behavior of the algorithm and the effectiveness of n-gram blocking and the Hamming diversity factor.\n",
        "\n",
        "- Start by running your enhanced beam search decoder on different input sentences. Observe how the outputs change with these variations.\n",
        "\n",
        "- Vary the `n_gram_block_size` parameter. A smaller size may lead to more repetitions, whereas a larger size could prevent them but might also affect the natural flow of the text.\n",
        "\n",
        "- Experiment with different values for the `diversity_factor`. Observe how increasing or decreasing this value impacts the diversity of the generated sequences."
      ],
      "metadata": {
        "id": "lPuokM6TR286"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing Your Enhanced Beam Search Decoder Implementation\n",
        "text = \"\" # @param {type:\"string\"}\n",
        "k = 3 # @param {type:\"integer\"}\n",
        "max_length = 25 # @param {type:\"number\"}\n",
        "n_gram_block_size = 2 # @param {type:\"integer\"}\n",
        "diversity_factor = 0.1 # @param {type:\"number\"}\n",
        "\n",
        "sequences = enhanced_beam_search_decoder(model, tokenizer, text, k, max_length, n_gram_block_size, diversity_factor)\n",
        "for sequence in sequences:\n",
        "  print(f\"Sequence: {sequence[0]}, Score: {sequence[1]}\")\n"
      ],
      "metadata": {
        "id": "meLpHGplQQyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After conducting your experiments, reflect on the following questions:\n",
        "\n",
        "<b><font color=\"green\">\n",
        "4. How does changing the n-gram blocking size affect the output of the beam search? Are there any trade-offs involved in choosing a particular n-gram size?\n",
        "5. How does the diversity factor influence the variety of the generated sequences? Did you notice a difference in the output quality with varying diversity factors?\n",
        "6. How do these enhancements (n-gram blocking and diversity factor) improve upon the vanilla beam search algorithm?\n",
        "</font></b>"
      ],
      "metadata": {
        "id": "NKl9QeOmSCsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Machine Translation and BLEU Metric\n",
        "\n",
        "In this section of the assignment, you will explore how to evaluate the quality of translations produced by generative language models using the BLEU (Bilingual Evaluation Understudy) metric. The BLEU metric is widely used for assessing the quality of machine-generated translations by comparing them with a set of reference translations."
      ],
      "metadata": {
        "id": "r7lMCBOiYNqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The WMT16 Task\n",
        "\n",
        "For this task, we'll use the WMT16 (Workshop on Machine Translation 2016) dataset to provide English to German translation pairs. This dataset is a benchmark in the NLP community for evaluating translation models.\n"
      ],
      "metadata": {
        "id": "UMKvHzyeZQj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load WMT dataset\n",
        "wmt_en_de = load_dataset(\"wmt16\", \"de-en\", split=\"test\")  # Use the appropriate split\n",
        "\n",
        "# Sample a subset for demonstration purposes\n",
        "wmt_en_de_subset = wmt_en_de.shuffle().select(range(100))  # select 100 samples randomly\n"
      ],
      "metadata": {
        "id": "UkAn-3lDZQ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Metric\n",
        "\n",
        "The BLEU metric quantifies the similarity between a machine-generated translation and a set of high-quality reference translations. It does this by computing the precision of matched n-grams between the translations and references, applying a brevity penalty to prevent favoring overly short translations.\n",
        "\n",
        "As part of this exercise, you are expected to implement the `calculate_bleu_score` function. This function will take a list of predicted sentences and a list of reference sentences and return the BLEU score.\n",
        "\n",
        "To implement the BLEU metric, students should follow a step-by-step process that aligns with the mathematical foundations of the metric. The BLEU score calculation involves multiple components, each addressing a different aspect of the translation's quality.\n",
        "\n",
        "#### Step-by-Step Guide\n",
        "\n",
        "1. **Understanding BLEU**\n",
        "  - Start by understanding the BLEU score's components. The BLEU score evaluates the quality of machine-translated text against reference translations using n-gram precision and a brevity penalty.\n",
        "  - Read about the mathematical details in this [foundational blog post on BLEU](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b).\n",
        "\n",
        "2. **Tokenization**\n",
        "  - Tokenize both the predicted sentences and the reference sentences. This means splitting each sentence into its constituent words or tokens.\n",
        "\n",
        "3. **N-gram Precision Calculation**\n",
        "  - For n-grams of sizes 1 to 4 (unigrams to 4-grams), calculate the precision. Precision is the ratio of the number of matching n-grams in the predicted text to the number of n-grams in the predicted text.\n",
        "  - Use the `ngrams` function from NLTK to generate n-grams from the sentences.\n",
        "\n",
        "4. **Count Overlapping N-grams**\n",
        "  - For each n-gram size, count the number of n-grams in the predicted text that appear in the reference text. This count is capped by the maximum number of times each n-gram appears in the reference text (to avoid over-counting repeated n-grams).\n",
        "\n",
        "5. **Calculate Individual Precisions**\n",
        "  - Divide the count of overlapping n-grams by the total number of n-grams in the predicted text for each n-gram size. This gives you the precision for each n-gram size.\n",
        "\n",
        "6. **Compute Brevity Penalty**\n",
        "  - The brevity penalty penalizes overly short predicted texts. It's calculated as `exp(1 - r/c)` where `r` is the length of the reference text and `c` is the length of the predicted text. If the predicted text is longer than the reference, the brevity penalty is 1 (no penalty).\n",
        "\n",
        "7. **Calculate BLEU Score**\n",
        "  - The final BLEU score is the geometric mean of the four n-gram precisions, multiplied by the brevity penalty. Use the `math` library to calculate the geometric mean.\n",
        "\n",
        "\n",
        "<font color=\"red\">Warning: You are not allowed to use predefined implementations of the BLEU metric from libraries or frameworks!</font>"
      ],
      "metadata": {
        "id": "EGIHpMmXaB1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Q3: Implementing the BLEU Metric\n",
        "\n",
        "# import math\n",
        "# from collections import Counter\n",
        "# from nltk.util import ngrams\n",
        "\n",
        "# def calculate_bleu_score(predictions, references):\n",
        "#   \"\"\"\n",
        "#   Calculate BLEU score for a list of predicted sentences given a list of reference sentences.\n",
        "\n",
        "#   Args:\n",
        "#   predictions (list of str): A list of predicted sentences.\n",
        "#   references (list of str): A list of reference sentences.\n",
        "\n",
        "#   Returns:\n",
        "#   float: The calculated BLEU score.\n",
        "#   \"\"\"\n",
        "#   ## Your code begins ##\n",
        "#   pass\n",
        "#   ## Your code ends ##\n"
      ],
      "metadata": {
        "id": "TTFPnIRqbF-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Q3: Implementing the BLEU Metric\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def calculate_bleu_score(predictions, references):\n",
        "  \"\"\"\n",
        "  Calculate BLEU score for a list of predicted sentences given a list of reference sentences.\n",
        "\n",
        "  Args:\n",
        "  predictions (list of str): A list of predicted sentences.\n",
        "  references (list of str): A list of reference sentences.\n",
        "\n",
        "  Returns:\n",
        "  float: The calculated BLEU score.\n",
        "  \"\"\"\n",
        "  # Tokenization\n",
        "  tokenized_predictions = [prediction.split() for prediction in predictions]\n",
        "  tokenized_references = [reference.split() for reference in references]\n",
        "\n",
        "  # Initialize variables for precision calculations\n",
        "  precisions = [0, 0, 0, 0]\n",
        "\n",
        "  for i in range(4):  # For n-grams from 1 to 4\n",
        "    for prediction, reference in zip(tokenized_predictions, tokenized_references):\n",
        "      prediction_ngrams = list(ngrams(prediction, i + 1, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "      reference_ngrams = list(ngrams(reference, i + 1, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "      reference_ngram_counts = Counter(reference_ngrams)\n",
        "      prediction_ngram_counts = Counter(prediction_ngrams)\n",
        "\n",
        "      # Count overlapping n-grams\n",
        "      overlap = sum(min(count, reference_ngram_counts[ngram]) for ngram, count in prediction_ngram_counts.items())\n",
        "      total = len(prediction_ngrams)\n",
        "\n",
        "      # Calculate precision\n",
        "      precisions[i] += overlap / total if total > 0 else 0\n",
        "\n",
        "  # Calculate geometric mean of precisions\n",
        "  precisions = [p / len(predictions) for p in precisions]  # Average over all predictions\n",
        "  score = math.exp(sum(math.log(p) for p in precisions if p) / 4)\n",
        "\n",
        "  # Calculate brevity penalty\n",
        "  r = sum(len(ref) for ref in tokenized_references)\n",
        "  c = sum(len(pred) for pred in tokenized_predictions)\n",
        "  brevity_penalty = math.exp(1 - r / c) if c < r else 1\n",
        "\n",
        "  return score * brevity_penalty\n"
      ],
      "metadata": {
        "id": "1EEfmIAPcRrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Models with Your BLEU Implementation\n",
        "\n",
        "In this section, you will apply your implementation of the BLEU metric to evaluate the performance of two generative models: T5 and mGPT. This practical application will not only test the effectiveness of your BLEU score calculation but also provide insights into the comparative strengths and weaknesses of these models in translation tasks."
      ],
      "metadata": {
        "id": "y-9Op5j3dnhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate BLEU Scores for T5 and mGPT\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def translate_with_t5(text, model, tokenizer):\n",
        "  input_ids = tokenizer(f\"translate English to German: {text}\", return_tensors=\"pt\").input_ids.to(device)\n",
        "  outputs = model.generate(input_ids)\n",
        "  translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return translation\n",
        "\n",
        "def translate_with_mgpt(text, model, tokenizer):\n",
        "  prompt = f\"Translate English to German: '{text}'\"\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "  inputs['max_new_tokens'] = 20\n",
        "  outputs = model.generate(**inputs)\n",
        "  translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  translation = translation[len(prompt):]\n",
        "  return translation.strip()\n",
        "\n",
        "\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Load distilgpt2 model and tokenizer\n",
        "mgpt_model = GPT2LMHeadModel.from_pretrained('ai-forever/mGPT').to(device)\n",
        "mgpt_tokenizer = GPT2Tokenizer.from_pretrained('ai-forever/mGPT')\n",
        "\n",
        "\n",
        "\n",
        "t5_translations = [translate_with_t5(example[\"translation\"][\"en\"], t5_model, t5_tokenizer) for example in wmt_en_de_subset]\n",
        "mgpt_translations = [translate_with_mgpt(example[\"translation\"][\"en\"], mgpt_model, mgpt_tokenizer) for example in wmt_en_de_subset]\n",
        "\n",
        "references = [example[\"translation\"][\"de\"] for example in wmt_en_de_subset]\n",
        "\n",
        "t5_bleu_score = calculate_bleu_score(predictions=t5_translations, references=references)\n",
        "mgpt_bleu_score = calculate_bleu_score(predictions=mgpt_translations, references=references)\n",
        "\n",
        "print(f\"T5 BLEU Score: {t5_bleu_score:.4f}\")\n",
        "print(f\"mGPT BLEU Score: {mgpt_bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "xu8HrM4RbXpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calculating the BLEU scores, reflect on the following questions:\n",
        "\n",
        "<b>\n",
        "<font color=\"green\">\n",
        "\n",
        "7. Which model achieved a higher BLEU score? Why do you think this model performed better in terms of translation quality? Analyze why. Consider the architectural differences between T5 and mGPT, and how these might contribute to T5's effectiveness in translation.\n",
        "\n",
        "</font>\n",
        "</b>"
      ],
      "metadata": {
        "id": "JMvHOhQwd4r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **T5 BLEU Score**: 0.1723\n",
        "- **mGPT BLEU Score**: 0.0000\n",
        "\n",
        "It's clear that the T5 model achieved a significantly higher BLEU score compared to the mGPT model. Let's analyze why this might be the case, focusing on the models' design and training objectives.\n",
        "\n",
        "### T5 (Text-to-Text Transfer Transformer)\n",
        "\n",
        "1. **Purpose-Built for Translation**: T5 is designed as a text-to-text model, meaning it can handle a variety of NLP tasks, including translation, as \"text-to-text\" problems. It's trained on a multi-task setting where translation is a key task.\n",
        "\n",
        "2. **Training Data and Fine-Tuning**: T5 is trained on a diverse dataset that includes translation tasks. It's also common to fine-tune T5 on specific translation datasets, further enhancing its performance on such tasks.\n",
        "\n",
        "3. **Model Architecture**: T5 uses an encoder-decoder architecture, which is a standard approach for sequence-to-sequence tasks like translation. This architecture allows T5 to effectively encode the source sentence and generate a coherent translation.\n",
        "\n",
        "4. **Attention Mechanism**: T5's use of the Transformer architecture, with its attention mechanism, allows it to focus on different parts of the input sequence when generating each word of the output. This is particularly beneficial for translation.\n",
        "\n",
        "### mGPT (Modified Generative Pre-trained Transformer)\n",
        "\n",
        "1. **Not Specifically Designed for Translation**: GPT models, including mGPT variants, are primarily designed for generative language tasks (like text completion), not specifically for translation. While they can be prompted to perform translation, it's not their primary function.\n",
        "\n",
        "2. **Training Objectives**: GPT models are trained with a causal language modeling objective (predicting the next word in a sentence) rather than a translation objective.\n",
        "\n",
        "3. **Model Architecture**: GPT models use a decoder-only Transformer architecture. Unlike the encoder-decoder structure in models like T5, this architecture isn't optimized for tasks that require understanding one sequence and generating a different, related sequence.\n",
        "\n",
        "4. **Lack of Fine-Tuning**: Without fine-tuning on a specific translation dataset, GPT models might struggle with accurately translating between languages, as seen in the low BLEU score.\n",
        "\n",
        "### Analysis\n",
        "\n",
        "- The **higher BLEU score for T5** is likely due to its design as a multi-task model, including translation, and its encoder-decoder architecture, which is more suited for translation tasks.\n",
        "- The **low BLEU score for mGPT** reflects that while it is a powerful generative model, translation is not its primary function, and it lacks the architectural advantages that models like T5 have for this specific task.\n",
        "\n",
        "This comparison highlights the importance of choosing a model that aligns with the specific requirements of a task and how the underlying architecture and training of a model can significantly impact its performance on different types of NLP tasks."
      ],
      "metadata": {
        "id": "zw1xNYK1mnEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizing Attention in T5\n",
        "\n",
        "In this analytical section, you will dive into the inner workings of the T5 model by visualizing its attention mechanisms. The attention mechanism in transformers is a critical component that determines which parts of the input sequence are focused on during each step of the decoding process. Understanding attention can provide deeper insights into how models like T5 handle context and relationships within the input data.\n",
        "\n",
        "Your task is to implement a function that extracts the self-attention and cross-attention weights from the T5 model when processing a given piece of text. Using these weights, you will create visualizations to explore how the model attends to different parts of the input and output sequences during translation."
      ],
      "metadata": {
        "id": "kJO91j_qb87R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement `get_attention_weights` Function:\n",
        "\n",
        "Refer to the [T5 model documentation on Hugging Face](https://huggingface.co/docs/transformers/model_doc/t5) to understand how to access the attention weights (both self-attention and cross-attention) from the model's outputs.\n",
        "Use this knowledge to complete the `get_attention_weights` function, which should generate output sequences for decoder input and extract the attention weights for visualization.\n"
      ],
      "metadata": {
        "id": "fTGqso1uziCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Q4: Implement `get_attention_weights` Function\n",
        "\n",
        "# def get_attention_weights(text, model, tokenizer):\n",
        "#   \"\"\"\n",
        "#   Generate the input sequence and output sequence using the model, then retrieve the self-attention\n",
        "#   and cross-attention weights for visualization.\n",
        "\n",
        "#   Args:\n",
        "#   text (str): The text to be processed by the model, including any task-specific prefix.\n",
        "#   model (T5ForConditionalGeneration): The pre-trained T5 model.\n",
        "#   tokenizer (T5Tokenizer): The tokenizer corresponding to the T5 model.\n",
        "\n",
        "#   Returns:\n",
        "#   tuple: A tuple containing three elements:\n",
        "#       - self_attentions (torch.Tensor): A tensor of shape (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
        "#         containing the self-attention weights from the encoder.\n",
        "#       - cross_attentions (torch.Tensor): A tensor of shape (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
        "#         containing the cross-attention weights from the decoder.\n",
        "#       - output_sequences (torch.Tensor): A tensor containing the generated output sequence ids.\n",
        "#   \"\"\"\n",
        "#   ## Your code begins ##\n",
        "#   pass\n",
        "#   ## Your code ends ##"
      ],
      "metadata": {
        "id": "TYEPSKGOzaD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Q4: Implement `get_attention_weights` Function\n",
        "\n",
        "# def get_attention_weights(text, model, tokenizer):\n",
        "#   \"\"\"\n",
        "#   Generate the input sequence and output sequence using the model, then retrieve the self-attention\n",
        "#   and cross-attention weights for visualization.\n",
        "\n",
        "#   Args:\n",
        "#   text (str): The text to be processed by the model, including any task-specific prefix.\n",
        "#   model (T5ForConditionalGeneration): The pre-trained T5 model.\n",
        "#   tokenizer (T5Tokenizer): The tokenizer corresponding to the T5 model.\n",
        "\n",
        "#   Returns:\n",
        "#   tuple: A tuple containing three elements:\n",
        "#       - self_attentions (torch.Tensor): A tensor of shape (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
        "#         containing the self-attention weights from the encoder.\n",
        "#       - cross_attentions (torch.Tensor): A tensor of shape (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
        "#         containing the cross-attention weights from the decoder.\n",
        "#       - output_sequences (torch.Tensor): A tensor containing the generated output sequence ids.\n",
        "#   \"\"\"\n",
        "#   # Encode the input text and generate output with attention weights\n",
        "#   inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "#   with torch.no_grad():  # Ensure no gradient is computed to save memory\n",
        "#     outputs = model.generate(inputs.input_ids, output_attentions=True)\n",
        "\n",
        "#   # Extract attention weights\n",
        "#   self_attentions = outputs.encoder_attentions  # Self-attention from the encoder\n",
        "#   cross_attentions = outputs.decoder_attentions  # Cross-attention from the decoder\n",
        "#   output_sequences = outputs.sequences\n",
        "\n",
        "#   return self_attentions, cross_attentions, output_sequences\n"
      ],
      "metadata": {
        "id": "wG5O2NzLudf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Q4: Implement `get_attention_weights` Function\n",
        "\n",
        "def get_attention_weights(text, model, tokenizer):\n",
        "  \"\"\"\n",
        "  Generate the input sequence and output sequence using the model, then retrieve the self-attention\n",
        "  and cross-attention weights for visualization.\n",
        "\n",
        "  Args:\n",
        "  text (str): The text to be processed by the model, including any task-specific prefix.\n",
        "  model (T5ForConditionalGeneration): The pre-trained T5 model.\n",
        "  tokenizer (T5Tokenizer): The tokenizer corresponding to the T5 model.\n",
        "\n",
        "  Returns:\n",
        "  tuple: A tuple containing three elements:\n",
        "      - self_attentions (torch.Tensor): A tensor of shape (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
        "        containing the self-attention weights from the encoder.\n",
        "      - cross_attentions (torch.Tensor): A tensor of shape (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
        "        containing the cross-attention weights from the decoder.\n",
        "      - output_sequences (torch.Tensor): A tensor containing the generated output sequence ids.\n",
        "  \"\"\"\n",
        "  # Tokenize the input text\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "  input_ids = inputs.input_ids\n",
        "\n",
        "  # Create dummy decoder input IDs (same length as input IDs)\n",
        "  decoder_input_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "  # Forward pass with attention weights\n",
        "  outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "\n",
        "  # Extract attention weights\n",
        "  self_attentions = outputs.encoder_attentions\n",
        "  cross_attentions = outputs.decoder_attentions\n",
        "\n",
        "  # Generate output sequence for visualization\n",
        "  with torch.no_grad():\n",
        "    generated_ids = model.generate(input_ids)\n",
        "  output_sequences = generated_ids\n",
        "\n",
        "  return self_attentions, cross_attentions, output_sequences\n"
      ],
      "metadata": {
        "id": "Bcd10JiQvVYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.num_layers"
      ],
      "metadata": {
        "id": "IkXfXHmq01Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Attention Weights\n",
        "text = \"I'm going to school\" # @param {type:\"string\"}\n",
        "layer_to_visualize = 5 # @param {type:\"integer\"}\n",
        "head_to_visualize = 7 # @param {type:\"integer\"}\n",
        "attention_type = \"cross\" # @param [\"cross\", \"self\"]\n",
        "\n",
        "def visualize_attention(attention_weights, tokens, output_tokens, layer_num, head_num, attention_type):\n",
        "  attention = attention_weights[layer_num][0][head_num].detach().cpu().numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  if attention_type == \"cross\":\n",
        "    sns.heatmap(attention, xticklabels=tokens, yticklabels=output_tokens, annot=False, cmap='viridis')\n",
        "  else:\n",
        "    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, annot=False, cmap='viridis')\n",
        "\n",
        "  plt.title(f\"{attention_type.title()} Attention (Layer {layer_num + 1}, Head {head_num + 1})\")\n",
        "  plt.xlabel(\"Decoder Tokens\" if attention_type == \"cross\" else \"Input Tokens\")\n",
        "  plt.ylabel(\"Encoder Tokens\" if attention_type == \"cross\" else \"Input Tokens\")\n",
        "  plt.show()\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model.eval()\n",
        "\n",
        "self_attentions, cross_attentions, output_sequences = get_attention_weights(text, model, tokenizer)\n",
        "\n",
        "input_tokens = [tokenizer.decode(token_id) for token_id in tokenizer(text, return_tensors=\"pt\")['input_ids'][0]]\n",
        "output_tokens = [tokenizer.decode(token_id) for token_id in output_sequences[0]]\n",
        "\n",
        "if attention_type == \"cross\":\n",
        "  visualize_attention(cross_attentions, input_tokens, output_tokens, layer_to_visualize, head_to_visualize, \"cross\")\n",
        "else:\n",
        "  visualize_attention(self_attentions, input_tokens, input_tokens, layer_to_visualize, head_to_visualize, \"self\")\n"
      ],
      "metadata": {
        "id": "5_Z9rwxysKiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_attentions, cross_attentions, output_sequences = get_attention_weights(text, model, tokenizer)\n"
      ],
      "metadata": {
        "id": "XXxTuJyr1_9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_attentions[2][0][0].detach().cpu().size()"
      ],
      "metadata": {
        "id": "8xZvHsTS4x4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cross_attentions)"
      ],
      "metadata": {
        "id": "U0fuOF_a4kop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_attentions[5][0].detach().cpu().numpy().shape"
      ],
      "metadata": {
        "id": "EnaypshA85gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain a deeper understanding of the T5 model's attention mechanisms, you should modify various parameters and observe the resulting changes in attention patterns. This analysis will reveal how the model allocates attention at different stages of processing and how each head within the layers may capture different types of information.\n",
        "\n",
        "As you perform the analysis, consider the following questions:\n",
        "\n",
        "<font color=\"green\"><b>\n",
        "\n",
        "8. How do attention patterns differ between the first and last layers of the model? What does this suggest about the role of initial versus deeper layers in processing the input?\n",
        "9. What variations do you observe in the attention patterns of different heads within the same layer? Can you hypothesize what different types of information each head might be capturing?\n",
        "10. How does the complexity or length of the input text influence the attention patterns? Do certain types of sentences result in more or less attention spread across the input tokens?\n",
        "\n",
        "</b></font>\n",
        "\n",
        "Document your observations and hypotheses regarding the different attention patterns you observe. Answer the questions provided, supporting your answers with specific examples from your visualizations. This exercise will help you understand the nuanced ways in which the T5 model processes and translates input sequences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "59ll4g9220yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"yellow\"><b> observations\n",
        "### English to French Translation:\n",
        "\n",
        "1. **Layer 1, Head 1:**\n",
        "   - The first layer's first head shows a broad attention distribution, with attention spread across many input tokens for each output token. This suggests that early in the model, there's a wide consideration of the context.\n",
        "\n",
        "2. **Layer 4, Head 1:**\n",
        "   - By the middle layers, we see more focused attention patterns, indicating a refined consideration of specific parts of the input.\n",
        "\n",
        "3. **Layer 6, Head 1:**\n",
        "   - The last layer's first head shows very focused attention on specific tokens, suggesting that deeper layers make more precise alignments between input and output tokens.\n",
        "\n",
        "4. **Layer 6, Head 4:**\n",
        "   - Here, the attention is slightly more spread out than in the first head of the same layer. Different heads in the same layer often capture different types of relationships or features.\n",
        "\n",
        "5. **Layer 6, Head 8:**\n",
        "   - This head shows a different pattern, with some attention outside the main diagonal, suggesting that this head may be capturing more long-range dependencies or different types of information than the first head.\n",
        "\n",
        "### English to German Translation:\n",
        "\n",
        "1. **Layer 1, Head 1:**\n",
        "   - Similar to the English to French translation, the first layer shows broad attention, indicating a wide context consideration at the start of the model's processing.\n",
        "\n",
        "2. **Layer 4, Head 1:**\n",
        "   - Again, there's more focus in the middle layer, but with a different pattern, possibly due to the different nature of the German language compared to French, or due to the different sentence structure.\n",
        "\n",
        "3. **Layer 6, Head 1, 4, and 8:**\n",
        "   - In the last layer, all heads show very focused attention, but with significant differences between them, suggesting that each head is capturing different aspects of the alignment between input and output.\n",
        "\n",
        "</b></font>\n"
      ],
      "metadata": {
        "id": "x-DABj2yL0zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"yellow\"><b> answer the questions:\n",
        "\n",
        "1. **Attention Patterns Between First and Last Layers:**\n",
        "   - The initial layers tend to have a broader attention spread, which likely corresponds to a more generalized understanding of the input context. The deeper layers have more focused attention, which suggests a refined processing geared towards specific token-to-token relationships needed for translation.\n",
        "\n",
        "2. **Variations in Attention Patterns of Different Heads:**\n",
        "   - Different heads within the same layer can show varied attention patterns, indicating that they are capturing different aspects of the input. Some heads focus on sequential tokens, while others may focus on particular tokens that are key to the meaning or structure of the sentence.\n",
        "\n",
        "3. **Complexity or Length of Input Text:**\n",
        "   - The complexity or length of the input text affects attention patterns. Longer or more complex sentences might show more spread out attention, as the model needs to integrate more information over larger spans of text.\n",
        "\n",
        "</b></font>"
      ],
      "metadata": {
        "id": "DUgxSXT7MyTa"
      }
    }
  ]
}
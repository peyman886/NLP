{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCK8LnZQBNVM"
      },
      "source": [
        "### GPU Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiW-sPnTNyaN"
      },
      "source": [
        "\n",
        "for use local gpu run this comand in anaconda prompet:\n",
        "\n",
        "      1) pip install jupyter_http_over_ws\n",
        "      2) jupyter serverextension enable --py jupyter_http_over_ws\n",
        "      3) jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
        "      \n",
        "copy url and paste on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb-jj78BsGRp"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "#Check GPU\n",
        "# torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# article"
      ],
      "metadata": {
        "id": "fGd4Gu4UHl8z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2yPpCUHu8gC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from PyPDF2 import PdfReader\n",
        "\n",
        "# pdf_reader = PdfReader(\"Is ChatGPT a Good Recommender A Preliminary Study.pdf\")\n",
        "# pdf_text = ''\n",
        "# for page_num in range(len(pdf_reader.pages)):\n",
        "#     page = pdf_reader.pages[page_num]\n",
        "#     pdf_text += page.extract_text()\n",
        "\n",
        "# pdf_text[401:-15625]\n",
        "\n",
        "# file_path = 'example.txt'\n",
        "# with open(file_path, 'w') as file:\n",
        "#     # 2. Write the string to the file\n",
        "#     string_to_save = pdf_text[401:-15625]\n",
        "#     file.write(string_to_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This structure indicates that the LLMRec system can handle scenarios where either some input data is provided or where only an instruction is needed. The system then uses these templates to format the prompts sent to the language model, and the responses are extracted based on the specified split marker."
      ],
      "metadata": {
        "id": "w80k_MPWxMf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# readme"
      ],
      "metadata": {
        "id": "mDGKBcqhw0JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## overview of the project:\n",
        "\n",
        "LLMRec is a recommender system based on large language models (LLMs), designed for benchmarking these models on various recommendation tasks. The key features and components :\n",
        "\n",
        "**Abstract**: LLMRec focuses on five recommendation tasks: rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. It utilizes well-designed templates to transform recommendation data into textual prompts for benchmarking popular LLMs.\n",
        "\n",
        "**Prompt Construction**: The project involves using various prompts for evaluating `*off-the-shelf*` LLMs. Additionally, there's mention of task-specific finetuning for various open-source LLMs, supported by LLMRec's prompt construction module.\n",
        "\n",
        "**Data Acquisition**: `?`\n",
        "\n",
        "Given this overview, the project focuses on using LLMs for various recommendation tasks, with an emphasis on prompt construction and evaluation."
      ],
      "metadata": {
        "id": "xtFNsK5HRBgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "IL5FH-bYRFBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "several utility functions for data processing, including filtering and converting JSON data, counting words in sentences, splitting and merging CSV files, and comparing CSV files."
      ],
      "metadata": {
        "id": "mgL6iicANcuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data format transform `func`"
      ],
      "metadata": {
        "id": "VjxyhdoMR-XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CR3j7QYHbsvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_json_data(input_file, data_type='generic', target_id=None):\n",
        "  \"\"\"\n",
        "  Processes and converts JSON data based on specified criteria and saves it as a new JSON file.\n",
        "\n",
        "  This function can handle both generic and recommendation-specific data. It filters and processes\n",
        "  entries in the input JSON file based on the specified criteria and saves the processed data to a new file.\n",
        "  The output file's name is derived from the input file's name and the provided criteria.\n",
        "\n",
        "  Parameters:\n",
        "  - input_file (str): Path to the input JSON file.\n",
        "  - data_type (str): Type of data to process ('generic' or 'recommendation').\n",
        "  - target_id (str, optional): ID for filtering data. Used only for recommendation data.\n",
        "                              If None, no filtering is applied.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize an empty list to store filtered data\n",
        "  filtered_data = []\n",
        "\n",
        "  # Construct the base of the output file name by removing the extension from the input file\n",
        "  output_file_name = os.path.splitext(input_file)[0]\n",
        "\n",
        "  # Determine the suffix for the output file based on data type and target ID\n",
        "  output_suffix = f\"_{data_type}\" + (f\"_{target_id}\" if target_id else '_all')\n",
        "\n",
        "  # Open and read the input file\n",
        "  with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      # Load each line as a JSON object\n",
        "      data = json.loads(line)\n",
        "\n",
        "      # Process recommendation data if specified\n",
        "      if data_type == 'recommendation' and (target_id is None or target_id == 'all' or data['task_type'] == target_id):\n",
        "        # Append the transformed data to the filtered_data list\n",
        "        filtered_data.append({\n",
        "          'instruction': data['source'],\n",
        "          'input': '',\n",
        "          'output': data['target']\n",
        "        })\n",
        "      elif data_type == 'generic':\n",
        "        # For generic data, append the data as is (or apply any generic transformation if needed)\n",
        "        filtered_data.append(data)\n",
        "\n",
        "  # Construct the full output file name\n",
        "  output_file = output_file_name + output_suffix + '.json'\n",
        "\n",
        "  # Write the filtered and processed data to the output file\n",
        "  with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for data in filtered_data:\n",
        "      # Write each piece of data as a JSON object, followed by a newline\n",
        "      f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "\n",
        "  # Print a confirmation message with the name of the output file\n",
        "  print(f\"Processed file saved as: {output_file}\")\n"
      ],
      "metadata": {
        "id": "ZvByTsqWY_Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(input_file):\n",
        "  \"\"\"\n",
        "  Calculates the maximum word count in sentences from a JSON file.\n",
        "\n",
        "  This function reads each line (sentence) from the specified JSON file and calculates the word count.\n",
        "  It keeps track of and prints the maximum word count found in the file. This is useful for analyzing\n",
        "  the length of text data.\n",
        "\n",
        "  Parameters:\n",
        "  - input_file (str): Path to the input JSON file.\n",
        "  \"\"\"\n",
        "  # Calculate the maximum word count in sentences from a JSON file\n",
        "  max_word_count = 0\n",
        "  with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      data = json.loads(line)\n",
        "      source_sentence = data['source']\n",
        "      # Count the number of words in the sentence (split by spaces)\n",
        "      word_count = len(source_sentence.split())\n",
        "      # Update the maximum word count if current sentence has more words\n",
        "      max_word_count = max(max_word_count, word_count)\n",
        "\n",
        "  print(f\"Maximum word count in a sentence in the file: {max_word_count}\")"
      ],
      "metadata": {
        "id": "c72pX02yWOxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_csv_file(file_path, num_parts):\n",
        "  \"\"\"\n",
        "  Splits a CSV file into multiple parts.\n",
        "\n",
        "  The function divides a CSV file into the specified number of parts and saves each part as a separate\n",
        "  CSV file. This is useful for handling large datasets or creating data splits for model training and testing.\n",
        "\n",
        "  Parameters:\n",
        "  - file_path (str): Path to the CSV file to be split.\n",
        "  - num_parts (int): Number of parts to split the file into.\n",
        "  \"\"\"\n",
        "  # Read the CSV file using pandas\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # Calculate the size of each part of the split\n",
        "  total_rows = len(df)\n",
        "  rows_per_part = total_rows // num_parts\n",
        "\n",
        "  # Split the data and save as multiple CSV files\n",
        "  file_name = os.path.basename(file_path)\n",
        "  file_name_without_ext = os.path.splitext(file_name)[0]\n",
        "  file_directory = os.path.dirname(file_path)\n",
        "\n",
        "  for i in range(num_parts):\n",
        "    start_index = i * rows_per_part\n",
        "    end_index = start_index + rows_per_part\n",
        "    # For the last part, include all remaining rows\n",
        "    if i == num_parts - 1:\n",
        "      end_index = total_rows\n",
        "\n",
        "    part_df = df[start_index:end_index]\n",
        "    part_file_name = f\"{file_name_without_ext}_{i + 1}.csv\"\n",
        "    part_file_path = os.path.join(file_directory, part_file_name)\n",
        "    part_df.to_csv(part_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "3zhdrt40WU7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_csv_files(file_name, output_file):\n",
        "  \"\"\"\n",
        "  Merges multiple CSV files into a single file.\n",
        "\n",
        "  This function finds all CSV files in the specified directory that start with the given file name\n",
        "  and merges them into a single CSV file. The merged file is saved to the specified output path.\n",
        "\n",
        "  Parameters:\n",
        "  - file_name (str): Base name of the files to be merged.\n",
        "  - output_file (str): Path for the output merged CSV file.\n",
        "  \"\"\"\n",
        "  # Locate all CSV files in the directory that match the specified file name\n",
        "  file_directory = os.path.dirname(output_file)\n",
        "  csv_files = [file for file in os.listdir(file_directory) if file.startswith(file_name) and file.endswith('.csv') and file != file_name + '.csv']\n",
        "  csv_files.sort()  # Sort the files by name\n",
        "\n",
        "  # Concatenate all found CSV files into a single DataFrame and save it\n",
        "  merged_df = pd.concat([pd.read_csv(os.path.join(file_directory, file)) for file in csv_files])\n",
        "  merged_df.to_csv(output_file, index=False)\n"
      ],
      "metadata": {
        "id": "aVuv3USwWcYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_csv_files(file1, file2):\n",
        "  \"\"\"\n",
        "  Compares two CSV files to check if they are identical.\n",
        "\n",
        "  The function reads two CSV files and compares them. It prints a message indicating whether\n",
        "  the files are completely identical or not.\n",
        "\n",
        "  Parameters:\n",
        "  - file1 (str): Path to the first CSV file.\n",
        "  - file2 (str): Path to the second CSV file.\n",
        "  \"\"\"\n",
        "  # Compare two CSV files to check if they are identical\n",
        "  df1 = pd.read_csv(file1)\n",
        "  df2 = pd.read_csv(file2)\n",
        "\n",
        "  # Print a message based on whether the DataFrames are identical\n",
        "  if df1.equals(df2):\n",
        "    print(\"The two CSV files are identical.\")\n",
        "  else:\n",
        "    print(\"The two CSV files are not identical.\")"
      ],
      "metadata": {
        "id": "W64E4JKuWgEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  # Example usage and testing code\n",
        "  # Uncomment and modify according to the specific use case\n",
        "  parser = argparse.ArgumentParser(description='Filter and convert JSON file.')\n",
        "  parser.add_argument('input_file', type=str, help='Path to the input JSON file', default='data/train_prompt.json')\n",
        "  parser.add_argument('target_id', type=str, nargs='?', default=None, help='Target ID for filtering (optional)')\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  input_file_path = args.input_file\n",
        "  target_id = args.target_id\n",
        "\n",
        "  filter_and_convert(input_file_path, target_id)\n",
        "  file_path = 'data/test.csv'\n",
        "  num_parts = 3\n",
        "  split_csv_file(file_path, num_parts)\n",
        "  merge_csv_files('test', 'data/combine_test.csv')\n",
        "  file1 = 'data/test.csv'\n",
        "  file2 = 'data/combine_test.csv'\n",
        "  compare_csv_files(file1, file2)\n",
        "  word_count('data/chatglm-data-full/full_train.json')"
      ],
      "metadata": {
        "id": "v3M8TYxRSSGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Templates `JSON`"
      ],
      "metadata": {
        "id": "7i7gFxHiz2LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Template = {\n",
        "    \"description\": \"Template for LLMRec\",\n",
        "    \"prompt_input\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "    \"prompt_no_input\": \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "    \"response_split\": \"### Response:\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "1lmDrNyj0E0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `rec_template.json` file contains the structure for the prompt templates used in the LLMRec system. Here is a breakdown of its contents:\n",
        "\n",
        "1. **Description**: It is labeled as 'Template for LLMRec', indicating its purpose in the system.\n",
        "\n",
        "2. **Prompt with Input (`prompt_input`)**:\n",
        "   - This template is used when there is an input provided for the prompt.\n",
        "   - The format includes an 'Instruction' section, an 'Input' section, and a placeholder for 'Response'.\n",
        "   - The placeholders `{instruction}` and `{input}` will be replaced with actual instruction and input data when generating a prompt.\n",
        "\n",
        "3. **Prompt without Input (`prompt_no_input`)**:\n",
        "   - This template is used when there is no input data to include in the prompt.\n",
        "   - It only includes an 'Instruction' section followed by a placeholder for 'Response'.\n",
        "\n",
        "4. **Response Split (`response_split`)**:\n",
        "   - This is a marker used to split the generated text to extract the 'Response' part.\n",
        "   - It is set to '### Response:', which means this marker is used to identify where the response begins in the model's output.\n"
      ],
      "metadata": {
        "id": "Q5N7Gq6G0C59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompter `class`"
      ],
      "metadata": {
        "id": "hN5JL67ExQmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The design of the `Prompter` class indicates a flexible approach to generating prompts for recommendation tasks, where the specifics of each prompt are determined by a `JSON` template. This aligns with the concept of transforming recommendation tasks into natural language tasks, as described in the article."
      ],
      "metadata": {
        "id": "YsUj3PvYxjRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "class Prompter(object):\n",
        "\n",
        "    # Slots for memory efficiency (instances of this class can only have these two attributes, and no other attributes can be added dynamically.)\n",
        "    __slots__ = (\"template\", \"_verbose\")\n",
        "\n",
        "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
        "        \"\"\"\n",
        "        Initialize the Prompter with a specific template.\n",
        "\n",
        "        Parameters:\n",
        "        - template_name (str): The name of the template file to use for generating prompts.\n",
        "        - verbose (bool): If True, the prompter will print additional information.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        self._verbose = verbose\n",
        "\n",
        "        # Set a default template if not provided\n",
        "        if not template_name:\n",
        "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
        "            template_name = \"alpaca\"\n",
        "        # Load the template file\n",
        "        file_name = osp.join(\"./templates\", f\"{template_name}.json\")\n",
        "        if not osp.exists(file_name):\n",
        "            raise ValueError(f\"Can't read {file_name}\")\n",
        "\n",
        "        with open(file_name) as fp:\n",
        "            self.template = json.load(fp)\n",
        "\n",
        "        if self._verbose:\n",
        "            print(\n",
        "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
        "            )\n",
        "\n",
        "    def generate_prompt(self, instruction: str, input: Union[None, str] = None, label: Union[None, str] = None,) -> str:\n",
        "        ''''\n",
        "        Generate a prompt based on the provided instruction and optional input/label\n",
        "\n",
        "        Parameters:\n",
        "        - instruction (str): Instruction or context for the prompt.\n",
        "        - input (Union[None, str]): Optional input data to include in the prompt.\n",
        "        - label (Union[None, str]): Optional label or expected output to append.\n",
        "\n",
        "        Returns:\n",
        "        str: The generated prompt.\n",
        "        '''\n",
        "\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "\n",
        "        # Append label (response/output) if provided\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "\n",
        "        # Print the prompt if verbose mode is on\n",
        "        if self._verbose:\n",
        "            print(res)\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the response part from the model's output.\n",
        "\n",
        "        Parameters:\n",
        "        - output (str): The complete output text from the model.\n",
        "\n",
        "        Returns:\n",
        "        str: The extracted response portion of the output.\n",
        "        \"\"\"\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()\n"
      ],
      "metadata": {
        "id": "NtHbWczFvbBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generation"
      ],
      "metadata": {
        "id": "TMkZY5Cc3Lnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will involve extracting the core functionality needed for generating prompts and obtaining model outputs, and then restructuring it for interactive use in a Jupyter notebook."
      ],
      "metadata": {
        "id": "5wDZ_MhL5hOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install peft"
      ],
      "metadata": {
        "id": "SlFJYpbGENw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel # A model wrapper for performance improvements\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "# from utils.prompter import Promper # Custom class for managing prompt templates"
      ],
      "metadata": {
        "id": "v_tuQONA6myO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load model\n",
        "Matches and sets up the device for the model, and loads the model accordingly."
      ],
      "metadata": {
        "id": "6eKGFC946vAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(base_model: str = \"\", load_8bit: bool = False, lora_weights: str = \"tloen/alpaca-lora-7b\"):\n",
        "  \"\"\"\n",
        "  Matches and sets up the device for the model, and loads the model accordingly and config it.\n",
        "\n",
        "  Parameters:\n",
        "  - base_model (str): The base model identifier.\n",
        "  - load_8bit (bool): Whether to load the model in 8-bit precision for better performance.\n",
        "  - lora_weights (str): Path or identifier for LoRA weights for the model.\n",
        "\n",
        "  Returns:\n",
        "  The loaded model, set up for the appropriate device.\n",
        "  \"\"\"\n",
        "  # Ensure that a base model is specified\n",
        "  assert (\n",
        "      base_model\n",
        "  ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n",
        "\n",
        "  # Determine the device: CUDA, MPS, or CPU\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "  # Common parameters for model loading\n",
        "  common_params = {\n",
        "    \"base_model\": base_model,\n",
        "    \"torch_dtype\": torch.float16 if device != \"cpu\" else None,\n",
        "    \"device_map\": {\"\": device}\n",
        "  }\n",
        "\n",
        "  # Load the base model with specific settings based on the device\n",
        "  match device:\n",
        "    case \"cuda\":\n",
        "      common_params.update({\"load_in_8bit\": load_8bit})\n",
        "    case \"mps\":\n",
        "      pass  # No additional parameters for MPS\n",
        "    case _:\n",
        "      common_params.update({\"low_cpu_mem_usage\": True})\n",
        "\n",
        "  model = LlamaForCausalLM.from_pretrained(**common_params)\n",
        "  # Load PeftModel with LoRA weights\n",
        "  model = PeftModel.from_pretrained(model, lora_weights, **common_params)\n",
        "\n",
        "  # Unwind broken configurations from a specific source, possibly due to model incompatibilities or custom requirements\n",
        "  model.config.pad_token_id = tokenizer.pad_token_id = 0  # Set padding token ID to 0 (typically unknown token)\n",
        "  model.config.bos_token_id = 1  # Set beginning of sentence token ID\n",
        "  model.config.eos_token_id = 2  # Set end of sentence token ID\n",
        "\n",
        "  # Convert the model to half precision for improved performance, unless 8-bit loading is specified\n",
        "  if not load_8bit:\n",
        "    model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Compile the model with TorchScript if using PyTorch version 2 or above and not on Windows platform\n",
        "  if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "t2kVE2cd6uaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate\n",
        "Main function to set up and run the language model for generating outputs based on prompts"
      ],
      "metadata": {
        "id": "ZmzJIOJ7El_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluate function to generate responses based on instructions and input\n",
        "def generate(base_model, instruction, input=None, prompt_template: str = \"med_template\",\n",
        "             temperature=0.1, top_p=0.75, top_k=40, num_beams=4, max_new_tokens=128, **kwargs,):\n",
        "  \"\"\"\n",
        "  Generate a response from the model based on the given instruction and optional input.\n",
        "\n",
        "  This function takes an instruction (and optionally additional input), generates a prompt,\n",
        "  and then uses the model to generate a response based on this prompt. The function allows\n",
        "  configuration of various generation parameters like temperature, top_p, top_k, and num_beams.\n",
        "\n",
        "  Parameters:\n",
        "  - base_model (str): The base model for language generation to be used.\n",
        "  - instruction (str): The instruction or query to be processed by the model.\n",
        "  - input (str, optional): Additional input data to be included in the prompt.\n",
        "  - temperature (float): The temperature for controlling randomness in response generation.\n",
        "  - top_p (float): Nucleus sampling parameter controlling the size of the probability mass considered for sampling.\n",
        "  - top_k (int): Top-k sampling parameter controlling the number of highest probability vocabulary tokens to keep for sampling.\n",
        "  - num_beams (int): Number of beams for beam search. More beams increase the chances of finding more diverse solutions but are slower.\n",
        "  - max_new_tokens (int): The maximum number of new tokens to generate in the response.\n",
        "  - prompt_template (str): Name of the prompt template to use.\n",
        "  - **kwargs: Additional keyword arguments for generation configuration.\n",
        "\n",
        "  Returns:\n",
        "  str: The generated response from the model.\n",
        "  \"\"\"\n",
        "  # Initialize the prompter with the specified template\n",
        "  prompter = Prompter(prompt_template)\n",
        "\n",
        "  # Load the tokenizer for the specified base model\n",
        "  tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "\n",
        "  # Generate the prompt using the provided instruction and input\n",
        "  prompt = prompter.generate_prompt(instruction, input)\n",
        "\n",
        "  # Tokenize the prompt for model input\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids = inputs[\"input_ids\"].to(device)\n",
        "\n",
        "  # Configure generation parameters (temperature, top_p, etc.)\n",
        "  generation_config = GenerationConfig(\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    top_k=top_k,\n",
        "    num_beams=num_beams,\n",
        "    **kwargs,\n",
        "  )\n",
        "\n",
        "  # Load the model based on the device and settings\n",
        "  model = load_model(base_model=base_model)\n",
        "\n",
        "  # Generate the output without computing gradients (for efficiency)\n",
        "  with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      generation_config=generation_config,\n",
        "      return_dict_in_generate=True,\n",
        "      output_scores=True,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "  # Decode the generated sequence to human-readable text\n",
        "  s = generation_output.sequences[0]\n",
        "  output = tokenizer.decode(s)\n",
        "  # Extract and return the response from the output\n",
        "  return prompter.get_response(output)\n"
      ],
      "metadata": {
        "id": "zOFU0HqhE2Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "base_model = \"decapoda-research/llama-7b-hf\"\n",
        "prompt_template = \"med_template\"\n",
        "generation_params = {\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.75,\n",
        "    \"top_k\": 40,\n",
        "    \"num_beams\": 4,\n",
        "    \"max_new_tokens\": 128\n",
        "}\n",
        "\n",
        "# Load the model\n",
        "model = load_model(base_model)\n",
        "\n",
        "# Generate a response\n",
        "instruction = \"Tell me about alpacas.\"\n",
        "input_data = \"\"\n",
        "response = generate(model, instruction, input_data, prompt_template, generation_params)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "JhH4lssHgWJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old testing code follows.\n",
        "\n",
        "\"\"\"\n",
        "# testing code for readme\n",
        "for instruction in [\n",
        "    \"Tell me about alpacas.\",\n",
        "    \"Tell me about the president of Mexico in 2019.\",\n",
        "    \"Tell me about the king of France in 2019.\",\n",
        "    \"List all Canadian provinces in alphabetical order.\",\n",
        "    \"Write a Python program that prints the first 10 Fibonacci numbers.\",\n",
        "    \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n",
        "    \"Tell me five words that rhyme with 'shock'.\",\n",
        "    \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n",
        "    \"Count up from 1 to 500.\",\n",
        "]:\n",
        "    print(\"Instruction:\", instruction)\n",
        "    print(\"Response:\", evaluate(instruction))\n",
        "    print()\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "jEQAmJQ2j-QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# infer_rec"
      ],
      "metadata": {
        "id": "37tsqEHQu5iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "# import fire  # Library for creating command line interfaces\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel  # Performance-efficient model wrapper\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "# from utils.prompter import Prompter  # Custom class for handling prompt generation\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # Library for progress bars\n",
        "\n",
        "# Set environment variables for Transformers cache\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/data/private/peilin/cache'\n",
        "os.environ['HF_HOME'] = '/data/private/peilin/cache'\n",
        "# Uncomment to specify GPU device\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "# Check if CUDA (GPU support) is available and set the device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "\n",
        "def load_instruction(instruct_dir):\n",
        "    \"\"\"\n",
        "    Load instructions from a JSON file.\n",
        "\n",
        "    Parameters:\n",
        "    - instruct_dir (str): Path to the instruction JSON file.\n",
        "\n",
        "    Returns:\n",
        "    A list of instructions loaded from the file.\n",
        "    \"\"\"\n",
        "    input_data = []\n",
        "    with open(instruct_dir, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            d = json.loads(line)\n",
        "            input_data.append(d)\n",
        "    return input_data\n",
        "\n",
        "def load_instruction_from_csv(instruct_dir, prompt_idx='all'):\n",
        "    \"\"\"\n",
        "    Load instructions from a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - instruct_dir (str): Path to the instruction CSV file.\n",
        "    - prompt_idx (str): Specific prompt index to load. Defaults to 'all'.\n",
        "\n",
        "    Returns:\n",
        "    A tuple containing the list of instructions and the dataframe.\n",
        "    \"\"\"\n",
        "    input_data = []\n",
        "    df = pd.read_csv(instruct_dir, dtype='str', usecols=['prompt', 'target', 'task', 'few_zero'])\n",
        "    dict_from_df = df.to_dict(orient='index')\n",
        "    for key, value in dict_from_df.items():\n",
        "        data = {\n",
        "            'output': value['target'].strip(),\n",
        "            'instruction': value['prompt'].strip()\n",
        "        }\n",
        "        input_data.append(data)\n",
        "    return input_data, df\n",
        "\n",
        "def main(\n",
        "    # Parameters for the main function\n",
        "    load_8bit: bool = False,\n",
        "    base_model: str = \"decapoda-research/llama-7b-hf\",\n",
        "    instruct_dir: str = \"./data/beauty_sequential_single_prompt_test_sample.json\",\n",
        "    output_dir: str = \"output/\",\n",
        "    use_lora: bool = False,\n",
        "    lora_weights: str = \"tloen/alpaca-lora-7b\",\n",
        "    prompt_template: str = \"rec_template\",\n",
        "    max_new_tokens: int = 10,\n",
        "    num_return_sequences: int = 10,\n",
        "    num_beams: int = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Main function for fine-tuning and evaluating a language model.\n",
        "\n",
        "    Parameters:\n",
        "    - load_8bit (bool): Flag to indicate if 8-bit precision is to be used.\n",
        "    - base_model (str): Identifier for the base language model.\n",
        "    - instruct_dir (str): Directory containing the instruction data.\n",
        "    - output_dir (str): Directory to save output data.\n",
        "    - use_lora (bool): Flag to indicate if LoRA weights are to be used.\n",
        "    - lora_weights (str): Identifier for LoRA weights.\n",
        "    - prompt_template (str): Template for generating prompts.\n",
        "    - max_new_tokens (int): Maximum number of new tokens to generate.\n",
        "    - num_return_sequences (int): Number of return sequences.\n",
        "    - num_beams (int): Number of beams for beam search.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Initialize the prompter, tokenizer, and model\n",
        "    prompter = Prompter(prompt_template)\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,device_map=\"auto\",)\n",
        "    if use_lora:\n",
        "    print(f\"using lora {lora_weights}\")\n",
        "    model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    lora_weights,\n",
        "    torch_dtype=torch.float16,\n",
        "    )\n",
        "    # Adjust the model configuration to handle specific token ids\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # Set pad token id\n",
        "    model.config.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    model.config.eos_token_id = 2  # Set end of sentence token id\n",
        "\n",
        "    # Convert the model to half precision if 8-bit loading is not used\n",
        "    if not load_8bit:\n",
        "        model.half()  # This can fix bugs for some users\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Compile the model with TorchScript for performance optimization\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    def evaluate(\n",
        "        instruction,\n",
        "        input=None,\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        top_k=40,\n",
        "        num_beams=num_beams,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a given instruction and input.\n",
        "\n",
        "        Parameters:\n",
        "        - instruction (str): Instruction or query to be processed.\n",
        "        - input (str, optional): Additional input data.\n",
        "        - temperature, top_p, top_k, num_beams, max_new_tokens, num_return_sequences: Model generation settings.\n",
        "\n",
        "        Returns:\n",
        "        A list of generated answers.\n",
        "        \"\"\"\n",
        "        prompt = prompter.generate_prompt(instruction, input)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            num_beams=num_beams,\n",
        "            **kwargs,\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "            )\n",
        "        answer_list = []\n",
        "        for i in range(num_return_sequences):\n",
        "            s = generation_output.sequences[i]\n",
        "            output = tokenizer.decode(s)\n",
        "            answer_list.append(prompter.get_response(output))\n",
        "        return '\\t'.join(answer_list)\n",
        "\n",
        "    def write_to_json(data, output_dir):\n",
        "        \"\"\"\n",
        "        Write data to a JSON file.\n",
        "\n",
        "        Parameters:\n",
        "        - data: Data to be written.\n",
        "        - output_dir (str): Path to the output JSON file.\n",
        "        \"\"\"\n",
        "        with open(output_dir, 'w') as f:\n",
        "            for item in data:\n",
        "                json.dump(item, f)\n",
        "                f.write('\\n')\n",
        "\n",
        "    def infer_from_json(instruct_dir):\n",
        "        \"\"\"\n",
        "        Perform inference from a JSON file and save the results.\n",
        "\n",
        "        Parameters:\n",
        "        - instruct_dir (str): Path to the instruction JSON file.\n",
        "        \"\"\"\n",
        "        input_data = load_instruction(instruct_dir)\n",
        "        output_data = []\n",
        "        for d in tqdm(input_data):\n",
        "            instruction = d[\"source\"]\n",
        "            output = d[\"target\"]\n",
        "            model_output = evaluate(instruction)\n",
        "            task_type = d[\"task_type\"]\n",
        "            output_data.append({'labels': d['target'], 'predict': model_output})\n",
        "        file_name = \"rec_{}_{}.json\".format(task_type, base_model.split('/')[-1]) if not use_lora else 'rec_{}_{}_{}.json'.format(task_type, base_model.split('/')[-1], lora_weights.split('/')[-1])\n",
        "        output_path = os.path.join(output_dir, file_name)\n",
        "        write_to_json(output_data, output_path)\n",
        "\n",
        "    # Conditional execution based on the provided instruction directory\n",
        "    if instruct_dir != \"\":\n",
        "        filename, file_extension = os.path.splitext(instruct_dir)\n",
        "        if file_extension[1:] == 'json':\n",
        "            infer_from_json(instruct_dir)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "    else:\n",
        "        raise ValueError(\"Instruction directory not provided\")\n",
        "\n",
        "if name == \"main\":\n",
        "fire.Fire(main)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "plCK3ihNL0tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5BI_QzqcJ4v"
      },
      "source": [
        "# finetune_rec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import List\n",
        "import fire  # Library for creating command line interfaces\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset  # Library for loading datasets\n",
        "\n",
        "# Import LoRA related modules\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        ")\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from utils.data_format_transform import filter_and_convert_rec\n"
      ],
      "metadata": {
        "id": "wjfUvjz-VLEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjCC0YSA5a7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    # Parameters for the train function\n",
        "    base_model: str = \"decapoda-research/llama-7b-hf\",  # Base model identifier\n",
        "    data_path: str = \"data/beauty_sequential_single_prompt_train_sample.json\",  # Training data path\n",
        "    valid_data_path: str = \"data/beauty_sequential_single_prompt_test.json\",  # Validation data path\n",
        "    output_dir: str = \"./lora-alpaca\",  # Output directory for the trained model\n",
        "    # Training hyperparameters\n",
        "    batch_size: int = 32,\n",
        "    micro_batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    learning_rate: float = 3e-4,\n",
        "    cutoff_len: int = 256,  # Max sequence length\n",
        "    val_set_size: int = 500,  # Size of validation set\n",
        "    save_eval_steps: int = 10,  # Steps interval for saving and evaluating\n",
        "    # LoRA hyperparameters\n",
        "    lora_r: int = 8,\n",
        "    lora_alpha: int = 16,\n",
        "    lora_dropout: float = 0.05,\n",
        "    lora_target_modules: List[str] = [\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
        "    # LLM hyperparameters\n",
        "    train_on_inputs: bool = False,  # If False, masks out inputs in loss calculation\n",
        "    group_by_length: bool = False,  # Grouping strategy for efficiency\n",
        "    # WandB (Weights & Biases) parameters for experiment tracking\n",
        "    wandb_project: str = \"llama_med\",\n",
        "    wandb_run_name: str = \"\",\n",
        "    wandb_watch: str = \"\",\n",
        "    wandb_log_model: str = \"\",\n",
        "    resume_from_checkpoint: str = None,  # Path to resume training from a checkpoint\n",
        "    prompt_template_name: str = \"alpaca\",  # Default prompt template\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains the LlamaForCausalLM model with LoRA modifications.\n",
        "\n",
        "    Parameters:\n",
        "    - base_model (str): Identifier for the base pre-trained model.\n",
        "    - data_path (str): Path to the training data file.\n",
        "    - valid_data_path (str): Path to the validation data file.\n",
        "    - output_dir (str): Directory to save the trained model and outputs.\n",
        "    - Other parameters include various training settings, LoRA configurations, and WandB settings.\n",
        "    \"\"\"\n",
        "    # Ensure only the primary process prints the training parameters\n",
        "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
        "        # Printing training parameters for reference\n",
        "        print(f\"Training parameters: {locals()}\")\n",
        "\n",
        "    # Ensure the base model is specified\n",
        "    assert base_model, \"Base model must be specified.\"\n",
        "\n",
        "    # Calculate the gradient accumulation steps\n",
        "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "\n",
        "    # Initialize the prompter with the specified template\n",
        "    prompter = Prompter(prompt_template_name)\n",
        "\n",
        "    # Set up device mapping for distributed data parallel (DDP) if applicable\n",
        "    device_map = \"auto\"\n",
        "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "    ddp = world_size != 1\n",
        "    if ddp:\n",
        "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "        gradient_accumulation_steps //= world_size\n",
        "\n",
        "    # Determine if Weights & Biases (WandB) will be used for experiment tracking\n",
        "    use_wandb = len(wandb_project) > 0 or (\n",
        "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
        "    )\n",
        "    # Set WandB environment variables if specified\n",
        "    if len(wandb_project) > 0:\n",
        "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
        "    if len(wandb_watch) > 0:\n",
        "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
        "    if len(wandb_log_model) > 0:\n",
        "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
        "\n",
        "    # Load the base model\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=device_map,\n",
        "    )\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token_id = 0  # Set pad token id to 'unk' (unknown)\n",
        "    tokenizer.padding_side = \"left\"  # Set padding side for batched inference\n",
        "\n",
        "    # Tokenization function\n",
        "    def tokenize(prompt, add_eos_token=True):\n",
        "        \"\"\"\n",
        "        Tokenizes the prompt with optional EOS token addition.\n",
        "        \"\"\"\n",
        "        result = tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            max_length=cutoff_len,\n",
        "            padding=False,\n",
        "            return_tensors=None,\n",
        "        )\n",
        "        # Add EOS token if needed and if space allows\n",
        "        if (result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "                and len(result[\"input_ids\"]) < cutoff_len\n",
        "                and add_eos_token):\n",
        "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "            result[\"attention_mask\"].append(1)\n",
        "\n",
        "        # Set labels for training\n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "        return result\n",
        "\n",
        "    # Function to generate and tokenize prompts\n",
        "    def generate_and_tokenize_prompt(data_point):\n",
        "        \"\"\"\n",
        "        Generates a full prompt from the data point and tokenizes it.\n",
        "        \"\"\"\n",
        "        full_prompt = prompter.generate_prompt(\n",
        "            data_point[\"source\"],\n",
        "            None,\n",
        "            data_point[\"target\"],\n",
        "        )\n",
        "        tokenized_full_prompt = tokenize(full_prompt)\n",
        "        if not train_on_inputs:\n",
        "            user_prompt = prompter.generate_prompt(\n",
        "                data_point[\"source\"], None\n",
        "            )\n",
        "            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
        "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "\n",
        "            tokenized_full_prompt[\"labels\"] = [\n",
        "                -100\n",
        "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
        "                user_prompt_len:\n",
        "            ]  # could be sped up, probably\n",
        "        return tokenized_full_prompt\n",
        "\n",
        "    # Prepare the model for int8 training (for efficient training)\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "\n",
        "    # Configure LoRA settings\n",
        "    config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=lora_target_modules,\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    # Load dataset\n",
        "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
        "        data = load_dataset(\"json\", data_files=data_path)\n",
        "    else:\n",
        "        data = load_dataset(data_path)\n",
        "\n",
        "    if os.path.exists(valid_data_path):\n",
        "        if valid_data_path.endswith(\".json\") or valid_data_path.endswith(\".jsonl\"):\n",
        "            valid_data = load_dataset(\"json\", data_files=valid_data_path)\n",
        "        else:\n",
        "            valid_data = load_dataset(data_path)\n",
        "    else:\n",
        "        valid_data = None\n",
        "\n",
        "    if resume_from_checkpoint:\n",
        "        # Check the available weights and load them\n",
        "        checkpoint_name = os.path.join(\n",
        "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
        "        )  # Full checkpoint\n",
        "        if not os.path.exists(checkpoint_name):\n",
        "            checkpoint_name = os.path.join(\n",
        "                resume_from_checkpoint, \"adapter_model.bin\"\n",
        "            )  # only LoRA model - LoRA config above has to fit\n",
        "            resume_from_checkpoint = (\n",
        "                False  # So the trainer won't try loading its state\n",
        "            )\n",
        "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
        "        if os.path.exists(checkpoint_name):\n",
        "            print(f\"Restarting from {checkpoint_name}\")\n",
        "            adapters_weights = torch.load(checkpoint_name)\n",
        "            model = set_peft_model_state_dict(model, adapters_weights)\n",
        "        else:\n",
        "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
        "\n",
        "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
        "\n",
        "    if val_set_size > 0 and not valid_data:\n",
        "        train_val = data[\"train\"].train_test_split(\n",
        "            test_size=val_set_size, shuffle=True, seed=2023\n",
        "        )\n",
        "        train_data = (\n",
        "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "        )\n",
        "        val_data = (\n",
        "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "        )\n",
        "    elif val_set_size > 0 and valid_data:\n",
        "        train_data = (\n",
        "            data[\"train\"].shuffle(seed=2023).map(generate_and_tokenize_prompt)\n",
        "        )\n",
        "        val_sample = valid_data[\"train\"].train_test_split(\n",
        "            test_size=val_set_size, shuffle=True, seed=2023\n",
        "        )\n",
        "        val_data = (\n",
        "            val_sample[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "        )\n",
        "    else:\n",
        "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "        val_data = None\n",
        "\n",
        "    if not ddp and torch.cuda.device_count() > 1:\n",
        "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
        "        model.is_parallelizable = True\n",
        "        model.model_parallel = True\n",
        "\n",
        "    # Set up the trainer\n",
        "    # ... Trainer configuration and initialization\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        model=model,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        args=transformers.TrainingArguments(\n",
        "            per_device_train_batch_size=micro_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            warmup_ratio=0.1,\n",
        "            num_train_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            fp16=True,\n",
        "            logging_steps=8,\n",
        "            optim=\"adamw_torch\",\n",
        "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
        "            save_strategy=\"steps\",\n",
        "            eval_steps=save_eval_steps if val_set_size > 0 else None,\n",
        "            save_steps=save_eval_steps,\n",
        "            output_dir=output_dir,\n",
        "            save_total_limit=1000,\n",
        "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
        "            ddp_find_unused_parameters=False if ddp else None,\n",
        "            group_by_length=group_by_length,\n",
        "            report_to=\"wandb\" if use_wandb else None,\n",
        "            run_name=wandb_run_name if use_wandb else None,\n",
        "        ),\n",
        "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
        "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "        ),\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "\n",
        "\n",
        "    # Adjust model's state_dict for PEFT\n",
        "    old_state_dict = model.state_dict\n",
        "    model.state_dict = (\n",
        "        lambda self, *_, **__: get_peft_model_state_dict(\n",
        "            self, old_state_dict()\n",
        "        )\n",
        "    ).__get__(model, type(model))\n",
        "\n",
        "    # Compile the model with TorchScript for better performance if applicable\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # Start the training process\n",
        "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    print(\"\\nTraining completed successfully.\")\n",
        "\n",
        "if name == \"main\":\n",
        "fire.Fire(train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HwDfbXqMs_OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_wandb(wandb_project, wandb_watch, wandb_log_model):\n",
        "  \"\"\"\n",
        "  Sets up Weights & Biases (WandB) environment variables and configuration.\n",
        "  \"\"\"\n",
        "  use_wandb = len(wandb_project) > 0 or (\n",
        "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
        "  )\n",
        "  if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
        "  if len(wandb_watch) > 0:\n",
        "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
        "  if len(wandb_log_model) > 0:\n",
        "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
        "  return use_wandb"
      ],
      "metadata": {
        "id": "ygNSsdIl7SK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model"
      ],
      "metadata": {
        "id": "8Vwz3MOXIrk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(base_model, config, wandb_project, wandb_watch, wandb_log_model):\n",
        "    \"\"\"\n",
        "    Matches and sets up the device for the model, and loads the model accordingly and config it.\n",
        "    \"\"\"\n",
        "\n",
        "    # Configures device mapping for distributed data parallel (DDP) if applicable.\n",
        "    device_map = \"auto\"\n",
        "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "    ddp = world_size != 1\n",
        "    if ddp:\n",
        "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "        gradient_accumulation_steps //= world_size\n",
        "\n",
        "    # Determine if Weights & Biases (WandB) will be used for experiment tracking\n",
        "    use_wandb = initialize_wandb(wandb_project, wandb_watch, wandb_log_model)\n",
        "    # Load the base model\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        "    )\n",
        "\n",
        "    # Prepare the model for int8 training (for efficient training)\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "\n",
        "    # Configure LoRA settings\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    if resume_from_checkpoint:\n",
        "    # Check the available weights and load them\n",
        "    checkpoint_name = os.path.join(\n",
        "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
        "    )  # Full checkpoint\n",
        "    if not os.path.exists(checkpoint_name):\n",
        "        checkpoint_name = os.path.join(\n",
        "            resume_from_checkpoint, \"adapter_model.bin\"\n",
        "        )  # only LoRA model - LoRA config above has to fit\n",
        "        resume_from_checkpoint = (\n",
        "            False  # So the trainer won't try loading its state\n",
        "        )\n",
        "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
        "    if os.path.exists(checkpoint_name):\n",
        "        print(f\"Restarting from {checkpoint_name}\")\n",
        "        adapters_weights = torch.load(checkpoint_name)\n",
        "        model = set_peft_model_state_dict(model, adapters_weights)\n",
        "    else:\n",
        "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
        "\n",
        "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
        "\n",
        "    if not ddp and torch.cuda.device_count() > 1:\n",
        "      # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
        "      model.is_parallelizable = True\n",
        "      model.model_parallel = True\n",
        "\n",
        "    return model, use_wandb"
      ],
      "metadata": {
        "id": "GCfs74gU_wRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenize"
      ],
      "metadata": {
        "id": "oIUmqT-V_w3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_tokenizer(base_model):\n",
        "  \"\"\"\n",
        "  Initializes the tokenizer and sets pad token and padding side.\n",
        "  \"\"\"\n",
        "  tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "  tokenizer.pad_token_id = 0  # Set pad token id to 'unk' (unknown)\n",
        "  tokenizer.padding_side = \"left\"  # Set padding side for batched inference\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "YQ6G4Ezf8enO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenize(prompt, tokenizer, cutoff_len, add_eos_token=True):\n",
        "  \"\"\"\n",
        "  Tokenizes the prompt with optional EOS token addition.\n",
        "  \"\"\"\n",
        "  result = tokenizer(\n",
        "    prompt,\n",
        "    truncation=True,\n",
        "    max_length=cutoff_len,\n",
        "    padding=False,\n",
        "    return_tensors=None,\n",
        "  )\n",
        "  # Add EOS token if needed and if space allows\n",
        "  if (result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "        and len(result[\"input_ids\"]) < cutoff_len\n",
        "        and add_eos_token):\n",
        "    result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "    result[\"attention_mask\"].append(1)\n",
        "\n",
        "  # Set labels for training\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "Vyvovr3b-e8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and tokenize prompts\n",
        "def generate_and_tokenize_prompt(data_point, train_on_inputs, prompter, cutoff_len, tokenizer):\n",
        "  \"\"\"\n",
        "  Generates a full prompt from the data point and tokenizes it.\n",
        "  \"\"\"\n",
        "  full_prompt = prompter.generate_prompt(\n",
        "    data_point[\"source\"],\n",
        "    None,\n",
        "    data_point[\"target\"],\n",
        "  )\n",
        "  tokenized_full_prompt = tokenize(full_prompt, cutoff_len=cutoff_len, tokenizer=tokenizer)\n",
        "  if not train_on_inputs:\n",
        "    user_prompt = prompter.generate_prompt(\n",
        "      data_point[\"source\"], None\n",
        "    )\n",
        "    tokenized_user_prompt = tokenize(user_prompt, cutoff_len=cutoff_len,\n",
        "                                     tokenizer=tokenizer, add_eos_token=False)\n",
        "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "\n",
        "    tokenized_full_prompt[\"labels\"] = [\n",
        "      -100\n",
        "    ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
        "      user_prompt_len:\n",
        "    ]  # could be sped up, probably\n",
        "  return tokenized_full_prompt"
      ],
      "metadata": {
        "id": "IfaQR3bP-QOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data"
      ],
      "metadata": {
        "id": "7MAH51X5IFKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_dataset(data_path, valid_data_path, val_set_size, prompter, cutoff_len, tokenizer, train_on_inputs):\n",
        "    \"\"\"\n",
        "    Load training and validation datasets.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the training data file.\n",
        "        valid_data_path (str): Path to the validation data file.\n",
        "        val_set_size (int): Size of the validation set.\n",
        "        generate_and_tokenize_prompt (function): Function to generate and tokenize prompts.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the training and validation datasets.\n",
        "    \"\"\"\n",
        "    generate_and_tokenize_prompt = lambda data_point: generate_and_tokenize_prompt(data_point, train_on_inputs=train_on_inputs, cutoff_len=cutoff_len, prompter=prompter, tokenizer=tokenizer)\n",
        "    # Helper function to load dataset based on file extension\n",
        "    def load_data(path):\n",
        "        if path.endswith(\".json\") or path.endswith(\".jsonl\"):\n",
        "            return load_dataset(\"json\", data_files=path)\n",
        "        return load_dataset(path)\n",
        "\n",
        "    # Load datasets\n",
        "    data = load_data(data_path)\n",
        "    valid_data = load_data(valid_data_path) if os.path.exists(valid_data_path) else None\n",
        "\n",
        "    # Split and prepare datasets\n",
        "    if val_set_size > 0:\n",
        "        if valid_data:\n",
        "            train_data = data[\"train\"].shuffle(seed=2023).map(generate_and_tokenize_prompt)\n",
        "            val_data = valid_data[\"train\"].train_test_split(\n",
        "                test_size=val_set_size, shuffle=True, seed=2023\n",
        "            )[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "        else:\n",
        "            train_val = data[\"train\"].train_test_split(\n",
        "                test_size=val_set_size, shuffle=True, seed=2023\n",
        "            )\n",
        "            train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "            val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "    else:\n",
        "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "        val_data = None\n",
        "\n",
        "    return train_data, val_data"
      ],
      "metadata": {
        "id": "4S1PHBxJBnEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train model"
      ],
      "metadata": {
        "id": "3j7NaQGvIM7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# d = {'x':[1,2,3], 'y':[4,5,6]}\n",
        "# df = pd.DataFrame(d)\n",
        "# df['x'].map(lambda x: f(x, y=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "1GWVdL1pNG0H",
        "outputId": "d1fd03c5-2392-4253-c15e-3b0df42878cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "f() got an unexpected keyword argument 'y'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-228ef698584a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4537\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4538\u001b[0m         \"\"\"\n\u001b[0;32m-> 4539\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4540\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-228ef698584a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: f() got an unexpected keyword argument 'y'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def f(x):\n",
        "#   x = [1,2,3]\n",
        "#   y = [2]\n",
        "#   return y\n",
        "# x = [7]\n",
        "# y = f(x)\n",
        "# print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WMcs5KMO8ZY",
        "outputId": "f398d39d-9fb3-4e43-f290-f5d6255170bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training process function\n",
        "def train_model(\n",
        "    # Parameters for the train function\n",
        "    base_model: str = \"decapoda-research/llama-7b-hf\",  # Base model identifier\n",
        "    data_path: str = \"data/beauty_sequential_single_prompt_train_sample.json\",  # Training data path\n",
        "    valid_data_path: str = \"data/beauty_sequential_single_prompt_test.json\",  # Validation data path\n",
        "    output_dir: str = \"./lora-alpaca\",  # Output directory for the trained model\n",
        "    # Training hyperparameters\n",
        "    batch_size: int = 32,\n",
        "    micro_batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    learning_rate: float = 3e-4,\n",
        "    cutoff_len: int = 256,  # Max sequence length\n",
        "    val_set_size: int = 500,  # Size of validation set\n",
        "    save_eval_steps: int = 10,  # Steps interval for saving and evaluating\n",
        "    # LoRA hyperparameters\n",
        "    lora_r: int = 8,\n",
        "    lora_alpha: int = 16,\n",
        "    lora_dropout: float = 0.05,\n",
        "    lora_target_modules: list[str] = [\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
        "    # LLM hyperparameters\n",
        "    train_on_inputs: bool = False,  # If False, masks out inputs in loss calculation\n",
        "    group_by_length: bool = False,  # Grouping strategy for efficiency\n",
        "    # WandB (Weights & Biases) parameters for experiment tracking\n",
        "    wandb_project: str = \"llama_med\",\n",
        "    wandb_run_name: str = \"\",\n",
        "    wandb_watch: str = \"\",\n",
        "    wandb_log_model: str = \"\",\n",
        "    resume_from_checkpoint: str = None,  # Path to resume training from a checkpoint\n",
        "    prompt_template_name: str = \"alpaca\",  # Default prompt template\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Orchestrates the entire training process.\n",
        "    \"\"\"\n",
        "    # Ensure only the primary process prints the training parameters\n",
        "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
        "        # Printing training parameters for reference\n",
        "        print(f\"Training parameters: {locals()}\")\n",
        "\n",
        "    # Ensure the base model is specified\n",
        "    assert base_model, \"Base model must be specified.\"\n",
        "\n",
        "    # Calculate the gradient accumulation steps\n",
        "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "\n",
        "    #load model and setup config its\n",
        "    lora_config = LoraConfig(r=lora_r, lora_alpha=lora_alpha,# Configure LoRA settings\n",
        "                             target_modules=lora_target_modules, lora_dropout=lora_dropout)\n",
        "    model, use_wandb = setup_model(base_model=base_model, config=lora_config,\n",
        "                       wandb_project=wandb_project, wandb_watch=wandb_watch,\n",
        "                                  wandb_log_model=wandb_log_model)\n",
        "\n",
        "    # Initialize the prompter with the specified template\n",
        "    prompter = Prompter(prompt_template_name)\n",
        "\n",
        "    tokenizer = initialize_tokenizer(base_model)\n",
        "\n",
        "    # setup data\n",
        "    train_data, val_data = setup_dataset(data_path=data_path, valid_data_path=valid_data_path, val_set_size=val_set_size,\n",
        "                                         prompter=prompter, cutoff_len=cutoff_len, tokenizer=tokenizer, train_on_inputs=train_on_inputs)\n",
        "\n",
        "    # Set up the trainer\n",
        "    # ... Trainer configuration and initialization\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        model=model,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        args=transformers.TrainingArguments(\n",
        "            per_device_train_batch_size=micro_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            warmup_ratio=0.1,\n",
        "            num_train_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            fp16=True,\n",
        "            logging_steps=8,\n",
        "            optim=\"adamw_torch\",\n",
        "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
        "            save_strategy=\"steps\",\n",
        "            eval_steps=save_eval_steps if val_set_size > 0 else None,\n",
        "            save_steps=save_eval_steps,\n",
        "            output_dir=output_dir,\n",
        "            save_total_limit=1000,\n",
        "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
        "            ddp_find_unused_parameters=False if ddp else None,\n",
        "            group_by_length=group_by_length,\n",
        "            report_to=\"wandb\" if use_wandb else None,\n",
        "            run_name=wandb_run_name if use_wandb else None,\n",
        "        ),\n",
        "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
        "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "        ),\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "\n",
        "\n",
        "    # Adjust model's state_dict for PEFT\n",
        "    old_state_dict = model.state_dict\n",
        "    model.state_dict = (\n",
        "        lambda self, *_, **__: get_peft_model_state_dict(\n",
        "            self, old_state_dict()\n",
        "        )\n",
        "    ).__get__(model, type(model))\n",
        "\n",
        "    # Compile the model with TorchScript for better performance if applicable\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # Start the training process\n",
        "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    print(\"\\nTraining completed successfully.\")"
      ],
      "metadata": {
        "id": "5gI4EQnE5suN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_train():\n",
        "  pass"
      ],
      "metadata": {
        "id": "IHxXs4pCGeh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
ABSTRACT
Recommendation systems have witnessed significant advancements
and have been widely used over the past decades. However, most
traditional recommendation methods are task-specific and there-
fore lack efficient generalization ability. Recently, the emergence
of ChatGPT has significantly advanced NLP tasks by enhancing
the capabilities of conversational models. Nonetheless, the appli-
cation of ChatGPT in the recommendation domain has not been
thoroughly investigated. In this paper, we employ ChatGPT as a
general-purpose recommendation model to explore its potential
for transferring extensive linguistic and world knowledge acquired
from large-scale corpora to recommendation scenarios. Specifically,
we design a set of prompts and evaluate ChatGPT‚Äôs performance
on five recommendation scenarios, including rating prediction,
sequential recommendation, direct recommendation, explanation
generation, and review summarization. Unlike traditional recom-
mendation methods, we do not fine-tune ChatGPT during the entire
evaluation process, relying only on the prompts themselves to con-
vert recommendation tasks into natural language tasks. Further,
we explore the use of few-shot prompting to inject interaction in-
formation that contains user potential interest to help ChatGPT
better understand user needs and interests. Comprehensive experi-
mental results on Amazon Beauty dataset show that ChatGPT has
achieved promising results in certain tasks and is capable of reach-
ing the baseline level in others. We conduct human evaluations
on two explainability-oriented tasks to more accurately evaluate
the quality of contents generated by different models. The human
evaluations show ChatGPT can truly understand the provided in-
formation and generate clearer and more reasonable results. We
hope that our study can inspire researchers to further explore the
potential of language models like ChatGPT to improve recommen-
dation performance and contribute to the advancement of the rec-
ommendation systems field. The prompts and codes are available
in https://github.com/williamliujl/LLMRec.
CCS CONCEPTS
‚Ä¢Information systems ‚ÜíRecommender systems .
‚àóBoth authors contributed equally to this research.
CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom
2023. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnnKEYWORDS
Large-Language Model, ChatGPT, Recommendation System
ACM Reference Format:
Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang.
2023. Is ChatGPT a Good Recommender? A Preliminary Study. In The
1st workshop on recommendation with generative models, October 21‚Äì25,
2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
As a crucial technique for addressing information overload and
enhancing user experience, recommendation systems have wit-
nessed significant advancements over the past decade and have
been widely used in various web applications such as product rec-
ommendation [ 32,50,52,60], video recommendation [ 40,55,67],
news recommendation [ 56‚Äì58], music recommendation [ 27,48]
and so on. In the meanwhile, with the development of deep learn-
ing, recommendation systems have gone through several stages.
In early ages, collaborative filtering-based methods [ 5,6,45,63]
are primarily used to model the user‚Äôs behavior patterns from the
user-item interactions. Later on, with the introduction of user and
item side information into recommendation systems, content-based
recommendation [ 37,38,41,54,59] and knowledge-based recom-
mendation [ 2,8,16,18] have gained attention due to their ability
to provide personalized recommendations.
However, most traditional recommendation methods are task-
specific. Therefore, specific data is required to train specific models
for different tasks or application scenarios, which lack efficient gen-
eralization ability. To address this issue, researchers have shifted
their focus towards implementing Pretrained Language Models
(PLMs) in recommendation scenarios since PLMs have demon-
strated impressive adaptability to improve the performance of
downstream NLP tasks significantly [ 33]. To effectively convert
user interaction data into text sequences, a variety of prompts [ 65]
are designed to convert user interaction data into text sequences.
Furthermore, P5 [ 19] and M6-Rec [ 11] focus on building a founda-
tion model to support a wide range of recommendation tasks.
Recently, the emergence of ChatGPT has significantly advanced
NLP tasks by enhancing the capabilities of conversational models,
making it a valuable tool for businesses and organizations. Chataug
et al. [ 12] leverages ChatGPT to rephrase sentences for text data
augmentation. Jiao et al. [ 23] finds the translation ability of Chat-
GPT performs competitively with commercial translation productsarXiv:2304.10149v3  [cs.IR]  27 Oct 2023CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Junling Liu, et al.
on high-resource and low-resource languages. Bang et al. [ 3] finds
ChatGPT outperforms the previous state-of-the-art zero-shot model
by a large margin in the sentiment analysis task. Nonetheless, the
application of ChatGPT in the recommendation domain has not
been thoroughly investigated, and whether ChatGPT can perform
well on classical recommendation tasks remains an open question.
Therefore, it is necessary to establish a benchmark to preliminarily
evaluate and compare ChatGPT with traditional recommendation
models, thereby providing valuable insights and facilitating fur-
ther exploration of the potential of large-scale language models in
recommendation systems.
To bridge this research gap, in this paper, we directly employ
ChatGPT as a general-purpose recommendation model that can han-
dle various recommendation tasks, and attempt to explore whether
the extensive linguistic and world knowledge acquired from large-
scale corpora can be effectively transferred to recommendation sce-
narios. Our main contribution is the construction of a benchmark
to track ChatGPT‚Äôs performance in recommendation scenarios,
and a comprehensive analysis and discussion of its strengths and
limitations. Specifically, we design a set of prompts and evaluate
ChatGPT‚Äôs performance on five recommendation tasks, including
rating prediction, sequential recommendation, direct recommen-
dation, explanation generation, and review summarization. Unlike
traditional recommendation methods, we do not fine-tune ChatGPT
during the entire evaluation process, relying only on the prompts
themselves to convert recommendation tasks into natural language
tasks. Furthermore, we explore the use of few-shot prompting to
inject interaction information that contains user potential interests
to help ChatGPT better understand user needs and preferences.
Comprehensive experimental results on Amazon Beauty dataset
reveal that, from the perspective of accuracy, ChatGPT performs
well in rating prediction but poorly in sequential and direct recom-
mendation tasks, achieving only similar performance levels to early
baseline methods on certain metrics. On the other hand, while
ChatGPT demonstrates poor performance in terms of objective
evaluation metrics for explainable recommendation tasks such as
explanation generation and review summarization, our additional
human evaluations show that ChatGPT outperforms state-of-the-
art methods. This highlights the limitations of using an objective
evaluation approach to accurately reflect ChatGPT‚Äôs true explain-
able recommendation capabilities. Furthermore, despite ChatGPT‚Äôs
unsatisfactory performance in accuracy-based recommendation
tasks, it is worth noting that ChatGPT has not been specifically
trained on any recommendation data. Thus, there is still signifi-
cant potential for improvement in future research by incorporating
more relevant training data and techniques. We believe that our
benchmark not only sheds light on ChatGPT‚Äôs recommendation ca-
pabilities but also provides a valuable starting point for researchers
to better understand the advantages and shortcomings of Chat-
GPT in recommendation tasks. Moreover, we hope that our study
can inspire researchers to design new methods that leverage the
strengths of language models like ChatGPT to improve recom-
mendation performance, and contribute to the advancement of the
recommendation systems field.2 RELATED WORK
2.1 Large Language Models and ChatGPT
Language Models (LMs) are a fundamental component of natural
language processing (NLP) and have been the focus of research for
several decades. Recently, the emergence of large-scale LMs has led
to significant progress in NLP tasks such as machine translation[ 1,
9, 62], summarization[34, 47], and dialogue generation[14, 28].
Large Language Models (LLMs) are a subclass of LMs that lever-
age massive amounts of data and computational resources to achieve
state-of-the-art performance on a wide range of NLP tasks. The
history of LLMs can be traced back to the early work on neural
networks and language modeling. [ 4] introduced neural language
models that learned to predict the next word in a sentence given
the previous words. Later, the development of recurrent neural
networks (RNNs) and long short-term memory (LSTM) networks
further improved the ability of models to capture long-term de-
pendencies in language[ 22]. However, traditional neural language
models still struggled with capturing the rich semantic and contex-
tual relationships present in natural language. The introduction of
the Transformer architecture by [ 53] was a major breakthrough in
this area. The Transformer model utilizes self-attention mechanisms
to capture the relationships between all elements in a sequence
simultaneously, allowing for more comprehensive contextual under-
standing. This architecture has been used as the backbone of many
successful LLMs, including BERT[13], GPT-2[42], and XLNet[61].
ChatGPT[ 39] is a state-of-the-art dialogue system developed by
OpenAI in 2022. It is a state-of-the-art natural language processing
(NLP) model that has been widely used in various vertical domains,
such as text generation and dialogue systems. In text generation,
ChatGPT has shown impressive results in generating coherent
and diverse text, surpassing the performance of previous models
[7]. In dialogue systems, it has been used for task-oriented and
open-domain conversations, achieving state-of-the-art performance
in both settings [ 66]. Although the value of ChatGPT has been
validated in various fields, whether it can still be effective in the
recommendation domain remains an under-explored topic, which
motivates us to construct such a benchmark to gain insights into the
potential of large language models for recommendation systems.
2.2 Language Model for Recommendation
Language Models (LMs), such as BERT [ 13] and GPT [ 39], have
demonstrated impressive adaptability to improve the performance
of downstream NLP tasks significantly, thanks to extensive linguis-
tic and world knowledge learned from large-scale corpora. Inspired
by these achievements, an increasing amount of attention is being
paid to the application of LMs in recommender scenarios, yielding
several recent breakthroughs in this field. For instance, LMRec-
Sys [ 65] utilizes prompts to reconstitute some recommendation
tasks as multi-token cloze tasks, aiming to address zero-shot and
data efficiency issues. P5 [ 19] is the first attempt to integrate differ-
ent recommendation tasks within a shared conditional language
generation framework (i.e., T5 [ 43]). To effectively convert user in-
teraction data into text sequences, a variety of prompts are designed
to accommodate the specific characteristics of each recommenda-
tion task. Similarly, M6-Rec [ 11] focuses on building a foundationIs ChatGPT a Good Recommender? A Preliminary Study CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom
User profiles
Item Attributes
User-item interactionTask-specific Prompt GenerationTask DescriptionFormat Indicator
ChatGPT as Recommender
Accuracy-oriented TasksUser-specific Injection
Rating PredictionSequential RecommendationDirect RecommendationFormat Correction
Review SummaryExplanation GenerationExplainability-oriented TasksOutput Refinement
EvaluationObjective EvaluationFinal OutputHuman EvaluationRe-recommendFormat CheckÔºüSuccessfulÔºü
Figure 1: Workflow of utilizing ChatGPT to perform five rec-
ommendation tasks and evaluate its recommendation per-
formance.
model to support a wide range of recommendation tasks, includ-
ing retrieval, ranking, and explanation generation, etc. Notably,
the authors also provide practical solutions for model deployment
in real-world settings. Chat-REC [ 17], a concurrent work closely
related to our study, leverages ChatGPT as an interface for conver-
sational recommendations, thereby augmenting the performance of
existing recommender models and rendering the recommendation
process more interactive and explainable.
Different from Chat-REC, our work is inspired by P5 and treats
ChatGPT as a self-contained recommendation system that does
not rely on any external systems. Based on this, we conduct a
thorough evaluation and comparison of its performance on clas-
sic recommendation tasks including sequential recommendation,
rating prediction, etc. By doing so, we hope our analysis can offer
valuable insights for researchers to delve deeper into the potential
of large-scale language models in the domain of recommendation.
3 RECOMMENDATION WITH CHATGPT
The workflow of using ChatGPT to complete recommendation tasks
is illustrated in Fig.1, which consists of three steps. First, different
prompts are constructed based on the specific characteristics of
the recommendation tasks (Section 3.1). Second, these prompts are
used as inputs for ChatGPT, which generates the recommendation
results according to the requirements specified in the prompts.
Finally, the output from ChatGPT is checked and refined by the
refinement module, and the refined results are returned to the user
as the final recommendation results (Section 3.2).
3.1 Task-specific Prompt Construction
In this section, we investigate the recommendation capability of
ChatGPT by designing prompts tailored to different tasks. Each
prompt comprises three parts: task description, behavior injection,
and format indicator. The task description is utilized to adapt recom-
mendation tasks to natural language processing tasks. The behavior
injection is designed to assess the impact of few-shot prompting,
which incorporates user-item interaction to aid ChatGPT in cap-
turing user preferences and needs more effectively. The formatindicator serves to constrain the output format, making the recom-
mendation results more comprehensible and assessable.
3.1.1 Rating Prediction. Rating prediction is a crucial task in rec-
ommendation systems that aims to predict the ratings that a user
would give to a particular item. This task is essential in personal-
izing recommendations for users and improving the overall user
experience. Some recent advancements in this field include the use
of deep learning models[ 20], and the use of matrix factorization
techniques[ 26], which are effective in dealing with the sparsity
problem in recommendation systems. In line with the innovative
recommendation paradigm of the LLM, we conducted experiments
on a rating task that involved formulating two unique prompt types
to elicit the results. We provide some sample prompts in Fig.2.
3.1.2 Sequential Recommendation. Sequential recommendation is
a subfield of recommender systems that aims to predict a user‚Äôs
next item or action based on their past sequential behavior. It has
received increasing attention in recent years due to its potential
applications in various domains, such as e-commerce, online adver-
tising, and music recommendation. In sequential recommendation,
researchers have proposed various methods, including recurrent
neural networks[ 31], contrastive learning[ 69], and attention-based
models[ 53,70], for capturing the temporal dependencies and pat-
terns in user-item interactions. We have devised three distinct
prompt formats for the sequential recommendation task family.
These include: 1) direct prediction of the user‚Äôs next item based
on their interaction history, 2) selection of a possible next item
from a list of candidates, where only one item is positive and based
on the user‚Äôs interaction history, and 3) prediction of whether a
specific item will be the next one interacted with by the user, using
their previous interaction history as a basis. These prompt formats
have been designed to enhance the accuracy and effectiveness of se-
quential recommendations, and are grounded in rigorous academic
principles. Examples of these prompts can be seen in Fig.2.
3.1.3 Direct Recommendation. Direct Recommendation, also known
as explicit feedback recommendation or rating-based recommen-
dation, is a type of recommendation system that relies on explicit
feedback from users in the form of ratings or reviews. Unlike other
recommendation systems that rely on implicit feedback, such as
user behavior or purchase history, direct recommendation systems
are able to provide more personalized and accurate recommenda-
tions by taking into account the explicit preferences of users. For
this task, we develop the item selection prompt that selects the most
appropriate item from a list of potential candidates. These prompt
formats are based on rigorous academic principles and aim to opti-
mize the accuracy and relevance of recommendations. Examples of
these prompts can be seen in Fig.2.
3.1.4 Explanation Generation. Explanation generation refers to
providing users or system designers with explanations to clarify
why such items are recommended. In this way, it enhances the
transparency, persuasiveness, effectiveness, trustworthiness, and
user satisfaction of recommendation systems. Furthermore, it fa-
cilitates system designers in diagnosing, debugging, and refining
the recommendation algorithm. Large language models such as
ChatGPT can use the vast amount of knowledge they contain to
learn the user‚Äôs interests through their historical interaction recordsCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Junling Liu, et al.
Rating Prediction
Here is user rating history: 
1. Bundle Monster 100 PC 3D Designs Nail Art Nailart Manicure Fimo Canes Sticks Rods Stickers Gel Tips, 5.0;
2. Winstonia ‚Äòs Double Ended Nail Art Marbling Dotting Tool Pen Set w/ 10 Different Sizes 5 Colors - Manicure Pedicure, 5.0;
3. Nail Art Jumbo Stamp Stamping Manicure Image Plate 2 Tropical Holiday by Cheeky&reg, 5.0 ;
4.Nail Art Jumbo Stamp Stamping Manicure Image Plate 6 Happy Holidays by Cheeky&reg, 5.0;
Based on above rating history, please predict user's rating for the product:  "SHANY Nail Art Set (24 Famouse Colors Nail Art Polish, Nail 
Art Decoration)" , (1 being lowest and5 being highest,The output should be like: (x stars, xx%), do not explain the reason.)How will user rate this product_title:  "SHANY Nail Art Set (24 Famous Colors Nail Art Polish, Nail Art Decoration)"  , and 
product_category: Beauty?  ( 1 being lowest and 5 being highest ) Attention! Just give me back the exact number a result , and you don't need 
a lot of t ext. zero-shot
few-shot
Sequential Recommendation
Requirements: you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a 
python list. Do not explain the reason or include any other words. 
Given the user's interaction history in chronological order:  ['Avalon Biotin B -Complex Thickening Conditioner, 14 Ounce', 'Conair 1600 Watt 
Folding Handle Hair Dryer', ‚Ä¶..., 'RoC Multi -Correxion 4 -Zone Daily Moisturizer, SPF 30, 1.7 Ounce'] , the next interacted item is ['Le Edge 
Full Body Exfoliator - Pink'] . Now, if the interaction history is updated to  ['Avalon Biotin B -Complex Thickening Conditioner, 14 Ounce', 
'Conair 1600 Watt Folding Handle Hair Dryer', ‚Ä¶..., 'RoC Multi -Correxion 4 -Zone Daily Moisturizer, SPF 30, 1.7 Ounce', 'Le Edge Full 
Body Exfoliator - Pink']  and the user is likely to interact again, recommend the next item.Requirements: you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a 
python list. Do not explain the reason or include any other words. 
The user has interacted with the following items in chronological order:  ['Better Living Classic Two Chamber Dispenser, White', 'Andre 
Silhouettes Shampoo Cape, Metallic Black', ‚Ä¶..., 'John Frieda JFHA5 Hot Air Brush, 1.5 inch'] .Please recommend the next item that the user 
might interact with.zero-shot
few-shot
Direct Recommendation
Requirements: you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a 
python list. Do not explain the reason or include any other words. 
The user has interacted with the following items (in no particular order):  ['Maybelline New York Eye Studio Lasting Drama Gel Eyeliner, 
Eggplant 956, 0.106 Ounce', ""L'Oreal Paris Healthy Look Hair Color, 8.5 Blonde/White Chocolate"", ‚Ä¶‚Ä¶, 'Duo Lash Adhesive, Clear, 0.25 
Ounce'] . Given that the user has interacted with 'WAWO 15 Color Professionl Makeup Eyeshadow Camouflage Facial Concealer Neutral 
Palette'  from a pool of candidates:  ['MASH Bamboo Reusable Cuticle Pushers Remover / Manicure Pedicure Stick', 'Urban Decay All Nighter 
Long -Lasting Makeup Setting Spray 4 oz', ......,'Classic Cotton Balls Jumbo Size, 100 Count'] , please recommend the best item from a new 
candidate pool, ['Neutrogena Ultra Sheer Sunscreen SPF 45 Twin Pack 6.0 Ounce', 'Blinc Eyeliner Pencil - Black', ......,'Skin MD Natural + 
SPF15 combines the benefits of a shielding lotion and a sunscreen lotion'] . Note that the candidates in the new pool are not ordered in any 
particular way. Requirements: you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a 
python list. Do not explain the reason or include any other words. 
The user has interacted with the following items (in no particular order):  [""Skin Obsession Jessner's Chemical Peel Kit Anti -aging and Anti -
acne Skin Care Treatment"", 'Xtreme Brite Brightening Gel 1oz.', ‚Ä¶..., 'Reviva - Light Skin Peel, 1.5 oz cream'] . From the candidates listed 
below, choose the top 10 items to recommend to the user and rank them in order of priority from highest to lowest. Candidates:  ['Rogaine for 
Women Hair Regrowth Treatment 3 - 2 ounce bottles', 'Best Age Spot Remover', ‚Ä¶..."" L'Oreal Kids Extra Gentle 2 -in-1 Shampoo With a 
Burst of Cherry Almond, 9.0 Fluid Ounce""] . zero-shot
few-shot
Figure 2: Example prompts of accuracy-based tasks on Beauty dataset. The black texts represent the description of the task, the
red texts indicate the format requirements, the blue texts represent user historical information or few-shot information, and
the gray texts indicate the current input.
and provide reasonable explanations for their behavior. Specifically,
We ask ChatGPT model to generate a textual explanation to justify
a user‚Äôs preference towards a selected item as shown in Fig.3. For
each category, additional auxiliary information such as the hint
word and the star rating could be included.
3.1.5 Review Summarization. Automatic generation of summaries
is becoming increasingly important in Natural Language Process-
ing, as the demand for concise and easily comprehensible content
continues to grow. Similar to the explanation generation task, wecreate two types of prompts: zero/few-shot prompts, and provide
some example prompts in Fig.3.
3.2 Output Refinement
To ensure the diversity of generated results, ChatGPT incorpo-
rates a degree of randomness into its response generation process,
which may result in different responses for the same input. How-
ever, when using ChatGPT for recommendation, this randomness
can sometimes cause difficulties in evaluating the recommendedIs ChatGPT a Good Recommender? A Preliminary Study CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom
Explanation Generation
 Here are some recommended products and their corresponding explanations for user: 
1. product "TIGI Catwalk Curl Collection Curlesque Curls Rock Amplifier, 5.07 Ounce, Packaging May Vary"  and its explanation "One of 
the few things I have found that work for white people with curly hair"
2. product  "DevaCurl Mist -er Right Lavender Curl Revitalizer 12.0 oz"  and its explanation "it makes my hair greasy and gross"
Help user generate a 5.0 -star explanation about this product  "SHANY Nail Art Set (24 Famouse Colors Nail Art Polish, Nail Art Decoration)" 
with around 10 words ? Here is user's interaction history: 
1. Prolab Caffeine - Maximum Potency 200 mg 100 Tablets
2. DevaCurl Mist -er Right Lavender Curl Revitalizer 12.0 oz
3. TIGI Catwalk Curl Collection Curlesque Leave -In Conditioner, 7.27 Ounce
4. e.l.f. Pigment Eyeshadow, Naturally Nude, 0.05 Ounce
Help user generate a 5.0 -star explanation about this product  "SHANY Nail Art Set (24 Famouse Colors Nail Art Polish, Nail Art Decoration)" 
with around 10 words .zero-shot
few-shot
Review Sumarization
 Here are some reviews and their corresponding summaries of user: 
1. Review: "After watching kardashian episode back in 2009 kim mentioned OPI my private jet.. and i was like what is that?  i looked it up 
online and LOVED it and i just got it 1 week shipping.. awesomee just love this color its sparkley brown and its turns black sometimes cool!! 
well for mee looolll LOVE this color &lt;3 on my toes and fingers lol" . Summary: "Loving this sooo muchh" 
2. Review :"I love this and im glad im adding this to my collection! (: a nice top coat or alone very shimmery and very pretty, especia lly the 
top brush cap thing its silver than the original black! (: i recommend this" . Summary: "Amazing color" 
 Write a short sentence to summarize the following product review from user: "So i was pretty excited that i got this in the mail, but 
seriously.....i think its just the color of mine, i don't know, not good cover stick... Received it sticking to the top...so basically it was broken 
when i opened it because of the air mail looks very pasty...very white i shall say...im never buying this product ever.." . The sentence should 
be around 4 words.Here are some summaries of user: 
1. "Loving this sooo muchh"
2. "Amazing color"
3. "..Hong Kong Collection &lt;3 OPI"
 Write a short sentence to summarize the following product review from user: "So i was pretty excited that i got this in the mail, but 
seriously.....i think its just the color of mine, i don't know, not good cover stick... Received it sticking to the top...so basically it was broken 
when i opened it because of the air mail looks very pasty...very white i shall say...im never buying this product ever.." . The sentence should 
be around 4 words .zero-shot
few-shot
Figure 3: Example prompts of explainability-oriented tasks on Beauty dataset. The black texts represent the description of
the task, the red texts indicate the format requirements, the blue texts represent user historical information or few-shot
information, and the gray texts indicate the current input.
items. While the format indicator in the prompt construction can
partially alleviate this issue, in practical usage, it still cannot guar-
antee the anticipated output format. Therefore, we devise output
refinement module to check the format of ChatGPT‚Äôs output. If
the output passes the format check, it is directly used as the fi-
nal output. If not, it is modified based on pre-defined rules. If the
format correction is successful, the corrected result is used as the
final output. If not, the corresponding prompt is fed into ChatGPT
for a re-recommendation until the format requirements are met. It
is worth noting that different tasks have different output format
requirements when evaluating ChatGPT. For example, for rating
prediction, only a specific score is needed, whereas for sequential
or direct recommendation, a list of recommended items is required.
Particularly for sequence recommendation, it is challenging to feed
all the items in the dataset to ChatGPT at once. As a result, Chat-
GPT‚Äôs output may not correctly match the item set in the dataset. To
address this issue, we introduce a text matching method based on
similarity in the correction process to map ChatGPT‚Äôs predictionsback to the original dataset. Although this method may not per-
fectly reflect ChatGPT‚Äôs ability, it can still indirectly demonstrate
its potential in sequential recommendation.
4 EVALUATION
To evaluate ChatGPT, we conduct extensive experiments on the real-
world Amazon dataset. Through the performance comparison with
various representative methods and ablation studies on different
tasks, we aim to answer the following research questions:
‚Ä¢RQ1 : How does ChatGPT perform as compared with the
state-of-the-art baseline models?
‚Ä¢RQ2 : What is the impact of few-shot prompting on perfor-
mance?
‚Ä¢RQ3 : How do we design the human evaluation to assess
explanation generation and summarization tasks?
4.1 Experimental Setup
4.1.1 Datasets. We conduct numerical and human evaluations
on the real-world Amazon recommendation dataset. The AmazonCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Junling Liu, et al.
dataset contains the customer review text with accompanying meta-
data on 29 categories of products. This paper focuses on evaluating
theBeauty category.
4.1.2 Metrics. In numerical evaluations, we employ Root Mean
Square Error (RMSE) and Mean Absolute Error (MAE) for rating pre-
diction. And we adopt top- kHit Ratio (HR@ k), top- kNormalized
Discounted Cumulative Gain (NDCG@ k) for sequential recommen-
dation and direct recommendation which are widely used in re-
lated works [ 19,68]. Specifically, we report results on HR@{1,5,10},
NCGG@{5,10} for evaluation. Besides, n-gram Bilingual Evaluation
Understudy (BLEU- n) and n-gram Recall-Roiented Understudy for
Gising Evaluation (ROUGE- n) are used to evaluate the explanation
generation and review summarization tasks. In human evaluations,
we have designed and deployed a crowdsourcing task to assess
the qualities of the generated explanations and review summaries.
Through this task, we aim to accurately evaluate the effectiveness
of the content by gathering feedback from a diverse range of human
evaluators.
4.1.3 Implementation Details. In order to verify that we can di-
rectly apply the knowledge learned by ChatGPT to recommendation
scenarios without the need for a large amount of task-specific data
for training, we apply gpt-3.5-turbo to conduct few-shot and zero-
shot experiments for the five tasks mentioned above. We collect
nitems that users have interacted with and kshots of historical
records to enable ChatGPT to learn users‚Äô interests implicitly. In
this experiment, we use the titles of the items as meta information,
and set ùëõ=10andùëò=3due to the limitation of a maximum
context length of 4096 tokens in ChatGPT. We randomly sample
100 records from the test set proposed by P5 [ 19] for evaluation.
For direct recommendation, we set the number of negative sam-
ples to 99, thus forming a candidate list of length 100 with one
positive item. Also, due to the addition of the candidate pool in
the request, we set the number of shots to 1. For sequential recom-
mendation, we input the user‚Äôs historical interacted items in order
and let ChatGPT predict the title of the next item that the user
might interact with, and use BERT[ 13] to calculate the vector of the
predicted title and compute the similarity between the predicted
title vector and the title vectors of all items, and select the item with
the highest similarity as the predicted item. For human evaluation
on explanation generation and review summarization, we sample
some results of different methods for each task, and each result will
be scored and ranked by three human evaluators. After obtaining
the manually annotated results, we will calculate the average top1
ratio and average ranking position of different methods to measure
their generation performance.
4.2 Baselines for multiple tasks
Following P5 [ 19], we gather a range of approaches that are repre-
sentative of various tasks. For rating prediction, we employ MF [ 25]
and MLP [ 10] as our baselines, both evaluated using mean square
root loss. For direct recommendation, we use BPR-MF [ 44], BPR-
MLP [ 10] and SimpleX [ 36] as baselines. For sequential recommen-
dation, we adopt Caser [ 51], HGN [ 35], GRU4Rec [ 21], BERT4Rec
[49], FDSA [ 64], SASRec [ 24] and S3-Rec [ 68] as baselines for com-
parison. For explanation generation, we utilize Attn2Seq [ 15], NRTTable 1: Performance comparison on rating prediction.
MethodsBeauty
RMSE MAE
MF 1.1973 0.9461
MLP 1.3078 0.9597
ChatGPT(zero-shot) 1.4059 1.1861
ChatGPT(few-shot) 1.0751 0.6977
[30] and PETER [ 29] as baselines. For review summarization, we
adopt pretrained T0 [ 46] and GPT-2 [ 42] as baselines. For more
details, you can refer to P5 [19] or relevant articles.
4.3 Performance Comparison on 5 Tasks
(RQ1&2)
4.3.1 Rating prediction. To evaluate the rating prediction perfor-
mance of ChatGPT, zero-shot and few-shot prompts were employed,
and the results obtained from the Beauty dataset were summarized
in Tab.1. The results indicate that, for the seen category on the
Beauty dataset, few-shot prompts outperform MF and MLP in terms
of both MAE and RMSE. These results provide evidence supporting
the feasibility of utilizing a conditional text generation framework
for rating prediction.
4.3.2 Sequential recommendation. To assess the sequential recom-
mendation capability of ChatGPT, we conducted both zero-shot and
few-shot experiments, the results of which are shown in Tab.2. We
found that, compared to the baselines, ChatGPT‚Äôs performance in
the zero-shot prompting setup is considerably inferior, with all met-
rics being significantly lower than the baselines. However, under
the few-shot prompting setup, while there is a relative improvement
in performance, such as NDCG@5 surpassing GRU4Rec, ChatGPT
is still generally outperformed by classical sequential recommenda-
tion methods in most cases. There are possibly two main reasons
contributing to this outcome: First, during the prompting design
process, all items are represented by their titles. Although this ap-
proach can alleviate the cold-start problem to some extent, it may
cause ChatGPT to focus more on semantic similarity rather than
the transition relationships between items, which are crucial for
effective recommendations. Second, due to the length constraint of
the prompts, it is not possible to input all items from the item set
into ChatGPT. This leads to ChatGPT lacking constraints in pre-
dicting the title of the next item, resulting in generating item titles
that do not exist in the dataset. Although it is possible to map these
predicted titles to existing titles in the dataset through semantic
similarity matching, our experiments show that this mapping does
not result in significant gains. Therefore, for sequential recommen-
dation tasks, merely employing ChatGPT is not a suitable choice.
Further exploration is needed to introduce more guidance and con-
straints to help ChatGPT accurately capture historical interests and
make reasonable recommendations within a limited scope.
4.3.3 Direct recommendation. Tab.3 illustrates the performance of
ChatGPT on the direct recommendation task. Unlike the sequen-
tial recommendation setup, direct recommendation requires the
recommendation model to select the most relevant item for the
user from a limited-sized item pool. We observed that, when usingIs ChatGPT a Good Recommender? A Preliminary Study CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom
Table 2: Performance comparison on sequential recommen-
dation.
MethodsBeauty
HR@5 NDCG@5 HR@10 NDCG@10
Caser 0.0205 0.0131 0.0347 0.0176
HGN 0.0325 0.0206 0.0512 0.0266
GRU4Rec 0.0164 0.0099 0.0283 0.0137
BERT4Rec 0.0203 0.0124 0.0347 0.0170
FDSA 0.0267 0.0163 0.0407 0.0208
SASRec 0.0387 0.0249 0.0605 0.0318
S3-Rec 0.0387 0.0244 0.0647 0.0327
P5-B 0.0493 0.0367 0.0645 0.0416
ChatGPT(zero-shot) 0.0000 0.0000 0.0000 0.0000
ChatGPT(few-shot) 0.0135 0.0135 0.0135 0.0135
Table 3: Performance comparison on direct recommendation.
MethodsBeauty
HR@5 NDCG@5 HR@10 NDCG@10
BPR-MF 0.1426 0.0857 0.2573 0.1224
BPR-MLP 0.1392 0.0848 0.2542 0.1215
SimpleX 0.2247 0.1441 0.3090 0.1711
P5-B 0.1564 0.1096 0.2300 0.1332
ChatGPT(zero-shot) 0.0217 0.0111 0.0652 0.0252
ChatGPT(few-shot) 0.0349 0.0216 0.0930 0.0398
zero-shot prompting, the recommendation performance is signif-
icantly inferior to supervised recommendation models. This can
be attributed to the insufficient information provided to ChatGPT,
resulting in an inability to capture user interests and generate more
random recommendations. While few-shot prompting can improve
ChatGPT‚Äôs recommendation performance by providing some of
the user‚Äôs historical preferences, it still fails to surpass the baseline
performance.
It is worth noting that we discovered during the experiments that
the construction of the item pool, specifically whether the item pool
is shuffled or not, has a considerable impact on the direct recom-
mendation performance. In an extreme scenario where the ground
truth item is placed at the first position in the item pool, we found
that the evaluation metrics were approximately ten times higher
than when the item pool was shuffled. This finding suggests that
ChatGPT exhibits a positional bias for the input item pool within
the prompt, tending to consider items towards the beginning of the
pool as more important, and thus more likely to be recommended.
This additional bias introduced by the language model renders us-
ing ChatGPT for direct recommendation a challenging endeavor.
4.3.4 Explanation Generation. In Tab.4, both zero-shot and few-
shot prompts are used to evaluate ChatGPT‚Äôs performance on ex-
planation generation. From the metrics perspective, the P5 model
has a better performance. As language models, P5 and ChatGPT
have different design goals and application scenarios. P5 aims to
generate explanatory language similar to known texts. Therefore,
P5 focuses on learning text structure and grammar rules duringTable 4: Performance comparison on explanation generation
(%).
MethodsBeauty
BLUE4 ROUGE1 ROUGE2 ROUGEL
Attn2Seq 0.7889 12.6590 1.6820 9.7481
NRT 0.8295 12.7815 1.8543 9.9477
PETER 1.1541 14.8497 2.1413 11.4143
P5-B 0.9742 16.4530 1.8858 11.8765
PETER+ 3.2606 25.5541 5.9668 19.7168
ChatGPT(zero-shot) 0.0000 8.5992 0.6995 4.7564
ChatGPT(few-shot) 1.1967 11.4103 2.5675 5.9119
Table 5: Performance comparison on review summarization
(%).
MethodsBeauty
BLUE4 ROUGE1 ROUGE2 ROUGEL
T0 1.2871 1.2750 0.3904 0.9592
GPT-2 0.5879 3.3844 0.6756 1.3956
P5-B 2.1225 8.4205 1.6676 7.5476
ChatGPT(zero-shot) 0.0000 3.8246 0.2857 3.1344
ChatGPT(few-shot) 0.0000 2.7822 0.0000 2.4328
training, making the generated results more standardized, as shown
in Fig.4. In contrast, ChatGPT focuses more on language interac-
tion and diversity. Its application scenario is usually to simulate
human conversation, so it needs to consider multiple factors such
as context, emotion, and logic when generating text to better ex-
press human thinking and language habits. This design is bound
to make the text generated by ChatGPT more diverse and creative.
Besides, P5 is fine-tuned on Beauty dataset while ChatGPT is uti-
lized in a zero-shot or few-shot experimental setting. Therefore, it
is understandable that ChatGPT may not perform as well as P5 in
metrics. Hence, we introduce human evaluation to better measure
the performance of different models in generating content.
4.3.5 Review summarization. We conduct zero-shot and few-shot
experiments to evaluate ChatGPT‚Äôs ability on review summariza-
tion, as shown in Tab.5. Similar to the explanation generation task,
ChatGPT does not have an advantage in metrics evaluation. How-
ever, although the summary result of P5 has extracted some key-
words, it has ignored relevant information from the entire review.
In contrast, ChatGPT can generate more effective and meaningful
summaries by deeply understanding and summarizing the reviews.
As shown in Fig.5. Hence, we also conduct human evaluation in
this task.
4.4 Human Evaluation (RQ3)
As shown in the experiments above, we conducted numerical eval-
uations on the explanation generation and review summarization
tasks using the test set constructed by P5. However, the ground-
truth explanations generated by P5 are not truly accurate because
P5 extracts sentences from views commenting on one or more item
feature words as users‚Äô explanations about their preferences. In that
case, we designed human evaluations to better assess the perfor-
mance of ChatGPT. Specifically, we randomly sample 20 prompts forCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Junling Liu, et al.
Explanation Generation Results
Ground truth:  "this is the best deal I've seen on nail polish in a long time " 
P5's output:   "great price and great quality and great price " 
ChatGPT ‚Äôs output: "SHANY's Nail Art Set is a must -have for stunning 
manicures."Wow, this is the best deal I've seen on nail polish in a long time. 
You get so many vibrant beautiful colors to choose from. These 
are nail art brushes for fine detail. I love that you can get a 
whole kit for this price!Love the colors. Didn't get any doubles. 1 bottle was not fully 
closed and the bottle chipped on the neck of the bottle. But 
being where the break was I just closed it and it is still usable. I 
wouldn't recommend this for painting your full nail (It is for 
art), but I would for stamping and nail art. Small brushes great 
for that. Not all work for stamping though, like the metallic 
ones.Ground truth:  "I wouldn't recommend this for painting your full nail (It is 
for art) " 
P5's output:   "great price and great price and great price " 
ChatGPT ‚Äôs output: "SHANY's Nail Art Set is a must -have for creative 
nails. "Absolutely great product.  I bought this for my fourteen year 
old niece for Christmas and of course I had to try it out, then I 
tried another one, and another one and another one.  So much 
fun!  I even contemplated keeping a few for myself!Ground truth:  "Absolutely great product " 
P5's output:   "great colors and great price for the price " 
ChatGPT ‚Äôs output: "Love this nail art set - perfect colors and variety!"Review s Results
Figure 4: Example explanation results of different models on Beauty dataset.
Review Summarization  Results
Ground truth:  "works! " 
P5's output:  "works " 
ChatGPT ‚Äôs output: "Effective and user -friendly. ""you can see and feel that it's working.  easy to use too.  after a 
few times you'll get the hang of it.""Great to use after the microdermabrasion roller needle 
process. Skin absorbs it quickly. Face looks more rejuvenated 
in the morning."Ground truth:  "Works Well " 
P5's output:  "Great " 
ChatGPT ‚Äôs output: "Quickly absorbed rejuvenating serum. ""These brushes are okay. I don't think they're anything special, 
but for the price their quality is okay. I don't know that I would 
buy them again though.".Ground truth:  "It's okay " 
P5's output:  "Okay " 
ChatGPT ‚Äôs output: "Average brushes for price. ""I truly love this soap. I have very sensitive skin and this is one 
of the few soaps that doesn't dry out or break out my skin. 
Would recommend to others. It smells soft as well"Ground truth: "Soap " 
P5's output:   "Great soap " 
ChatGPT ‚Äôs output: "Gentle, effective soap recommended. "Review s Results
Figure 5: Example summarization results of different models on Beauty dataset.
Table 6: Human evaluation for explanation generation on
Beauty dataset.
MethodsEvaluators
avg_top1_ration avg_positionEva_1 Eva_2 Eva_3 Eva_4
Ground truth 25.0% 45.0% 45.0% 50.0% 38.0% 1.83
P5 0.0% 0.0% 0.0% 0.0% 0.0% 2.71
ChatGPT(zero-shot) 75.0% 55.0% 55.0% 50.0% 62.0% 1.46
explanation generation and 97 prompts for review summarization
from the Beauty dataset and display every generated explanation
or summary to several evaluators. The evaluators rank the results
generated by ChatGPT, baseline, and ground truth for assessment.
avg_top1_ration represents the proportion in which the prompt
ranked first among the prompts. avg_position denotes the average
position of sorting for each prompt.
For explanation generation task, as shown in Tab.6, the results
of the four manual annotators have a certain degree of subjectivity,
but the score distribution is relatively consistent, with a general
consensus that the explanations generated by ChatGPT are clearerTable 7: Human evaluation for review summarization on
Beauty dataset.
MethodsEvaluators
avg_top1_ration avg_positionEva_1 Eva_2 Eva_3 Eva_4 Eva_5
Ground truth 12.5% 10.6% 8.7% 17.3% 22.1% 14.2% 2.91
P5 5.8% 0.0% 5.7% 11.5% 19.2% 8.5% 3.16
ChatGPT(zero-shot) 46.2% 37.5% 36.5% 45.2% 23.1% 37.7% 1.90
ChatGPT(few-shot) 35.6% 51.9% 49.0% 26.0% 35.6% 39.6% 2.01
and more reasonable, even better than the ground truth. Meanwhile,
P5‚Äôs performance is the worst, with explanations tending towards
a generic style and sentences that are not fluent. We can also draw
the same conclusion from the examples in Tab.4. For review summa-
rization task, we can find in Fig.5 that the contents summarized in
P5 are too general and do not extract useful information. However,
ChatGPT can truly understand the reviews and provide accurate
summaries, rather than simply extracting a few keywords from the
reviews. As shown in Tab.7, all annotators unanimously agree thatIs ChatGPT a Good Recommender? A Preliminary Study CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom
ChatGPT has the best performance, surpassing ground truth and
P5 by a large margin.
5 CONCLUSION AND FUTURE WORK
In this paper, we construct a benchmark to evaluate ChatGPT‚Äôs per-
formance in recommendation tasks and compare it with traditional
recommendation models. The experimental results show that Chat-
GPT performs well in rating prediction but poorly in sequential
and direct recommendation tasks, indicating the need for further
exploration and improvement. Despite its limitations, ChatGPT
outperforms state-of-the-art methods in terms of human evaluation
for explainable recommendation tasks, highlighting its potential to
generate explanations and summaries. We believe that our study
provides valuable insights into the strengths and limitations of
ChatGPT in recommendation systems, and we hope that it can
inspire future research to explore the use of large language models
to enhance recommendation performance. Moving forward, we
plan to investigate better ways to incorporate user interaction data
into large language models and bridge the semantic gap between
language and user interests.